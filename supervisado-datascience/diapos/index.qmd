---
title: "Aprendizaje Supervisado I"
subtitle: "M√©todos de predicci√≥n lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada ‚Ä¢ curso 2024-2025"
affiliation: Facultad de Estudios Estad√≠sticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier √Ålvarez Li√©bana</strong>](...) ‚Ä¢ Grado en Ciencia de Datos Aplicada (UCM) ‚Ä¢ curso 2024-2025"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelizaci√≥n

[**Vamos a juntar las piezas del puzzle para hacer ¬´magia¬ª**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¬°Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3¬™ planta).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier √Ålvarez Li√©bana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matem√°ticas (UCM). [**Doctorado en estad√≠stica**]{.hl-yellow} (UGR).

-   Encargado de la [**visualizaci√≥n y an√°lisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Espa√±ola de Estad√≠stica e IO**]{.hl-yellow} y la [**Real Sociedad Matem√°tica Espa√±ola**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estad√≠stica de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matem√°ticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estad√≠stico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicci√≥n lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluaci√≥n

-   [**Asistencia**]{.hl-yellow}. Se [**valorar√° muy positivamente**]{.hl-purple} la participaci√≥n. 

. . .

- [**Evaluaci√≥n continua**]{.hl-yellow}: 1 **examen te√≥rico a papel** (30%) y **3 entregas R en clase** (10%-25%-35%).
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**M√°s de 7 de continua**]{.hl-purple} -> podr√°s decidir **peso del final entre 0% y 100%**.
  - [**Entre 6 y 7 de continua**]{.hl-purple} -> decidir **peso del final entre un 30% y un 100%**.
  - [**Entre 5 y 6 de continua**]{.hl-purple} ->**peso del final entre un 50% y un 100%**.
  - [**Entre 3.5 y 5 de continua**]{.hl-purple} -> **peso del final entre un 70% y un 100%**.
  - [**Por debajo de 3.5 de continua**]{.hl-purple} -> **peso del final del 100%**.
  
Para que haga [**media el final**]{.hl-red} debes de sacar m√°s de un 3 sobre 10.

---

## Planificaci√≥n


* [**Entrega I (10%)**]{.hl-yellow}: 6 de febrero.

* [**Entrega II (25%)**]{.hl-yellow}: 15 o 22 de abril.

* [**Te√≥rico con papel y boli (30%)**]{.hl-yellow}: 8 de mayo.

* [**Entrega III (35%)**]{.hl-yellow}: 13 de mayo.


---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estar√°n disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia/supervisado-datascience/diapos>** 

En el men√∫ de las diapositivas (abajo a la izquierda) tienes una [**opci√≥n para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que ir√°n modific√°ndose)
  
&nbsp;

* [**Workbooks**]{.hl-yellow}: [**cuadernos de trabajo en casa**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud) y materiales extras

* [**Datos**]{.hl-yellow}: [**acceso a Drive**](https://drive.google.com/drive/folders/1sim9xjM2qRcWrsMHrv_qRlkwDbGKOWN9?usp=sharing)

---


## Requisitos

Para el curso los √∫nicos requisitos ser√°n:

1.  [**Conexi√≥n a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se dar√°n por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se dar√°n por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORAR√Å**]{.hl-purple} la salida html correspondiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estad√≠stico.


---

## Planificaci√≥n  {#planificacion}

::: column-screen-inset-right
::: {style="font-size:20px"}
|  CLASE | SEMANA | FECHAS | TOPIC | EJ. | WORKBOOK | ENTREGA | 
|:------:|:--------:|:--------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
| [1](#clase-1) | S1 | 23 ene | Repaso descriptiva |  | [üê£](#caso-practico-1-1) |  | 
| [2](#clase-2) | S2 | 28 ene | Medidas de asociaci√≥n |  [üíª](#tu-turno-2-1)  [üíª](#tu-turno-2-2) | [üê£](#caso-practico-2-1) [üê£](#caso-practico-2-2) [üê£](#caso-practico-2-3) |  | 
| [3-4](#clase-3) | S2-3 | 30 ene y 4 feb  | An√°lisis de la varianza | [üíª](#tu-turno-3-1)  | [üê£](#caso-practico-3-1) [üê£](#caso-practico-3-2) |  | 
| [5](#clase-5) | S3 | 6 feb | Entrega I  | ... |  | [üéØ 10%](#clase-5)  | 
| [6](#clase-6) | S4 | 13 feb  | Primeras regresiones | | [üê£](#caso-practico-6-1) |  | 
| [7](#clase-7) | S5 | 18 feb  | Inferencia | |  |  | 
| [8](#clase-8) | S5 | 20 feb  | Simulaci√≥n | [üíª](#tu-turno-8-1) | [üê£](#caso-practico-8-1)  |  | 
:::
:::



# Clase 1: repaso {#clase-1}

[**Objetivo de la predicci√≥n lineal. Concepto de linealidad. Repaso de estad√≠stica descriptiva (23 de enero de 2025)**]{style="color:#444442;"}



---

## ¬øQu√© es predecir?

En esta asignatura vamos a tratar principalmente lo que se conoce en el aprendizaje estad√≠stico como [**predicci√≥n (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la informaci√≥n aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo ser√° obtener una estimaci√≥n/predicci√≥n lo ¬´mejor posible¬ª haciendo uso de un

&nbsp;


[**modelo supervisado**]{.hl-yellow} de [**predicci√≥n**]{.hl-purple} [**lineal**]{.hl-green} (conocido como regresi√≥n lineal)


---


## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matem√°ticas, la estad√≠stica, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---


## ¬øModelo supervisado?


[**modelo supervisado**]{.hl-yellow} de predicci√≥n lineal

&nbsp;

En esta asignatura veremos el modelo m√°s simple de lo que se conoce como [**aprendizaje estad√≠stico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---


## Aprendizaje ¬øsupervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de modelos:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-yellow}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la informaci√≥n disponible. Ejemplos: regresi√≥n, knn, √°rboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de modelos:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-yellow}: no existe la distinci√≥n entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscar√° patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Modelo predictivo

Dentro del marco de un [**modelo de predicci√≥n supervisada**]{.hl-yellow} tendr√° siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $\mathbf{X}$ ser√°n los [**datos**]{.hl-yellow}

* $f(\cdot)$ ser√° nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} (con la informaci√≥n que tenemos $\mathbf{X}$).

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ ser√°n nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ ser√° el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} $E \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error deber√≠a ser reducido a **algo aleatorio (irreducible)**, aunque en estad√≠stica SIEMPRE nos vamos a equivocar).



---


## Clasificaci√≥n vs predicci√≥n

modelo supervisado de [**predicci√≥n**]{.hl-purple} lineal

&nbsp;

La **regresi√≥n lineal** se enmarca dentro del [**predicci√≥n**]{.hl-purple} supervisada

* [**Predicci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-purple} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificaci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-purple} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, n√∫mero de accidentes). La etiqueta tomar√° un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

üìö Ver ¬´The elements of Statistical Learning¬ª (Hastie et al., 2008)

---

## ¬øQu√© es predecir?


modelo de [**predicci√≥n**]{.hl-purple} lineal


&nbsp;

Es importante que - de momento - distingamos dos conceptos:

* [**Estimaci√≥n**]{.hl-purple}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicci√≥n**]{.hl-purple}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

M√°s adelante los llamaremos ¬´predicci√≥n en train¬ª y ¬´predicci√≥n en test¬ª

---

## ¬øQu√© es la linealidad?


modelo de predicci√≥n [**lineal**]{.hl-green}

&nbsp;

En matem√°ticas decimos que una funci√≥n $f(x)$ es [**lineal**]{.hl-green} cuando se cumple:

* [**Propiedad aditiva**]{.hl-green}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homog√©nea**]{.hl-green}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estad√≠stica llamamos [**modelo de predicci√≥n lineal**]{.hl-yellow} a un modelo que usa la informaci√≥n de covariables $X_1, X_2, \ldots, X_p$, de manera que su informaci√≥n siempre [**se relacionen entre s√≠ con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$



---


## Repaso descriptiva


La estad√≠stica descriptiva es una rama de la estad√≠stica que se dedica a [**recolectar, organizar, presentar y analizar un conjunto de datos**]{.hl-yellow} para describir las caracter√≠sticas y comportamientos de dicho conjunto.

&nbsp;

Adem√°s de para conocer y entender los datos es la fase en la que [**detectaremos errores e incongruencias**]{.hl-yellow}, teniendo muchas veces que hacer una [**depuraci√≥n de datos**]{.hl-yellow}

---

## Recolecci√≥n

La podemos hacer a trav√©s de **encuestas, experimentos, observaciones, registros**, etc. Lo m√°s importante en esta etapa es que los datos sean representativos del fen√≥meno o poblaci√≥n que se estudia. La rama de la estad√≠stica que se dedica a estudiar esta parte del an√°lisis se conoce como [**muestreo**]{.hl-yellow}, y es fundamental para evitar sesgos en la muestra.


![](https://sketchplanations.com/_next/image?url=https%3A%2F%2Fimages.prismic.io%2Fsketchplanations%2Ff2fdb7cb-f126-4897-ad78-4fd11c743172_SP%2B723%2B-%2BSampling%2Bbias.png%3Fauto%3Dcompress%2Cformat&w=828&q=75)

---

## Conceptos b√°sicos

En estad√≠stica es fundamental entender los conceptos de [**poblaci√≥n, muestra y variable**]{.hl-yellow}, ya que son la base para cualquier an√°lisis estad√≠stico.

. . .

-   [**Poblaci√≥n**]{.hl-yellow}

La poblaci√≥n es el **conjunto completo de elementos o individuos** sobre los cuales se desea obtener informaci√≥n. En la mayor√≠a de casos el acceso a la **totalidad de la poblaci√≥n es inviable** por motivos econ√≥micos, legales o √©ticos, as√≠ que en la mayor√≠a de situaciones las conclusiones deberemos sacarlas haciendo uso de una **muestra**.

. . .

**Ejemplo**: la diferencia entre censo y encuesta es que el primero recopila datos de todos los individuos de una poblaci√≥n, mientras que el segundo trata de estimarlos o inferirlos a partir de una muestra representativa de la misma.

---

## Conceptos b√°sicos

[**Muestra**]{.hl-yellow}: subconjunto de la poblaci√≥n que se selecciona para su an√°lisis con el fin de hacer inferencias o generalizaciones sobre la poblaci√≥n completa. La muestra debe ser **representativa de la poblaci√≥n**.

-   **Muestreo aleatorio simple**: cada miembro de la poblaci√≥n tiene la misma probabilidad de ser seleccionado.

-   **Muestreo estratificado**: la poblaci√≥n se divide en subgrupos (estratos) y se toma una muestra de cada uno.

-   **Muestreo (no aleatorio) sistem√°tico**: se selecciona cada n-√©simo miembro de la poblaci√≥n.

-   **Muestreo (no aleatorio) por cuotas**: se seleccionan aquellos individuos que cumplan ciertas condiciones.

-   **Muestreo por conveniencia**: se elige a los miembros que son m√°s f√°ciles de acceder, aunque este m√©todo puede introducir sesgos.

---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selecci√≥n**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

---


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo m√°s famoso es el caso [**¬´Dewey defeats Truman¬ª (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abri√≥ el Chicago Tribune en 1948, el mismo d√≠a en el que Truman gan√≥ al rep√∫blicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telef√≥nica (sin contar con el sesgo que, en aquella √©poca, solo la clase alta ten√≠a tel√©fono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¬øD√≥nde reforzar√≠as los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selecci√≥n) aparece cuando se toma una muestra de un fen√≥meno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---


## Conceptos b√°sicos

[**Variable**]{.hl-yellow}: **cualquier caracter√≠stica o atributo** que puede tomar diferentes valores entre los individuos de la poblaci√≥n o muestra. Las variables pueden ser de varios tipos seg√∫n su naturaleza:

-   [**Cualitativas (o categ√≥ricas)**]{.hl-purple}: describen cualidades o categor√≠as. Ejemplos:

    -   Nominales: no tienen un orden intr√≠nseco (e.g., g√©nero, estado civil, religi√≥n, etc).
    -   Ordinales: tienen un orden intr√≠nseco (e.g., niveles de satisfacci√≥n, grado acad√©mico, sano-leve-grave, tramo etario, tramo de ingresos, etc).

---

## Conceptos b√°sicos

[**Variable**]{.hl-yellow}: **cualquier caracter√≠stica o atributo** que puede tomar diferentes valores entre los individuos de la poblaci√≥n o muestra. Las variables pueden ser de varios tipos seg√∫n su naturaleza:

-   [**Cuantitativas**]{.hl-purple}: describen cantidades y pueden ser medidas num√©ricamente. Ejemplos:

    -   Discretas finitas: toman valores finitos (e.g., n√∫mero de hijos, n√∫mero de visitas al m√©dico, escala de dolor).
    -   Discretas infinitas: toman valores infinitos (o que se podr√≠an considerar como tal) pero podemos enumerarlas y sabemos siempre el siguiente elemento (e.g., n√∫mero de pelos de nuestra cabellera, n√∫mero de personas que pueden entrar en una tienda en un periodo dado).
    -   Continuas: pueden tomar cualquier valor dentro de un rango (e.g., altura, peso, tiempo de espera).

---

## Conceptos b√°sicos

[**Modalidades**]{.hl-yellow}: uno de los **posibles valores** que toma una **variable dentro de una muestra**. El **conjunto de modalidades posibles** que podr√≠a haber tomado (en tu poblaci√≥n) se suele conocer tambi√©n como soporte. Algunos ejemplos en funci√≥n del tipo de variables son:

-  **Cualitativa nominal (color de ojos)**: negro, azul y marr√≥n (3 modalidades en esa muestra de un espectro de colores m√°s amplio que podr√≠amos tener como soporte).

- **Cualitativa ordinal (estado del paciente)**: sano, leve y grave (3 modalidades en esa muestra de un conjunto de opciones - por ejemplo, sano, leve, grave, UCI, fallecido - que podr√≠amos tener).


---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)



---

## Repaso: medidas de centralizaci√≥n

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tama√±o muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geom√©tricamente**]{.hl-purple}: es el **valor ¬´m√°s cercano¬ª de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* F√°cil de calcular y entender
* F√°cil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores at√≠picos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralizaci√≥n

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco m√°s robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenaci√≥n)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralizaci√≥n

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores m√°s repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gr√°ficamente**]{.hl-purple}: representa el ¬´pico¬ª de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---


## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios-oculto.jpg)

**¬øCu√°l es la mediana, la media y la moda?**

---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersi√≥n

![](img/iker-jimenez.jpg)

¬øQu√© tiene que ver la imagen con la dispersi√≥n?


---

## Repaso: medidas de dispersi√≥n

![](img/extremos.jpg)

El cambio clim√°tico no solo es porque aumente la [**temperatura media (centralizaci√≥n)**]{.hl-yellow} sino por la aparici√≥n cada vez m√°s frecuente de fen√≥menos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} ‚Üí aumento de la [**DISPERSI√ìN**]{.hl-yellow}

---

## Repaso: medidas de dispersi√≥n

[**¬øC√≥mo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podr√≠a ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y despu√©s realizar su promedio.

---


## Wait for it

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersi√≥n

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¬øCu√°nto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¬øCu√°l es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersi√≥n es 0...[**¬øno hay dispersi√≥n?**]{.hl-red} ¬øNo deber√≠a de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersi√≥n

Para **evitar que se cancelen** los signos lo que haremos ser√° calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matem√°ticas (no es derivable como funci√≥n).
:::

---

## Repaso: medidas de dispersi√≥n


[**Problema**]{.hl-red}: si los datos est√°n en metros, la varianza estar√° en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¬øTiene sentido medir la dispersi√≥n de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersi√≥n

Para tener una [**medida de dispersi√≥n en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviaci√≥n t√≠pica**]{.hl-yellow}, como la ra√≠z cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersi√≥n

Todav√≠a tenemos un peque√±o problema.

Imagina que queremos **comparar la dispersi√≥n de dos conjuntos** de datos, estaturas de personas y di√°metros de n√∫cleos de c√©lulas. Y Supongamos que las medias son 170 cm y 5 micr√≥metros, y la desviaci√≥n t√≠pica de 1 cm y 1.5 micr√≥metros.

[**¬øQu√© conjunto de datos es m√°s disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersi√≥n adimensional** definiremos el [**coeficiente de variaci√≥n**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localizaci√≥n

Las [**medidas de posici√≥n o localizaci√≥n**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tama√±o (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlaci√≥n

[**¬øQu√© es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviaci√≥n: puede ser entendida como una [**medida que cuantifica la relaci√≥n de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

¬øY si qui√©semos medir la [**RELACI√ìN/ASOCIACI√ìN de una variable X respecto a otra variable Y (en lugar de consigo misma)**]{.hl-yellow}?


---

## Repaso: covarianza y correlaci√≥n

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detr√°s de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviaci√≥n de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlaci√≥n

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¬øQu√© cuantifica?**]{.hl-purple} La covarianza mide la [**asociaci√≥n LINEAL**]{.hl-red} (en torno a una recta) entre dos variables [**CONTINUAS**]{.hl-red}

. . .

* [**¬øQu√© dice su signo?**]{.hl-purple} El signo de la covarianza nos indicar√° la [**direcci√≥n de la dependencia lineal**]{.hl-yellow}: si es positiva, la relaci√≥n ser√° creciente (cuando X crece, Y crece); si es negativa, la relaci√≥n ser√° decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlaci√≥n

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, as√≠ que lo que haremos ser√° [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlaci√≥n lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones t√≠picas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre est√°n entre -1 y 1**]{.hl-yellow}

* m√°s cerca de -1 o 1 ‚Üí relaci√≥n lineal m√°s fuerte
* m√°s cerca de 0 ‚Üí ausencia de relaci√≥n **LINEAL**

---

## Repaso: covarianza y correlaci√≥n

![](img/correlaciones.jpg)


---

## üê£ Caso pr√°ctico I: anscombe {#caso-practico-1-1}

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

&nbsp;

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-i-anscombe)


# Clase 2: asociaci√≥n entre variables {#clase-2}

[**Causalidad vs dependencia. Asociaci√≥n continua vs continua, cualitativa vs cualitativa (28 de enero de 2025)**]{style="color:#444442;"}


---


## Estad√≠stica bivariante

Todo lo que hemos hecho con una variable podemos hacerlo tambi√©n de manera [**bivariante**]{.hl-yellow} considerando dos variables.

. . .

Uno de los principales objetivos de la estad√≠stica bivariante es [**determinar si existe relaci√≥n o dependencia entre dos variables**]{.hl-yellow}, es decir, cuando un cambio en el valor de una de ellas se asocia a un cambio en el de la otra (una [**dependencia estad√≠stica no implica un efecto causal**]{.hl-red}).

. . .

La situaci√≥n contraria, es decir, la ausencia de relaci√≥n, se denomina [**independencia**]{.hl-yellow}.


---

## Tipos de an√°lisis posibles

Una primera aproximaci√≥n al estudio de dos variables ser√° [**clasificar el tipo de an√°lisis**]{.hl-yellow}

- [**Cuali vs cuali**]{.hl-yellow}:
  - **Resumen**: tablas de contigencia (frecuencia cruzada).
  - **Inferencia**: prueba $\chi^2$ de independencia o test de Fisher.
  - **Gr√°ficos**: barras apiladas, gofres, gr√°ficos de ¬´flujo¬ª.

---

## Tipos de an√°lisis posibles

Una primera aproximaci√≥n al estudio de dos variables ser√° [**clasificar el tipo de an√°lisis**]{.hl-yellow}


- [**Cuani vs cuanti**]{.hl-yellow}:
  - **Resumen**: covarianza y correlaci√≥n.
  - **Inferencia**: test de correlaci√≥n (relaci√≥n lineal) y test de Kolmogorov-Smirnov (¬øambas distribuciones son iguales?). Test de igualdad de medias o igualdad de varianzas
  - **Gr√°ficos**: diagrama de dispersi√≥n, correlogramas, heatmaps.

---

## Tipos de an√°lisis posibles

Una primera aproximaci√≥n al estudio de dos variables ser√° [**clasificar el tipo de an√°lisis**]{.hl-yellow}


- [**Cuanti vs cuali**]{.hl-yellow}:
  - **Resumen**: medidas de centralizaci√≥n/dispersi√≥n/posici√≥n de la cuanti desagregado por los grupos de la cuali.
  - **Inferencia**: ANOVA (una v√≠a, dos v√≠as, ...). Test de igualdad de medias o igualdad de varianzas (desagregada por grupos)
  - **Gr√°ficos**: boxplots, gr√°ficos de viol√≠n (desagregados por grupos)


---

## Repaso: inferencia

¬øPero que era eso de la [**inferencia estad√≠stica**]{.hl-yellow}? Es un conjunto de m√©todos y t√©cnicas que permite [**inferir conclusiones sobre una poblaci√≥n a partir de una muestra**]{.hl-yellow} de datos.

. . .

Su prop√≥sito es utilizar la informaci√≥n muestral para estimar caracter√≠sticas de la poblaci√≥n, probar hip√≥tesis y realizar predicciones, basado en el c√°lculo de [**estad√≠sticos**]{.hl-yellow}

-   [**Par√°metro**]{.hl-yellow}: medida que describe una caracter√≠stica de la **poblaci√≥n** (ejemplo: la media poblacional $\mu$ de la estatura de las mujeres en Espa√±a).

-   [**Estad√≠stico**]{.hl-yellow}: medida que describe una caracter√≠stica de la **muestra** (ejemplo: la media muestral $\overline{x}$ de un conjunto de 100 mujeres).

---

## Repaso: inferencia

Haciendo uso de **estad√≠sticos que aproximen una correcta estimaci√≥n de los par√°metros**, los [**contraste de hip√≥tesis**]{.hl-yellow} son procedimientos estad√≠sticos para [**tomar decisiones sobre la validez de una afirmaci√≥n acerca de una poblaci√≥n**]{.hl-yellow} en funci√≥n de los datos muestrales.

. . .

La idea es muy parecido a un **juicio**: con las pruebas (muestra) el jurado (estad√≠stico) deben decidir sobre tu culpabilidad real (poblaci√≥n), pudiendo ser declarado **culpable** o **no culpable**.

. . .

Este proceso implica formula

-   [**Hip√≥tesis nula** $H_0$]{.hl-yellow}: es una afirmaci√≥n generalmente representa una **posici√≥n de no efecto o no diferencia** (ejemplo: entras siendo no culpable a un juicio)

-   [**Hip√≥tesis alternativa** $H_0$]{.hl-yellow}: es una afirmaci√≥n que [**se acepta si se rechaza la hip√≥tesis nula**]{.hl-yellow}. Representa un efecto o diferencia (ejemplo: culpable)

---

## Repaso: inferencia

-   [**Hip√≥tesis nula** $H_0$]{.hl-yellow}: es una afirmaci√≥n generalmente representa una **posici√≥n de no efecto o no diferencia** (ejemplo: entras siendo no culpable a un juicio)

-   [**Hip√≥tesis alternativa** $H_1$]{.hl-yellow}: es una afirmaci√≥n que [**se acepta si se rechaza la hip√≥tesis nula**]{.hl-yellow}. Representa un efecto o diferencia (ejemplo: culpable)

La idea es similar a la del juicio: solo vamos a [**rechazar $H_0$ (es decir, aceptar $H_1$) si hay MUCHAS EVIDENCIAS en la muestra**]{.hl-yellow} (solo se condena culpable a una persona si hay muchas evidencias que demuestran su culpabilidad, pero el acusado no tiene que demostrar su inocencia).

. . .

Llamaremos [**nivel de significancia** $\alpha$]{.hl-yellow} a la probabilidad de [**rechazar la hip√≥tesis nula cuando es verdadera**]{.hl-red} (condenar a un inocente, conocido como **error tipo I**. Normalmente $\alpha = 0.05$ aunque se pueden usar otros valores como 0.01 o 0.10 (a decidir ANTES de realizar el contraste.)



---

## p valor

El conocido como [**p-valor**]{.hl-yellow} es uno de los conceptos m√°s importantes en estad√≠stica pero tambi√©n peor usados. Puedes ver toda una revisi√≥n de qu√© significa y qu√© no en <https://pmc.ncbi.nlm.nih.gov/articles/PMC4877414/>

. . .

Podemos definir el [**p-valor**]{.hl-yellow} como un valor continuo que nos mide la [**compatibilidad de los datos observados con el modelo e hip√≥tesis asumidas**]{.hl-yellow}: 1 indica compatibilidad perfecta y 0 incompatibilidad completa.

* [**No repesenta la probabilidad de que la hip√≥tesis nula sea cierta**]{.hl-red}: el propio p-valor se calcula ASUMIENDO que lo es.

* [**No representa la probabilidad de que, por azar, se produzca nada**]{.hl-red}

---

## Asociaci√≥n cuali vs cuali

Una vez visto conceptos b√°sicos de inferencia vamos a empezar por un [**an√°lisis bivariante de dos variables cualitativas**]{.hl-yellow}

. . .

El primer paso siempre ser√° intentar resumir la informaci√≥n mediante el uso de [**tablas de contigencia**]{.hl-yellow}, en este caso bidimensionales.

. . .

¬øC√≥mo lo har√≠as con **tidyverse**? ¬øY con **R base**?

---

## Tablas de contigencia

Vamos a tomar la base de datos `SatisfaccionPacientes.csv` que captura datos de una encuesta de satisfacci√≥n de pacientes en un hospital

```{r}
library(readr)
datos <-
  read_csv(file = "./datos/SatisfaccionPacientes.csv") |> 
  janitor::clean_names()
datos
```

---

## Tablas de contigencia

Para calcular una [**tabla bidimensional de frecuencias**]{.hl-yellow} en tidyverse basta con **indicar dos variables en `count()`**

```{r}
datos |>
  count(genero, estado_civil)
```

---

## Tablas de contigencia

Lo habitual es mostrar esta tabla como una [**tabla con m filas y n columnas**]{.hl-yellow}, siendo $m$ el n√∫mero de modalidades distintas de la primera variable (en este caso $m=2$, femenino y masculino) y $n$ el n√∫mero de modalidades distintas de la segunda variable (en este caso $n = 4$). 

¬øC√≥mo hacer que la variable `estado_civil` pivote para pasar de estar en vertical a estar ¬´en horizontal¬ª? (echa un repaso a la parte de tidy data)

. . .

```{r}
datos |>
  count(genero, estado_civil) |> 
  pivot_wider(names_from = estado_civil, values_from = n)
```

---

## Tablas de contigencia


Esto se puede hacer **mucho m√°s sencillo de nuevo en `R base`** con `table()`


```{r}
table(datos$genero, datos$estado_civil)
```

---

## Tablas de contigencia


F√≠jate que ahora podemos [**normalizar las frecuencias de 3 formas**]{.hl-yellow}: respecto al total de los datos, por filas (`margin = 1`) o por columnas (`margin = 2`).

```{r}
prop.table(table(datos$genero, datos$estado_civil))
prop.table(table(datos$genero, datos$estado_civil), margin = 1)
prop.table(table(datos$genero, datos$estado_civil), margin = 2)

```

---

## Tablas de contigencia

Haciendo uso de las tablas anteriores intenta responder a las siguientes preguntas:

a) ¬øQu√© cantidad de pacientes mujeres est√°n solteras?

b) ¬øQu√© porcentaje, de entre los pacientes hombres, est√°n viudos?

c) ¬øQu√© porcentaje, de entre los que est√°n divorciados, son mujeres?

d) ¬øQu√© porcentaje (del total de pacientes) son hombres solteros?


```{r}
#| code-fold: true
# a) 22 mujeres
# b) 8.51%
# c) 57.89%
# d) 20%
```


---

## Tablas de contigencia

Puedes incluso visualizar dichas cantidades con `geom_tile()` indic√°ndole que el relleno dependa del conteo `n`

```{r}
datos |>
  count(genero, estado_civil) |>
  ggplot() +
  geom_tile(aes(x = genero, y = estado_civil, fill = n)) +
  theme_minimal()
```

---

## üíª Tu turno {#tu-turno-2-1}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset
### [**Ejercicio 1**]{.hl-yellow}

Para repasar lo aprendido vamos a poner todo en pr√°ctica con el dataset `SatisfaccionPacientes.csv`. 

```{r}
library(readr)
datos <-
  read_csv(file = "./datos/SatisfaccionPacientes.csv") |> 
  janitor::clean_names()
```


üìù Aplica el c√≥digo que sea necesario para responder a estas preguntas. ¬øCu√°l es el tama√±o muestral? ¬øCu√°ntas variables tenemos? ¬øCu√°ntas modalidades tenemos en la variable `estado_civil` (y cuantas observaciones en cada una)?

```{r}
#| code-fold: true
#| eval: false
# Tama√±o muestral / n√∫mero de observaciones
n <- nrow(datos)

# N√∫mero de variables
p <- ncol(datos)

# ¬øQu√© modalidades tenemos?
datos |>  count(estado_civil)
```


### [**Ejercicio 2**]{.hl-yellow}

üìù Determina el tipo de variable (cuantitativa vs. cualitativa).

```{r}
#| code-fold: true
#| eval: false
# Variables cuantitativas: tiempo, grado satisfacci√≥n, n√∫mero de visitas
# Variables cualitativas: g√©nero, estado civil, estado salud
glimpse(datos)
```

### [**Ejercicio 3**]{.hl-yellow}

üìù  Obten tablas de frecuencias (absoluta y relativa) en el caso de las cualitativas NOMINALES. Con ella intenta responder a las preguntas: a) ¬øcu√°ntas mujeres hay? b) ¬øqu√© % de individuos est√°n casados?

```{r}
#| code-fold: true
#| eval: false
# no podemos calcular acumulados ya que genero es nominal
datos |>  count(genero) |> 
  rename(frecuencia_abs = n) |> 
  mutate(frecuencia_rel = frecuencia_abs/sum(frecuencia_abs))
# Hay 53 mujeres

datos |> count(estado_civil) |> 
  rename(frecuencia_abs = n) |> 
  mutate(frecuencia_rel = frecuencia_abs/sum(frecuencia_abs))
# Hay 26% personas casadas
```


### [**Ejercicio 4**]{.hl-yellow}

üìù Convierte de manera adecuada la variable `genero` y `estado_civil` a cualitativa nominal

```{r}
#| code-fold: true
datos <-
  datos |>
  mutate(estado_civil = factor(estado_civil),
         genero = factor(genero))
```

### [**Ejercicio 5**]{.hl-yellow}

üìù Calcula la media, mediana, rango intercuart√≠lico y desviaci√≥n t√≠pica de edad y tiempo de espera.

```{r}
#| code-fold: true
resumen <-
  datos |>
  summarise(media_edad = mean(edad), sd_edad = sd(edad), mediana_edad = median(edad),
           IQR_edad = quantile(edad, probs = 0.75) - quantile(edad, probs = 0.25),
           # tiempo espera
           media_tiempo_espera = mean(tiempo_espera), sd_tiempo_espera = sd(tiempo_espera),
           mediana_tiempo_espera = median(tiempo_espera),
           IQR_tiempo_espera = quantile(tiempo_espera, probs = 0.75) - quantile(tiempo_espera, probs = 0.25))
```

### [**Ejercicio 6**]{.hl-yellow}

üìù Repite el anterior ejercicio pero obteniendo las m√©tricas desagregadas por sexo.

```{r}
#| code-fold: true
resumen <-
  datos |>
  summarise(media_edad = mean(edad), sd_edad = sd(edad), mediana_edad = median(edad),
           IQR_edad = quantile(edad, probs = 0.75) - quantile(edad, probs = 0.25),
           # tiempo espera
           media_tiempo_espera = mean(tiempo_espera), sd_tiempo_espera = sd(tiempo_espera),
           mediana_tiempo_espera = median(tiempo_espera),
           IQR_tiempo_espera = quantile(tiempo_espera, probs = 0.75) - quantile(tiempo_espera, probs = 0.25),
          .by = genero)
```

### [**Ejercicio 7**]{.hl-yellow}

üìù Realiza un gr√°fico de viol√≠n para la variable `tiempo_espera` para cada g√©nero

```{r}
#| code-fold: true
#| eval: false
ggplot(datos) +
  geom_violin(aes(x = genero, y = tiempo_espera, fill = genero, color = genero),
              alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
```

:::

---

## üê£ Caso pr√°ctico I: encuesta de satisfacci√≥n {#caso-practico-2-1}

Vamos a seguir poniendo en pr√°ctica lo aprendido el dataset `SatisfaccionPacientes.csv`

&nbsp;

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-i-encuesta-de-satisfacci%C3%B3n)


---

## Asociaci√≥n cuali vs cuali

Esas tablas de frecuencia ser√°n las que usen los [**diferentes contrastes de asociaci√≥n cuali vs cuali**]{.hl-yellow} para decidir si hay o no dependencia.

&nbsp;

El contraste m√°s conocido es la conocida como [**prueba de $\chi^2$ (chi-cuadrado)**]{.hl-yellow}: dada una tabla de contigencia entre dos cualitativas, el contraste [**compara dicha tabla con la que deber√≠amos obtener bajo la hip√≥tesis nula de independencia**]{.hl-yellow}

Vamos a hacerlo con nuestras variables `genero` y `estado_civil`

---

## Prueba chi-cuadrado

$$H_0:~\text{genero y estado civil son independientes}$$

$$H_1:~\text{genero y estado civil son dependientes}$$

```{r}
table(datos$genero, datos$estado_civil)
```

. . .

[**Si la hip√≥tesis nula fuese cierta**]{.hl-yellow}, ¬øqu√© esperar√≠amos?

---

## Prueba chi-cuadrado

1. Elegimos uno de los factores y calculamos su **proporci√≥n en la tabla general** (53% vs 47% en este caso)

```{r}
prop.table(table(datos$genero))
```

. . .

2. Calculamos la **tabla de contigencia**  de ambas variables obteniendo lo que denotaremos como [**frecuencias observadas $O_{ij}$**]{.hl-yellow}

```{r}
table(datos$genero, datos$estado_civil)
```

---

## Prueba chi-cuadrado


3. **Si ambas variables fuesen independientes**, en cada columna tendr√≠amos que tener **porcentajes parecidos a cuando lo hacemos sin desagregar** (53% mujeres y 47% hombres). Es decir, del total de casados (26) deber√≠amos tener $8.48$ mujeres y $7.52$ hombres; del total de divorciados (19) deber√≠amos tener $10.07$ mujeres y $8.93$ hombres; y as√≠ sucesivamente. Estas frecuencias las denotaremos como [**frecuencias esperadas $E_{ij}$**]{.hl-yellow}

$$E_{ij} = \frac{\text{suma fila i * suma fila j}}{\text{total}}$$


---

## Prueba chi-cuadrado


4. [**Resumimos lo que se desv√≠a una de otra**]{.hl-yellow} mediante el [**estad√≠stico chi-cuadrado**]{.hl-yellow}:

$$\begin{eqnarray}\chi^2 &=& \sum_{i,j} \frac{\left(O_{ij} - E_{ij} \right)^2}{E_{ij}} = \frac{(13.78 - 11)^2}{13.78} + \frac{(12.22 - 15)^2}{12.22} \nonumber \\
&+& \frac{(10.07 - 11)^2}{10.07} + \ldots + \frac{(6.11 - 4)^2}{6.11} = 2.75731\end{eqnarray}$$

. . .


5. Calculamos [**c√≥mo de extremo es el valor del estad√≠stico si la hip√≥tesis nula fuese cierta**]{.hl-yellow}, proporcion√°ndonos un **p-valor**.

---

## Prueba chi-cuadrado

Este proceso podemos hacerlo directamente aplicando `chisq.test()`, indic√°ndole las variables (o su tabla de frecuencias)

```{r}
contraste <- chisq.test(datos$genero, datos$estado_civil)
```

* `...$statistic`: tenemos guardado el valor del estad√≠stico

```{r}
contraste$statistic
```

* `...$observed`: tenemos guardada la tabla de frecuencias observada

```{r}
contraste$observed
```

---

## Prueba chi-cuadrado


* `...$expected`: tenemos guardada la tabla de frecuencias esperada

```{r}
contraste$expected
```

* `...$p.value`: tenemos guardado el p-valor.

```{r}
contraste$p.value
```

---

## Prueba chi-cuadrado


¬øC√≥mo [**interpretar el contraste**]{.hl-yellow}?

```{r}
contraste
```

Como $p.value = 0.4306 > \alpha = 0.05$, no podemos rechazar la hip√≥tesis nula: [**no hay evidencias suficientes en la muestra para concluir que haya dependencia**]{.hl-yellow}.

---

## Prueba de Fisher

Otra alternativa es el [**test exacto de Fisher**]{.hl-yellow}, una prueba estad√≠stica utilizada para [**determinar si hay una asociaci√≥n significativa entre dos variables cualitativas**]{.hl-yellow} especialmente √∫til cuando las frecuencias esperadas son bajas y tenemos dos grupos en cada cualitativa (la tabla de frecuencias es $2 \times 2$).

&nbsp;

Como **curiosidad** dicha prueba naci√≥ cuando Fisher trataba de comprobar si una compa√±era, Muriel Birstol, era capaz de detectar en un t√© con leche si se hab√≠a a√±adido primero el t√© o la leche en su taza (y del experiemnto del que naci√≥ la regla del $\alpha = 5%$).

. . .

Para aplicarlo nos basta con usar `fisher.test()`.

```{r}
fisher.test(datos$genero, datos$estado_civil)
```

---

## Prueba de Fisher

Como hemos dicho es especialmente √∫til cuando tenemos solo 2 modalidades en cada cualitativa ya que nos proporciona [**m√©tricas de asociaci√≥n**]{.hl-yellow}

Veamos un ejemplo con la tabla `placebo_medicamento.csv`

```{r}
datos_placebo <- read_csv(file = "./datos/placebo_medicamento.csv")
datos_placebo
```

---

## Prueba de Fisher


```{r}
fisher.test(datos_placebo$observado, datos_placebo$grupo_tratamiento)
```

Si te fijas ahora nos devuelve adem√°s un [**contraste de lo que se conoce como odds ratio (OR: raz√≥n de probabilidades)**]{.hl-yellow}

`alternative hypothesis: true odds ratio is not equal to 1`


---

## M√©tricas de asociaci√≥n

```{r}
table(datos_placebo$observado, datos_placebo$grupo_tratamiento)
```

La interpretaci√≥n de [**Odds ratio (OR)**]{.hl-yellow} es cuantificar la [**asociaci√≥n entre dos variables respecto a una asociaci√≥n esp√∫rea**]{.hl-yellow} ¬øCu√°nto [**mejoran los que tomaron medicamento respecto a una posible mejora basal**]{.hl-yellow} (aleatoria) del placebo?

* [**Ratio de mejora en tratados**]{.hl-purple}: $13/3 = 4.33333$
* [**Ratio de mejora en placebo**]{.hl-purple}: $6/11 = 0.54545$

$$OR = \frac{13/3}{6/11} = \frac{13*11}{6*3} = 7.94$$

Los pacientes sometidos a tratamiento mejoran 7.9 veces m√°s si el placebo mejorase por azar.

---

## M√©tricas de asociaci√≥n

```{r}
table(datos_placebo$observado, datos_placebo$grupo_tratamiento)
```

Otra de las m√©tricas habituales es la conocida como [**raz√≥n de prevalencias (Risk Ratio, RR)**]{.hl-yellow} que nos proporciona un [**ratio entre la probabilidad de prevalencia**]{.hl-yellow} de un evento en dos grupos.


* [**Prevalencia de mejora en tratados**]{.hl-purple}: $13/(3+13) = 0.8125$
* [**Prevalencia de mejora en placebo**]{.hl-purple}: $6/(11+6) = 0.35294$

$$RR = \frac{13/(3+13)}{6/(11+6)} = \frac{13*11}{6*3} = 2.30208$$
Los pacientes sometidos a tratamiento tienen m√°s del doble de ¬´riesgo¬ª de mejorar que los pacientes con placebo.

---

## M√©tricas de asociaci√≥n

Ambas m√©tricas podemos estimarlas tambi√©n con el paquete `{epitools}`

```{r}
library(epitools)
OR <- oddsratio(datos_placebo$observado, datos_placebo$grupo_tratamiento)
OR$measure
```

* Si $OR = 1$ no hay asociaci√≥n entre las variables.
* Si $OR > 1$ hay una asociaci√≥n positiva, es decir, la exposici√≥n est√° asociada con un mayor riesgo.
* Si $OR < 1$ hay una asociaci√≥n negativa, es decir, la exposici√≥n est√° asociada con un menor riesgo.


---

## M√©tricas de asociaci√≥n


```{r}
RR <- riskratio(datos_placebo$observado, datos_placebo$grupo_tratamiento)
RR$measure
```


* Si $RR = 1$ no hay diferencias en el riesgo entre los grupos.
* Si $RR > 1$ el grupo expuesto (en este caso medicado) tiene mayor riesgo (en este caso de mejorar)
* Si $RR < 1$ el grupo expuesto tiene menor riesgo.

---

## Gr√°ficos de barras

Volvamos al ejemplo de encuesta de satisfacci√≥n: vamos a intentar relacionar las dos variables cualitativas `genero` y `estado_civil` para **complementar el an√°lisis num√©rico realizado** (am√©n del `geom_tile()` que hemos hecho para visualizar la tabla de frecuencias)

. . .

Sabemos realizar un diagrama de barras de cada una por separado, [**¬øc√≥mo incluir la informaci√≥n de ambas con `geom_bar()`**]{.hl-yellow}

. . .

Piensa c√≥mo hacerlo recordando que `geom_bar()` solo admite una coordenada `x = ...` o `y = ...`. ¬øC√≥mo incluir la info de otra variable que no sea en `x` o `y`?

---

## Gr√°ficos de barras

```{r}
#| code-fold: true
ggplot(datos) +
  geom_bar(aes(x = estado_civil, fill = genero), alpha = 0.6) +
  ggthemes::scale_fill_colorblind() +
  labs(x = "Estado civil", y = "Frec. absolutas",
       fill = "G√©nero") +
  theme_minimal()
```

---

## Gr√°ficos de barras

La funci√≥n `geom_bar()` nos permite jugar un poco con el tipo de barras, que por defecto las muestra `stacked` (apiladas). Dicho ajuste podemos **cambiarlo con el argumento `position`**: si `position = "dodge"` las muestra de [**manera agrupada una detr√°s de otra**]{.hl-yellow}.

```{r}
#| code-fold: true
ggplot(datos) +
  geom_bar(aes(x = estado_civil, fill = genero),
           position = "dodge", alpha = 0.6) +
  ggthemes::scale_fill_colorblind() +
  labs(x = "Estado civil", y = "Frec. absolutas",
       fill = "G√©nero") +
  theme_minimal()
```

---

## Gr√°ficos de barras

La mejor opci√≥n para visualizar si hay asociaci√≥n es que **cada barra de estado civil representa el total y nos muestre el % de cada sexo** en cada una: si fuesen independientes, el reparto por sexo en cada barra deber√≠a ser similar. Lo haremos con `position = "fill"`

```{r}
#| code-fold: true
ggplot(datos) +
  geom_bar(aes(x = estado_civil, fill = genero),
           position = "fill", alpha = 0.6) +
  ggthemes::scale_fill_colorblind() +
  labs(x = "Estado civil", y = "Frec. relativas",
       fill = "G√©nero") +
  theme_minimal()
```


---

## Gr√°ficos de barras

```{r}
#| code-fold: true
ggplot(datos) +
  geom_bar(aes(y = estado_civil, fill = genero),
           position = "fill", alpha = 0.6) +
  ggthemes::scale_fill_colorblind() +
  labs(x = "Estado civil", y = "Frec. relativas",
       fill = "G√©nero") +
  theme_minimal()
```


---

## üíª Tu turno {#tu-turno-2-2}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset
### [**Ejercicio 1**]{.hl-yellow}

üìù Carga el fichero `placebo_medicamento_completo.csv` donde tenemos guardado los niveles de colesterol antes y despu√©s de un tratamiento: a 76 personas se les dio un medicamento para bajarlo y a 24 personas placebo.

```{r}
#| code-fold: true
datos <- read_csv(file = "./datos/placebo_medicamento_completo.csv")
```


### [**Ejercicio 2**]{.hl-yellow}

üìù A√±ade una nueva variable dicot√≥mica a los datos que nos guarde `mejora` si el paciente mejor√≥ tras el tratamiento y `no mejora` en caso negativo

```{r}
#| code-fold: true
datos <-
  datos |> 
  mutate("mejora" = if_else(colesterol_post <= colesterol_pre, "mejora",
                            "no mejora"))
```

### [**Ejercicio 3**]{.hl-yellow}

üìù Visualiza ambas variables (`mejora` y `tratamiento`) a la vez con un diagrama de barras de manera que podamos observar indicios de una posible independencia o dependencia entre ambas. Hazlo antes a papel y boli si lo necesitas

```{r}
#| code-fold: true
#| eval: false

# as√≠ pintar√≠amos en cada barra de tratamiento los mejora o no mejora
ggplot(datos) +
  geom_bar(aes(x = tratamiento, fill = mejora), alpha = 0.6) +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()

# pero dado que tienes m√°s tratados que del grupo control
# no permite comparar bien as√≠ que igualamos las barras
# para que cada barra sea el 100% de su categor√≠a
ggplot(datos) +
  geom_bar(aes(x = tratamiento, fill = mejora), alpha = 0.6,
           position = "fill") +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()

# Parece evidente visualmente que hay una diferencia entre mejora y no mejora
# en cada barra
```

### [**Ejercicio 4**]{.hl-yellow}

üìù Calcula la tabla de frecuencias absoluta y relativa que consideres necesarias para responder a las siguientes preguntas:

a) ¬øCu√°ntas personas de las tratadas con medicamento no mejoraron?

b) ¬øQu√© de personas del total del estudio acabaron mejorando habiendo tomando placebo?

c) ¬øQu√© % de personas tom√≥ medicamentos entre los que no mejoraron?

d) ¬øQu√© % de personas de los que tomaron medicamento mejoraron?

```{r}
#| code-fold: true
#| eval: false
table(datos$tratamiento, datos$mejora)
prop.table(table(datos$tratamiento, datos$mejora))
prop.table(table(datos$tratamiento, datos$mejora), margin = 1)
prop.table(table(datos$tratamiento, datos$mejora), margin = 2)
# 9 personas de las tratadas con medicamento no mejoraron
# 9% del total de personas mejoraron y tomaron placebo
# 37% de los que no mejoraron hab√≠an tomado la medicaci√≥n
# 88.1% de los que tomaron medicamento mejoraron
```

### [**Ejercicio 5**]{.hl-yellow}

üìù Para confirmar y cuantificar las evidencias que ya tenemos, vamos a realizar un contraste de hip√≥tesis. Realiza la prueba de chi-cuadrado e interpreta el resultado con $\alpha = 0.05$.

```{r}
#| code-fold: true
#| eval: false
chisq.test(datos$tratamiento, datos$mejora)
# Dado que p-value = 1.654e-06 << alpha --> debemos rechazar la hip√≥tesis nula -->
# hay evidencias suficientes para afirmar que hay relaci√≥n de dependencia
```

### [**Ejercicio 6**]{.hl-yellow}


> Realiza la prueba de chi-cuadrado y Fisher e incluye los p-valores en una tabla resumen haciendo uso de tidyverse. Exporta a un `.csv` dicha tabla resumen

```{r}
#| code-fold: true
resumen_pvalores <-
  datos |> 
  summarise("sig_chisq" = chisq.test(datos$tratamiento, datos$mejora)$p.value,
            "sig_fisher" = fisher.test(datos$tratamiento, datos$mejora)$p.value)
write_csv(resumen_pvalores, file = "./datos/resumen_pvalores.csv")
```

### [**Ejercicio 7**]{.hl-yellow}

üìù Realiza la prueba de Fisher y mira la salida completa. Interpreta la salida, no solo del contraste sino de los odd ratio.

```{r}
#| code-fold: true
fisher.test(datos$tratamiento, datos$mejora)

# OR estimado es de 11.95 --> al ser mayor que 1 implica que
# hay una asociaci√≥n positiva entre las variables
# hay 12 veces m√°s opciones de que te baje el colesterol si tomas el
# medicamento respecto a una posible mejora aleatoria (porque s√≠).
```

:::

---


## üê£ Caso pr√°ctico II: bronquitis y tabaco {#caso-practico-2-2}

Vamos a cargar el archivo de datos `fumadores.csv` donde tenemos datos de 96 pacientes sobre s√≠ o fuman y quienes han desarrollado o no bronquitis.

```{r}
datos <- read_csv(file = "./datos/fumadores.csv")
datos
```

&nbsp;

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-ii-bronquitis-y-tabaco)


---


## üê£ Caso pr√°ctico III: salud mental {#caso-practico-2-3}

Esta la base de datos `datos_salud_mental.csv` tenemos informaci√≥n recopilada de 100 pacientes que acuden a un centro de salud mental. Se quiere realizar un estudio para ver el **impacto que tienen distintas caracter√≠sticas sobre la ansiedad y depresi√≥n** en estos 100 pacientes. Los datos incluyen una variedad de variables relacionadas con la salud mental, as√≠ como caracter√≠sticas demogr√°ficas y de estilo de vida.

```{r}
datos <-
  read_csv(file = "./datos/datos_salud_mental.csv") |> 
  janitor::clean_names()
```

&nbsp;

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-iii-salud-mental)


---

## Asociaci√≥n cuanti vs cuanti


Como dec√≠amos, la idea detr√°s de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desv√≠a cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la funci√≥n `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlaci√≥n lineal

Vamos a practicar una vez m√°s como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Correlaci√≥n lineal

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ¬∫C)** y el **n√∫mero de d√≠as (variable Y) en el que el nivel de ozono super√≥ las 0.20 ppm (partes por mill√≥n)**

* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlaci√≥n lineal

Repite el ejercicio con pocas l√≠neas de c√≥digo `R`

* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlaci√≥n lineal
Realiza lo que consideres tanto a mano como en `R`

* ¬øExiste alguna **relaci√≥n de dependencia entre las variables**? ¬øDe qu√© tipo? ¬øC√≥mo de fuerte o d√©bil es dicha relaci√≥n? ¬øEn qu√© direcci√≥n es dicha relaci√≥n?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlaci√≥n lineal

No s√© si te has fijado qu√© sucede cuando intentamos [**calcular la covarianza/correlaci√≥n de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables num√©ricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la funci√≥n `cov()` sin m√°s, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendr√° un papel fundamental en estad√≠stica ya que contiene la informaci√≥n (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Adem√°s de ser [**sim√©trica**]{.hl-yellow}...¬øqu√© tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estad√≠sticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos par√°metros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¬øSe te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}



---

## Correlaci√≥n vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlaci√≥n nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlaci√≥n [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada m√°s.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* As√≠ la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlaci√≥n vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlaci√≥n estar√° cercana a cero** (ya que no hay relaci√≥n lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre s√≠**]{.hl-yellow} cuando existe un **patr√≥n num√©rico que las relaciona**

. . .

* [**Independencia implica incorrelaci√≥n**]{.hl-green}
* [**Incorrelaci√≥n NO implica independencia**]{.hl-red}

---

## Correlaci√≥n vs dependencia

![](img/escenarios-covarianza.png)

---


## Test de correlaciones

¬øPero c√≥mo saber que la correlaci√≥n observada es suficientemente peque√±a para considerarse incorreladas?

. . .

Con un [**contraste de correlaciones**]{.hl-yellow} haciendo uso de `cor.test()`


```{r}
cor.test(starwars$mass, starwars$height)
```


---

## Correlaci√≥n de rango

La [**correlaci√≥n (lineal) de Pearson**]{.hl-yellow} asume que las [**variables est√°n distribuidas normalmente**]{.hl-green}, en caso de existir asociaci√≥n tienen una relaci√≥n lineal y no tienen valores at√≠picos.

. . .


Esto significa que [**ANTES de aplicar un test de correlaci√≥n**]{.hl-yellow} deber√≠amos de **comprobar que ambas est√°n distribuidas normalmente**

---

## Contraste de normalidad

1. Antes de nada debemos de eliminar los pares con datos ausentres

```{r}
starwars_sin_NA <- starwars |> drop_na(mass, height)
```

. . .

2. Tras ello podemos chequearlo de manera visual

:::: columns
::: {.column width="50%"}

```{r}
ggplot(starwars) +
  geom_density(aes(x = height)) +
  theme_minimal() 
```

:::

::: {.column width="50%"}

```{r}
ggplot(starwars) +
  geom_density(aes(x = mass)) +
  theme_minimal() 
```

:::

::::

Obviamente ninguna de las dos aparenta ser Gaussiana (por asimetr√≠a y por outliers)

---

## Contraste de normalidad


3. Tambi√©n podemos cotejarlo mediante una [**prueba de inferencia no par√°metrica contastando la normalidad**]{.hl-yellow}

```{r}
library(performance)

# check_normality realiza el Shapiro test
check_normality(starwars_sin_NA$height)
check_normality(starwars_sin_NA$mass)
```

. . .

No deber√≠amos aplicar (al menos no interpretar) el test de correlaciones

---

## Correlaci√≥n de rango


Por ello existen dos alternativas: [**correlaci√≥n de Spearman**]{.hl-yellow} y [**correlaci√≥n de Kendall**]{.hl-yellow} 

&nbsp;

Ambos coeficientes cuantifican no una mera **correlaci√≥n lineal** sino una [**correlaci√≥n de rango**]{.hl-yellow}: cuantifica la **relaci√≥n entre los distintos rankings** de dos variables cuando se ordenan.

---


## Correlaci√≥n de Spearman

El [**coeficiente de correlaci√≥n de Spearman**]{hl-yellow} cuantifica de manera **no param√©trica** la interdependencia entre dos variables aleatorias (tanto continuas como discretas). Cuantifica el [**grado de asociaci√≥n mon√≥tona entre dos variables ordinales o continuas**]{.hl-yellow} 

$$\rho =1-\frac{6\sum D^{2}}{n (n^{2}-1)}$$

donde $D$ es la diferencia entre los correspondientes estad√≠sticos de orden de $x - y$.

```{r}
starwars |> 
  select(mass, height) |> 
  drop_na() |> 
  cor(method = "spearman")
cor.test(starwars$mass, starwars$height, method = "spearman")
```

---

## Correlaci√≥n de Spearman

![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Spearman_fig1.svg/600px-Spearman_fig1.svg.png)

---

## Tau de Kendall


El [**coeficiente de correlaci√≥n de Kendall**]{.hl-yellow} ($\tau$ de Kendall) cuantifica la [**asociaci√≥n ordinal de variables cualitativas ordinales (o cuantis)**]{.hl-yellow} de manera no param√©trica. 

Dados $\left(x_1, y_1\right), \ldots, \left(x_n, y_n\right)$ un conjunto de observaciones, se dice  que $\left(x_{i},y_{i}\right)$ y $\left(x_{j},y_{j}\right)$ (con $i < j$) son un [**par concordante**]{.hl-yellow} si el orden de clasificaci√≥n coincide ($x_i < x_j,~y_i < y_j$ o bien $x_j < x_i,~y_j < y_i$) 

$$\tau =\frac{\text{n pares concordantes} - \text{n pares discordantes}}{n \choose 2}$$

El coeficiente de Kendall suele usarse solo cuando $n$ es peque√±o y hay muchos empates (ver [usos](https://pubmed.ncbi.nlm.nih.gov/10221741/))

```{r}
starwars |> 
  select(mass, height) |> 
  drop_na() |> 
  cor(method = "kendall")
cor.test(starwars$mass, starwars$height, method = "kendall")
```

---

## Correlaci√≥n general

Kendall demostr√≥ en 1970 que tanto $\tau$ de Kendall como $\rho$ de Spearman son casos particulares de un [**coeficiente de correlaci√≥n general**]{.hl-yellow}.

Si tenemos $n$ observaciones, para cada par $\left(x_i, y_j \right)$ podemos definir $a_{ij}$ como un ranking en la variable $x$ de ambas observaciones ($a_{ij} > 0$ si $x_i > y_j$) y $b_{ij}$ como un ranking en la variable $y$  ($b_{ij} > 0$ si $x_i > y_j$), entonces

$$\Gamma = \frac{\sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}b_{ij}}{\sqrt {\left( \sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}^{2} \right) \left( \sum_{i=1}^{n}\sum_{j=1}^{n}b_{ij}^{2} \right)}}$$



---

## üíª Tu turno {#tu-turno-2-3}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset
### [**Ejercicio 1**]{.hl-yellow}

Vamos a tomar de nuevo nuestros datos de satisfacci√≥n de pacientes

```{r}
library(readr)
datos <-
  read_csv(file = "./datos/SatisfaccionPacientes.csv") |> 
  janitor::clean_names()
datos
```

üìù Obt√©n la matriz de correlaciones de Pearson haciendo uso de `cor()`. Luego haz uso de `correlate()` del paquete `{corrr}`

```{r}
#| code-fold: true
#| eval: false
datos |> 
  select(where(is.numeric)) |> 
  cor()

datos |> 
  select(where(is.numeric)) |> 
  corrr::correlate()
```

### [**Ejercicio 2**]{.hl-yellow}

üìù Obt√©n la matriz de correlaciones con kendall y spearman


```{r}
#| code-fold: true
#| eval: false
datos |> 
  select(where(is.numeric)) |> 
  corrr::correlate(method = "spearman")

datos |> 
  select(where(is.numeric)) |> 
  corrr::correlate(method = "kendall")
```

### [**Ejercicio 3**]{.hl-yellow}

üìù Analiza y argumenta, en funci√≥n de los resultados anteriores, la asociaci√≥n entre `edad` y `grado_satisfaccion`, y entre `tiempo_espera` y `grado_satisfaccion`

```{r}
#| code-fold: true
# Vemos que por ejemplo `edad` no correla con `grado_satisfaccion` ($-0.0339$ seg√∫n Pearson) pero `tiempo_espera` tiene una correlaci√≥n negativa ($-0.586$ seg√∫n Pearson) con `grado_satisfaccion`.
```

### [**Ejercicio 4**]{.hl-yellow}

üìù Con el paquete `{corrplot}` visualiza la matriz de correlaciones

```{r}
#| code-fold: true
datos |> 
  select(where(is.numeric)) |> 
  cor() |> 
  corrplot::corrplot(method = "square")
```

### [**Ejercicio 5**]{.hl-yellow}

üìù Investiga el paquete `{GGally}` y a funci√≥n `ggpairs()` para visualizar las correlaciones de todas las variables (salvo `id`)

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos |> select(-id)) +
  theme_minimal()

# cuali vs cuali: pictogramas (con rect√°ngulos)
# cuanti vs cuanti: scatter plot
# cuanti vs cuali: boxplot desagregados
# variable vs s√≠ misma: densidad
```

### [**Ejercicio 6**]{.hl-yellow}

üìù ¬øC√≥mo saber que la correlaci√≥n observada entre `edad` y `grado_satisfaccion` ($-0.0339$) es suficientemente peque√±a para considerarse incorreladas? ¬øC√≥mo saber si la correlaci√≥n entre `tiempo_espera` y `grado_satisfaccion` ($-0.586$) es suficientemente grande para considerar que es **significativa**?


```{r}
#| code-fold: true
#| eval: false
cor.test(datos$edad, datos$grado_satisfaccion)
cor.test(datos$tiempo_espera, datos$grado_satisfaccion)

# En uno el p-valor es bastante alto (**no rechazamos la hip√≥tesis nula de incorrelaci√≥n**) y en otro el p-valor es pr√°cticamente 0 (rechazamos la hip√≥tesis nula ->  **hay evidencias de correlaci√≥n significativa**).

# Ninguna de las 3 es normal as√≠ que lo apropiado ser√≠a
# contrastar la correlaci√≥n de rango
performance::check_normality(datos$edad)
performance::check_normality(datos$grado_satisfaccion)
performance::check_normality(datos$tiempo_espera)
```


:::



# Clases 3-4: an√°lisis de la varianza {#clase-3}

[**Asociaci√≥n cuali vs cuanti: an√°lisis de la varianza  (30 de enero y 4 de febrero de 2025)**]{style="color:#444442;"}

---

## Cuanti vs cuali

Ya conocemos herramientas b√°sicas para analizar la dependencia entre dos variables:

* [**Cuali vs cuali**]{.hl-yellow}: test de Fisher o prueba de $\chi^2$, tablas de contigencia, diagramas de barras.

* [**Cuanti vs cuanti**]{.hl-yellow}: correlaciones y test de correlaciones lineal (Pearson),  correlaciones y test de correlaciones de rango (Spearman y Kendall), diagramas de dispersi√≥n.

¬øPero qu√© sucede con la [**asociaci√≥n cuali vs cuanti**]{.hl-red}?

---

## Cuanti vs cuali

![](https://bookdown.org/edsaul_perez/bioestadaistica_avanzada/Figuras/Imagenes%20del%20libro_page-0006.jpg)

---

## Cuanti vs cuali
 
Imagina que tenemos **$n$ estudiantes** de los que disponemos sus **notas $y_{ij}$** para $j=1,2$ asignaturas alumnos (datos en `notas_1factor_2cat.csv`)

```{r}
datos <- read_csv(file = "./datos/notas_1factor_2cat.csv")
datos
```

&nbsp;

¬øExiste [**dependencia entre la nota y la variable asignatura (cualitativa)**]{.hl-yellow}? ¬øSon [**similares (en promedio) entre las asignaturas**]{.hl-green} o hay una de las dos m√°s sencilla?


---

## An√°lisis de la varianza

Disponemos de 

* [**Variable continua**]{.hl-yellow}: notas de $n$ estudiantes denotadas como $y_{ij}$, con $i=1,\ldots, n$.

* [**Variable cualitativa o factor**]{.hl-yellow}: dos asignaturas $j=1,2$

El objetivo es ver la posible [**asociaci√≥n entre la variable objetivo y el factor**]{.hl-yellow}, es decir, determinar si existe un **efecto del factor sobre el valor esperado de la variable continua**.

. . .

A eso se le conoce como [**an√°lisis de la varianza (ANOVA) o an√°lisis factorial**]{.hl-yellow}, desarrollada por R. Fisher en 1930, y empezaremos por el ejemplo m√°s sencillo de **un factor** (una sola variable cualitativa).

---

## ANOVA: un factor

¬øC√≥mo [**conceptualizar matem√°ticamente**]{.hl-yellow} un modelo que [**asuma que todas las asignaturas tienen medias similares**]{.hl-yellow}?

. . .


$$y_{ij} =  \mu_j + \varepsilon_{ij}, \quad \varepsilon_{ij} \quad \text{perturbaci√≥n aleatoria}$$


donde [**$\mu_j$ representa la media de los estudiantes en la asignatura $j$**]{.hl-yellow} y la [**perturbaci√≥n $\varepsilon_{ij}$ representa la desviaci√≥n**]{.hl-yellow} de la nota en la asignatura $j$ del estudiante $i$ (representan la variabilidad intr√≠nseca del EXPERIMENTO).

. . .

&nbsp;

F√≠jate que el **modelo dice lo siguiente**: las posibles [**diferencias (variabilidad) entre notas**]{.hl-yellow} de distintos alumnos se pueden deber a 

i) un grupo distinto (con su media $\mu_j$), es decir, algo [**EXPLICADO por los grupos**]{.hl-red}

ii) una componente aleatoria (algo [**NO EXPLICADO**]{.hl-purple} por los grupos).

---

## ANOVA: un factor


Esas perturbaciones vamos a asumir que cumplen una serie de [**hip√≥tesis**]{.hl-green}


1. [**Promedio nulo**]{.hl-green}: $E[\varepsilon_{ij}] = 0$ para todo $i,j$ (las desviaciones positivas se compensan con las negativas)

. . .

2. [**Varianza constante (homocedasticidad)**]{.hl-green}: $V[\varepsilon_{ij}] = cte = \sigma^2$ para todo $i,j$ (las desviaciones respecto a la media  son iguales para cada grupo)

. . .

3. [**Normalidad**]{.hl-green}: $\varepsilon_{ij} \sim N$

. . . 

4. [**Incorrelaci√≥n**]{.hl-green}: al ser normales esto autom√°ticamente implica que son [**independientes**]{.hl-green} (conocer la desviaci√≥n de un estudiante no nos da informaci√≥n sobre otro: el **orden de recolecci√≥n no afecta a los resultados**)


$$y_{ij} =  \mu_j + \varepsilon_{ij}, \quad \varepsilon_{ij} \sim N (0, \sigma^2) \quad \Rightarrow \quad  y_{ij} \sim N(\mu_j, \sigma^2)$$

---


## ANOVA: un factor

Vamos a visualizar las dos distribuciones

```{r}
#| code-fold: true
ggplot(datos) +
    geom_density(aes(x = notas, color = asignatura, fill = asignatura),
                 alpha = 0.5) +
   geom_point(aes(x = notas, y = 0, color = asignatura),
              size = 1.7, alpha = 0.85) +
    MetBrewer::scale_color_met_d(palette_name = "Renoir") +
    MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
    theme_minimal()
```

Aunque la distribuci√≥n B est√° m√°s desplazada a la izquierda, la **nota m√°s alta es de la asignatura B**. ¬øEs suficiente para decir que **A distinto de B**?

---


## ANOVA: un factor

$$y_{ij} =  \mu_j + \varepsilon_{ij}, \quad \varepsilon_{ij} \sim N (0, \sigma^2)$$

$$H_0:~\mu_1 = \mu_2 = cte \quad vs \quad H_1:~\mu_1 \neq \mu_2$$

En el caso de que **solo tengamos dos grupos** la soluci√≥n es f√°cil: basta con hacer un [**contraste de medias conocido como prueba t**]{.hl-yellow}

```{r}
t.test(datos |> filter(asignatura == "A") |> pull(notas),
       datos |> filter(asignatura == "B") |> pull(notas))
```

---


## ANOVA: un factor

¬øPero qu√© pasar√≠a si [**en lugar de dos asignaturas tuvi√©semos 6 asignaturas**]{.hl-yellow}?

. . .

El [**problema de hacer contrastes 2 a 2**]{.hl-red} es que tendr√≠amos que hacer ${6 \choose 2} = 15$ comparaciones (1-1, 1-2, ..., 1-6, 2-3, ..., 5-6).

. . .

No solo son muchos contrastes sino que si cada contraste se hace con una **probabilidad individual de error tipo I** de $\alpha = 0.05$, y suponiendo que todas las comparaciones fuesen independientes, la [**probabilidad de que se verifiquen las 15 igualdades de manera conjunta**]{.hl-yellow} ya no es de 0.95 sino $0.95^{15} = 0.463$: **si hacemos muchos contrastes es muy probable que, por azar, alguno salga significativo** (aunque realmente no haya diferencia en las medias)


---

## Correcci√≥n de Bonferroni

Un par√©ntesis: para solventar el [**problema de las comparaciones m√∫ltiples**]{.hl-red} un soluci√≥n habitual (la m√°s sencilla aunque no siempre la mejor) es lo que se conoce como [**correcci√≥n de Bonferroni**]{.hl-yellow}

. . .

Sea $A_i$ el suceso "se rechaza la igualdad de medias de la comparaci√≥n i" (que sucede con probabilidad $\alpha$ si la hip√≥tesis nula es cierta). Y sea $B = \bigcup_i A_i$ el suceso de rechazar alguna de las comparaciones (es decir, no todas son iguales). Entonces, si tenemos $c$ comparaciones

$$P(B) = P(\bigcup_i A_i) \leq \sum_i P(A_i) = c \alpha := \alpha_T$$


. . .

Si queremos que ese **$\alpha_T$ conjunto sea (al menos) de 0.05**, necesitamos que cada $\alpha$ individual sea $\alpha = \alpha_T/c$ (demasiado peque√±o si $c$ crece), o lo que es lo mismo definir un **pvalor ajustado** $pvalue_{j}^{adj} = c*pvalue_j$

---

## ANOVA: un factor

No solo existe un [**problema de comparaciones m√∫ltiples**]{.hl-red} sino que, adem√°s, ¬°no lo necesitamos!

. . .

Nosotros (de momento) **no queremos saber cu√°l de las asignaturas tiene m√°s o menos nota** media sino simplemente contestar a la pregunta: ¬øtodas tienen la misma nota media o [**existe un efecto entre la asignatura y la nota**]{.hl-yellow}?

. . .

Y para responder a dicha pregunta [**no necesitamos chequear 2 a 2**]{.hl-red}: basta con que [**encontremos una que no sea igual**]{.hl-yellow}



---

## ANOVA: un factor

$$y_{ij} =  \mu_j + \varepsilon_{ij}, \quad \varepsilon_{ij} \sim N (0, \sigma^2)$$

La idea es que [**no necesitamos comparar 2 a 2**]{.hl-red}. Si tenemos $c$ grupos sabemos que [**las medias $\mu_j$ ($j=1,\ldots,c$) nunca van a ser exactamente iguales**]{.hl-yellow} aunque el grupo no afecte, debido a la variabilidad intr√≠nseca del experimento $\varepsilon_{ij}$

. . .

¬øQu√© contraste deber√≠amos plantear si queremos [**comprobar si existe un efecto del factor (asignatura) en la variable objetivo (nota)**]{.hl-yellow}?

. . .

Si [**no existiese asociaci√≥n**]{.hl-yellow} entre asignatura y nota, tendr√≠amos que todas las medias ser√≠an similares

$$H_0:~independencia \rightarrow \mu_1 = \mu_2 = \ldots = \mu_c = cte \quad vs \quad H_1:~\text{no todas son iguales}$$

---

## ANOVA: un factor

$$H_0:~independencia \rightarrow \mu_1 = \mu_2 = \ldots = \mu_c = cte \quad vs \quad H_1:~\text{no todas son iguales}$$

¬øQu√© significa que una [**serie de valores sean constantes**]{.hl-yellow}? Si tuvi√©semos una variable $x$ en una tabla, ¬øc√≥mo comprobar f√°cilmente que todos los valores son id√©nticos?

. . .

La forma m√°s inmediata es [**calcular su varianza**]{.hl-yellow} ya que nos cuantifica la **variabilidad**: si la varianza/variabilidad es nula (o muy peque√±a), implica que la variable toma valores constantes.

---

## ANOVA: un factor

$$H_0:~independencia \rightarrow \mu_1 = \mu_2 = cte \quad vs \quad H_1:~\text{no todas son iguales}$$

As√≠ que podemos [**reformular la hip√≥tesis nula**]{.hl-green} en t√©rminos de la varianza: si las medias son iguales, la hip√≥tesis nula es lo mismo que decir que las [**diferencias entre notas (en las distintas asignaturas)**]{.hl-yellow} se debe √∫nicamente a una variabilidad aleatoria (y no una diferencia sistem√°tica), es decir

$$H_0:~\text{variab. notas distintas asignaturas es peque√±a} = \text{"varianza" medias es peque√±a}$$

¬øC√≥mo cuantificar [**variabilidad entre las medias**]{.hl-yellow}?

---

## ANOVA: un factor


$$H_0:~\text{variab. notas distintas asignaturas es peque√±a} = \text{"varianza" medias es peque√±a}$$


Para [**cuantificar la variabilidad**]{.hl-yellow} de las medias lo m√°s inmediato es calcular su varianza. Si $x = \left(\overline{y}_1, \ldots, \overline{y}_c \right)$ representa el vector de medias (muestrales), y $\overline{\overline{y}_j}$ su media, entonces su varianza ser√°

$$\color{red}{\frac{1}{c}\sum_{j=1}^{c}  \left(\overline{y}_j - \overline{\overline{y}_j} \right)^2 = \frac{1}{c}\sum_{j=1}^{c}  \left(\overline{y}_j - \overline{y} \right)^2}~\#$$

Dicha cantidad cuantifica la [**variabilidad de las medias de los grupos**]{.hl-red} (c√≥mo fluctua la media entre los grupos), dando a **cada grupo el mismo peso $1/c$**. ¬øY si di√©semos **peso a cada grupo en funci√≥n de su tama√±o**?


${\tiny \#~\overline{\overline{y}_j} = \frac{1}{c} \sum_{j=1}^{c} \overline{y}_j =   \sum_{j=1}^{c} \sum_{i=1}^{n_j} \frac{1}{c* n_{j}} y_{ij} = \overline{y }}$

---

## ANOVA: un factor
 

$$H_0:~\text{variab. notas distintas asignaturas es peque√±a} = \text{"varianza" medias es peque√±a}$$


$$\color{red}{\frac{1}{n}\sum_{j=1}^{c} n_j \left(\overline{y}_j - \overline{\overline{y}_j} \right)^2 = \sum_{j=1}^{c} \frac{n_j}{n} \left(\overline{y}_j - \overline{y} \right)^2}$$


La cantidad de arriba cuantifica la [**variabilidad (PONDERADA) de las medias de los grupos**]{.hl-red}. En otras palabras: es la [**variabilidad EXPLICADA por los grupos**]{.hl-red} (variabilidad ENTRE grupos), cuyo **estimador insesgado** es (una "cuasivarianza")


$$\color{red}{\hat{s}_{e}^2= \frac{n}{n-c} \sum_{j=1}^{c} n_j\left(\overline{y}_j - \overline{y} \right)^2 = \frac{1}{n-c}\sum_{j=1}^{c}n_j \left(\overline{y}_j - \overline{y} \right)^2 = \frac{1}{n-c} SSE}$$

---

## ANOVA: un factor



$$H_0:~\text{variab. notas distintas asignaturas es peque√±a} = \text{"varianza" medias es peque√±a}$$

$$\color{red}{\hat{s}_{e}^2= \frac{n}{n-c} \sum_{j=1}^{c}n_j \left(\overline{y}_j - \overline{y} \right)^2 = \frac{1}{n-c}\sum_{j=1}^{c} n_j \left(\overline{y}_j - \overline{y} \right)^2 = \frac{1}{n-c} SSE}$$

&nbsp;

Imagina que al calcularla sale, por ejemplo, $\hat{s}_{e}^2 = 3$...

**¬øEs mucho? ¬øPoco?** ¬øEst√° suficientemente alejada de 0 para considerar que las medias son diferentes? ¬øDe qu√© depende?


---

## ANOVA: un factor

Como siempre sucede con la varianza, per se no nos aporta nada: **debe ser comparada con otra cantidad**. ¬øDe qu√© depende?

. . .

Como (again) siempre sucede con la varianza, la magnitud de dicha variabilidad va a **depender de la propia magnitud de los datos**, en este caso, de la propia [**variabilidad inherente a la variable**]{.hl-yellow}: si la varianza basal de los datos es $2.9$, seguramente $\hat{s}_{e}^2 = 3$ sea poco; si la varianza de los datos es $0.01$, entonces $\hat{s}_{e}^2 = 3$ es una barbaridad (entre grupos var√≠an 300 veces m√°s que dentro de cada grupo).

. . .

&nbsp;

¬øC√≥mo [**cuantificar la variabilidad inherente de la variable (variabilidad NO EXPLICADA por los grupos)**]{.hl-purple}?

---

## ANOVA: un factor


Si recuerdas aquellos d√≠as de inferencia, los [**esimadores insesgados**]{.hl-yellow} de la media y varianza poblacional $\left(\mu, \sigma^2 \right)$ son

* [**Media**]{.hl-yellow}: $\hat{\mu} =  \overline{y}$, lo que implica que $\hat{\mu}_j = \overline{y}_j$ y  $\hat{\varepsilon}_{ij} = y_{ij} - \overline{y}_j$.

. . .

* [**Varianza**]{.hl-yellow} (hemos asumido igual en cada grupo): la varianza muestral ser√≠a 

$$\hat{\sigma}^2 = \frac{1}{n} \sum_{j=1}^{c}\sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2 =\sum_{j=1}^{c} \frac{1}{n}\frac{n_j}{n_j}\sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2 = \sum_{j=1}^{c} \frac{n_j}{n} s_{j}^2$$

es decir, el [**promedio de las desviaciones al cuadrado respecto a la media de cada grupo**]{.hl-purple} (tambi√©n expresado como media ponderada de cada variabilidad), donde $s_{j}^2 = \frac{1}{n_j}\sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2$ representa la varianza de cada grupo.

---


## ANOVA: un factor

$$\hat{\sigma}^2 = \frac{1}{n} \sum_{j=1}^{c}\sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2 = \sum_{j=1}^{c} \frac{n_j}{n} s_{j}^2$$


Sin embargo, dado que cada $\sum_{i=1}^{n_j} \left(\frac{y_{ij} - \overline{y}_j}{\sigma} \right)^2$ son **independientes** y siguen una $\chi^{2}_{n_j - 1}$ (suma de normales - tipificadas - al cuadrado es una $\chi^2$), tenemos que

$$n\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^{2}_{n-c} \Rightarrow E[n\frac{\hat{\sigma}^2}{\sigma^2}] = n - c \Rightarrow E[\hat{\sigma}^2] = \frac{n-c}{n} \sigma^2$$

. . .

Es decir, $\hat{\sigma}^2$ [**no es un estimador insesgado**]{.hl-red} de $\sigma^2$ as√≠ que recitificaremos por un factor definiendo la conocida como [**varianza residual**]{.hl-yellow}

$$\color{purple}{\hat{s}_{r}^2 = \frac{n}{n-c}\hat{\sigma}^2 = \frac{1}{n-c} \sum_{j=1}^{c}\sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2 = \frac{1}{n-c} \sum_{j=1}^{c}\sum_{i=1}^{n_j} \hat{\varepsilon}_{ij}^2 = \frac{1}{n-c}SSR}$$



---


## ANOVA: un factor

Si la [**hip√≥tesis nula fuese cierta**]{.hl-green} (todas las observaciones vienen de la misma poblaci√≥n con misma varianza y media), la [**varianza entre grupos (explicada por el grupo) ser√≠a la misma que la varianza  inherente a los datos**]{.hl-yellow} (calculada como el promedio de las distintas varianzas intra-grupo).


:::: columns
::: {.column width="50%"}


![](./img/anova-1.png)


:::
::: {.column width="50%"}

![](./img/anova-2.png)


:::
::::



---

## ANOVA: un factor

As√≠ tenemos el [**estimador insesgado de la varianza explicada**]{.hl-red} (variabilidad atribuida a los grupos)

$$\color{red}{\hat{s}_{e}^2= \frac{n}{n-c} \sum_{j=1}^{c} n_j\left(\overline{y}_j - \overline{y} \right)^2 = \frac{1}{n-c}\sum_{j=1}^{c} n_j \left(\overline{y}_j - \overline{y} \right)^2 = \frac{1}{n-c} SSE}$$

y el [**estimador insesgado de la varianza no explicada**]{.hl-purple} (variabilidad intr√≠nseca de la variable)

$$\color{purple}{\hat{s}_{r}^2 = \frac{n}{n-c}\hat{\sigma}^2 = \frac{1}{n-c} \sum_{j=1}^{c}\sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2 = \frac{1}{n-c} \sum_{j=1}^{c}\sum_{i=1}^{n_j} \hat{\varepsilon}_{ij}^2 = \frac{1}{n-c}SSR}$$

. . .

¬øQu√© [**relaci√≥n hay entre ellas**]{.hl-yellow}? ¬øEst√°n relacionadas? ¬øPueden ambas subir mucho o bajar mucho, o se compensa una con otra?


---

## ANOVA: un factor

$$y_{ij} =  \mu_j + \varepsilon_{ij}, \quad \varepsilon_{ij} \sim N (0, \sigma^2)$$

La [**variabilidad total (SST o VT)**]{.hl-green} de la variable $y$ se define como

$$SST = \displaystyle \sum_{l=1}^{n} \left(y_l - \overline{y} \right)^2 = \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y} \right)^2, \quad VT  = \frac{1}{n} \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y} \right)^2$$


---

## ANOVA: un factor


$$SST = \displaystyle \sum_{l=1}^{n} \left(y_l - \overline{y} \right)^2 = \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y} \right)^2, \quad VT  = \frac{1}{n} \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y} \right)^2$$

Si a continuaci√≥n **a√±adimos y restamos dentro del par√©ntesis la misma cantidad** (la media muestral de cada grupo $\overline{y}_j$, estimaci√≥n insesgada de $\mu_j$)

$$\begin{eqnarray}\color{green}{SST} &=& \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y} \right)^2 = \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left( \left(y_{ij} - \overline{y}_j \right) - \left(\overline{y}_j - \overline{y} \right) \right)^2 \nonumber \\ \color{green}{VT} &=& \frac{1}{n} \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left( \left(y_{ij} - \overline{y}_j \right) - \left(\overline{y}_j - \overline{y} \right) \right)^2
\end{eqnarray}$$

---


## ANOVA: un factor


Desarrollando el cuadrado

$$\begin{eqnarray}\color{green}{SST} &=& \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y} \right)^2 = \sum_{j=1}^{c} \sum_{i=1}^{n_j} \left( \left(y_{ij} - \overline{y}_j \right) - \left(\overline{y}_j - \overline{y} \right) \right)^2 \nonumber \\ &=& \color{purple}{\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2} +  \color{red}{\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(\overline{y}_j - \overline{y} \right)^2} + 2\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right) \left(\overline{y}_j - \overline{y} \right) \nonumber \\ \color{green}{VT} &=& \color{purple}{\frac{1}{n}\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2} +  \color{red}{\frac{1}{n}\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(\overline{y}_j - \overline{y} \right)^2} + \frac{2}{n}\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right) \left(\overline{y}_j - \overline{y} \right)
\end{eqnarray}$$

donde (dado que la suma de errores dentro de cada grupo es nula)

$$\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right) \left(\overline{y}_j - \overline{y} \right) = \sum_{j=1}^{c} \left(\overline{y}_j - \overline{y} \right) \sum_{i=1}^{n_j} \varepsilon_{ij} = 0$$


---

## ANOVA: un factor

$$\begin{eqnarray}\color{green}{SST} &=& \color{purple}{\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2} +  \color{red}{\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(\overline{y}_j - \overline{y} \right)^2} = \color{purple}{\sum_{j=1}^{c} \sum_{i=1}^{n_j} \left(y_{ij} - \overline{y}_j \right)^2} +  \color{red}{\sum_{j=1}^{c} n_j \left(\overline{y}_j - \overline{y} \right)^2} \nonumber \\ &=& \color{purple}{SSR} + \color{red}{SSE} = \color{purple}{(n-c)\hat{s}_{r}^2} + \color{red}{(c-1)\hat{s}_{e}^2}\end{eqnarray}$$



El [**primer t√©rmino**]{.hl-purple} es lo que hemos llamado [**variabilidad inherente de los datos (no explicada por los grupos o residual)**]{.hl-purple} calculada como el promedio de la variabilidad $s_{j}^2$ INTRA grupo. El [**segundo t√©rmino**]{.hl-red} es lo que hemos llamado [**variabilidad explicada por los grupos**]{.hl-red} (ENTRE grupos)


. . .


Cuando el modelo es lineal como el que tenemos, [**¬°ambas est√°n relacionadas por SST!**]{.hl-green} ya que la suma de ambas es la variabilidad total de nuestra variable.

. . .

El objetivo ser√° construir un **estad√≠stico** para cuantificar si $\hat{s}_{e}^2$ (variabilidad explicada por los grupos) es mucho m√°s grande que $\hat{s}_{r}^2$ (variabilidad inherente de nuestra variable) sabiendo que cuando sube una, baja la otra.

---

## ANOVA: un factor

Dado que la suma de normales al cuadrado es una chi-cuadrado, se puede **demostrar** como 

$$\frac{SSE}{\sigma^2} \sim \chi^2_{c-1}, \quad \frac{SSR}{\sigma^2} \sim \chi^2_{n-c}$$

Si tenemos $X \sim \chi_{a}^2$ y $Y \sim \chi_{b}^2$, tenemos que su cociente (ponderado por sus grados de libertad) es $\frac{X/a}{Y/b} \sim F_{a, b}$, lo que implica que


$$F = \frac{\frac{SSE}{(c-1)\sigma^2}}{\frac{SSR}{(n-c)\sigma^2}}  \sim F_{c-1, n-c}$$

---

## ANOVA: un factor


La varianza poblacional $\sigma^2$ divide y multiplica as√≠ que podemos eliminarla

$$F = \frac{\frac{SSE}{(c-1)\sigma^2}}{\frac{SSR}{(n-c)\sigma^2}} = \frac{\frac{SSE}{c-1}}{\frac{SSR}{n-c}} \sim F_{c-1, n-c}$$

. . .

Si multiplicamos y dividimos abajo por $n$

$$F = \frac{\frac{SSE}{c-1}}{\frac{SSR}{n-c}} = \frac{\frac{SSE}{c-1}}{\frac{nSSR}{(n-c)n}} = \frac{\frac{SSE}{c-1}}{\frac{n*VR}{n-c}} = \frac{\frac{SSE}{c-1}}{\frac{n*\hat{\sigma}^2}{n-c}} = \frac{\frac{SSE}{c-1}}{\hat{s}_{r}^2} \sim F_{c-1, n-c}$$

Si te fijas el **denominador** es $\hat{s}_{r}^2 = \frac{SSR}{n-c} = \frac{n*VR}{n-c}$, la mencionada [**varianza residual (estimada)**]{.hl-purple} (el mejor estimador muestral de la varianza poblacional, la variabilidad inherente a los datos)


---

## ANOVA: un factor


$$F = \frac{\frac{SSE}{c-1}}{\hat{s}_{r}^2} = \frac{\hat{s}_{e}^2}{\hat{s}_{r}^2} \sim F_{c-1, n-c}$$


El [**numerador**]{.hl-yellow} es $\hat{s}_{e}^2 = \frac{SSE}{c-1} =  \frac{1}{c-1}\sum_{j=1}^{c}n_j \left(\overline{y}_j - \overline{y} \right)^2$ la [**varianza explicada**]{.hl-red} calculada como la **variabilidad ponderada de las medias** (corregida por $n/(c-1)$ para hacerlo insesgado), que la enfrentar√© con el denominador (variabilidad de mis datos no atribuida a los grupos, la inherente a los datos) para ver si es [**m√°s grande de lo que cabr√≠a esperar**]{.hl-yellow} (si las medias fuesen iguales) o no.

---


## ANOVA: un factor

Vamos a ver el ejemplo en `R` haciendo uso de la funci√≥n `aov()`

* `data = ...`: la base de datos
* `formula = ...`: indicado como `var_objetivo ~ factor` (donde `~` representa un "vs")
* `|> summary()`: para obtener un resumen

```{r}
aov(data = datos, formula = notas ~ asignatura) |> summary()
```

---

## ANOVA: un factor


```{r}
aov(data = datos, formula = notas ~ asignatura) |> summary()
```

Si [**analizamos por filas**]{.hl-yellow} tenemos

* `asignatura`: la parte correspondiente la [**variabilidad explicada o entre grupos**]{.hl-red}

* `Residuals`: la parte correspondiente la [**variabilidad no explicada o intra grupos**]{.hl-purple}


---

## ANOVA: un factor


```{r}
aov(data = datos, formula = notas ~ asignatura) |> summary()
```


* `Df`: degrees of freedom (grados de libertad), definidos como $c-1 = 1$ para $SSE/VE$ (ya que $\frac{SSE}{\sigma^2} \sim \chi^2_{c-1}$) y $n-c = 998$ (ya que $\frac{SSR}{\sigma^2} \sim \chi^2_{n-c}$). 

* `Sum Sq`: sum of squares (suma de cuadrados), es decir, $SSE$ y $SSR$.

* `Mean Sq`: mean of squares (media de la suma de cuadrados, ponderado por `Df`), es decir, $\hat{s}_{e}^{2}$ y $\hat{s}_{r}^{2}$.

* `F value` y `Pr(>F)`: valor del estad√≠stico $F = \frac{\hat{s}_{e}^{2}}{\hat{s}_{r}^{2}}$ y p-valor del contraste

. . .

En este caso: [**rechazamos la hip√≥tesis nula**]{.hl-yellow} de igualdad de medias, es decir, [**existen evidencias suficientes para concluir que el grupo tiene un efecto en las notas**]{.hl-yellow}



---

## Caso real: crecimiento beb√©s

Vamos a usar la tabla `leche_materna.csv` que recopila los resultados obtenidos de **2 tratamientos de leche en polvo distintos para el crecimiento de beb√©s prematuros** (de los que se miden su peso tras el tratamiento, partiendo de unas caracter√≠sticas similares, incluyendo adem√°s un **grupo control**).

[**¬øEl objetivo?**]{.hl-yellow} Comprobar si alguno de los tratamientos fue efectivo.

```{r}
datos <-
    read_csv(file = "./datos/leche_materna.csv") |> 
    janitor::clean_names()
datos
```

---

## Caso real: crecimiento beb√©s

```{r}
datos
```


¬øC√≥mo [**formulamos el ANOVA**]{.hl-yellow}?

. . .

$$y_{ij} = \mu_j + \varepsilon_{ij}, \quad y_{ij} = \text{peso del ni√±o i del grupo j}, \quad j=1,2,3 \text{ (control, trt1, trt2)}$$

donde $\mu_j$ representa la media de peso de cada grupo, cuyo **estimador** insesgado es $\overline{y}_{j}$

---

## Caso real: crecimiento beb√©s

¬øLa [**hip√≥tesis nula**]{.hl-yellow}?

. . .

$$H_0:~\mu_1 = \mu_2 = \mu_3 \quad \text{(no diferencias entre tratamientos y control)}, \quad H_1:~existe$$

¬øExiste una [**asociaci√≥n entre el tratamiento/control y el peso**]{.hl_yellow}?

---

## Caso real: crecimiento beb√©s


Desde un punto de vista [**descriptivo**]{.hl_yellow}, ¬øc√≥mo resumirlo num√©rica y gr√°ficamente?

:::: columns
::: {.column width="47%"}

```{r}
datos |> 
  summarise("mean_peso" = mean(weight),
            "sd_peso" = sd(weight),
            .by = group)
```

:::

::: {.column width="53%"}

```{r}
#| code-fold: true
ggplot(datos) +
  geom_boxplot(aes(x = group, y = weight, fill = group,
                   color = group), alpha = 0.5) +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  theme_minimal()
```

:::
::::

---

## Caso real: crecimiento beb√©s

```{r}
#| code-fold: true
ggplot(datos |>
         rowid_to_column(var = "id") |>
         mutate("mean_weight" = mean(weight), .by = group)) +
  geom_point(aes(x = id, y = weight, color = group),
             alpha = 0.5, size = 4) +
  geom_line(aes(x = id, y = mean_weight, color = group),
            linewidth = 2) +
  scale_y_continuous(limits = c(2.5, 7.5)) +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  theme_minimal()
```

---

## Caso real: crecimiento beb√©s


```{r}
aov(data = datos, formula = weight ~ group) |> summary()
```

El resultado del [**ANOVA de un factor (one-way ANOVA)**]{.hl-yellow} es el siguiente:

Si [**analizamos por filas**]{.hl-yellow} tenemos

* `group`: la parte correspondiente la [**variabilidad explicada o entre grupos de tratamiento**]{.hl-red}

* `Residuals`: la parte correspondiente la [**variabilidad no explicada o intra grupos**]{.hl-purple}

---

## Caso real: crecimiento beb√©s


```{r}
aov(data = datos, formula = weight ~ group) |> summary()
```

Si [**analizamos por columnas**]{.hl-yellow} tenemos


* `Df`: tenemos 2 grados de libertad (3 grupos - 1) para la varianza explicada y 27 grados de libertad (30 - 3 grupos) para la varianza no explicada.

* `Sum Sq`: $SSE = 3.766$ y $SSR = 10.492$,

* `Mean Sq`:  $\hat{s}_{e}^{2} = 1.8832$ y $\hat{s}_{r}^{2} = 0.3886$.


* `F value` y `Pr(>F)`: valor del estad√≠stico $F = \frac{\hat{s}_{e}^{2}}{\hat{s}_{r}^{2}} = \frac{1.8832}{0.3886} = 4.846$ y p-valor

. . .

[**Rechazamos la hip√≥tesis nula**]{.hl-yellow}: [**existen evidencias para concluir que el tratamiento tiene un efecto en el peso**]{.hl-yellow}

# Clase 4: an√°lisis de la varianza {#clase-4}

[**Asociaci√≥n cuali vs cuanti: an√°lisis de la varianza (4 de febrero de 2025)**]{style="color:#444442;"}

---

## Caso real: crecimiento beb√©s


```{r}
aov(data = datos, formula = weight ~ group) |> summary()
```

¬øEl [**problema**]{.hl-red}?

. . .

Sabemos que el tratamiento (bien sea "real" bien sea el grupo control) tiene un efecto (al menos estad√≠stico) y que se observan [**medias distintas**]{.hl-yellow}. 

. . .

[**¬øPero cu√°les son distintas 2 a 2**]{.hl-purple}? ¬øSon distintas entre los tratamientos? ¬øSon distintas tratamientos vs control? ¬øSolo uno de los tratamientos es distinto o ambos?

En este caso necesitamos **3 comparaciones**.

---

## Pruebas post-hoc

Tras obtener un [**resultado significativo en un ANOVA**]{.hl-green} el siguiente objetivo ser√≠a determinar [**cu√°l de los grupos bajo estudio contribuye a esta significaci√≥n estad√≠stica**]{.hl-yellow} (pruebas post-hoc). 

. . .

Pero como hemos explicado, la estrategia de aplicar [**repetidamente pruebas t-Student**]{.hl-yellow} puede conllevar un [**aumento desmesurado de la probabilidad de cometer un error de Tipo I**]{.hl-red} (detectar diferencias por pura aleatoriedad)

. . .

&nbsp;

Alguno de los [**m√©todos post-hoc m√°s comunes**]{.hl-yellow} (ver [**m√°s**](https://journals.lww.com/jspinaldisorders/abstract/2022/06000/the_role_of_family_wise_error_rate_in_determining.7.aspx)) son **Bonferroni**, **Tukey** y **Dunnett**.

---

## post-hoc: bonferroni

El [**m√©todo de correcci√≥n Bonferroni**]{.hl-yellow} ya lo hemos explicado anteriormente y para implementarlo basta con usar `pairwise.t.test()` 

```{r}
# `pool.sd = FALSE` implica que, para cada comparativa, se calcula una desv en cada grupo
# `pool.sd = TRUE` --> la desviaci√≥n est√°ndar agrupada es un promedio ponderado de las sd de
# dos o m√°s grupos, tal que las sd individuales se promedian, con m√°s "peso" dado a
# tama√±os de muestra m√°s grandes. solo usar cuando los tama√±os de cada grupo sean muy distintos
pairwise.t.test(datos$weight, datos$group, p.adjust.method = "bonferroni", pool.sd = FALSE)
```

. . .

* **ctrl vs trt 1**: p-valor ajustado 0.751 --> [**sin diferencias significativas**]{.hl-red}

* **ctrl vs trt 2**: p-valor ajustado 0.144 --> [**sin diferencias significativas**]{.hl-red}

* **trt 1 vs trt 2**: p-valor ajustado 0.028 --> [**dif sig de los tratamientos**]{.hl-yellow}



---

## Pruebas post-hoc

* [**Bonferroni**]{.hl-yellow}:
  - [**Ventajas**]{.hl-green}: simplicidad y **no param√©trico**
  - [**Desventajas**]{.hl-red}: al intentar controlar tanto error tipo I (rechazar $H_0$ sin que existan diferencias) es un **m√©todo muy conservador** ->  cuesta mucho que se rechace $H_0$ -> [**alta tasa de errores  tipo II seg√∫n aumentan comparaciones**]{.hl-red}
  
. . .

Para evitar los problemas de Bonferroni cuando hay un alto n√∫mero de comparaciones existe una alternativa conocida como [**test de Tukey**]{.hl-yellow}. 

```{r}
aov(data = datos, formula = weight ~ group) |> TukeyHSD()
```


---


## post-hoc: Tukey

```{r}
aov(data = datos, formula = weight ~ group) |> TukeyHSD()
```


* [**Tukey**]{.hl-yellow}:
  - [**Ventajas**]{.hl-green}: controla eficazmente la tasa de error tipo I, muy √∫til para **muchas comparaciones** y  [**mayor poder estad√≠stico* vs Bonferroni**]{.hl-yellow}
  - [**Desventajas**]{.hl-red}: sensible a valores at√≠picos y es [**param√©trico (requiere normalidad, grupos de tama√±o similar e igualdad de varianzas)**]{.hl-red}

[**Poder o potencia estad√≠stica**]{.hl-pruple}: probabilidad de rechazar $H_0$ cuando dicha hip√≥tesis es falsa (probabilidad de no cometer un error del tipo II). 

---

## post-hoc: Dunnet

En muchas ocasiones **no siempre queremos comparaciones 2 a 2 indistintamente** sino que querremos [**confrontar el resto de grupos respecto a un grupo control**]{.hl-yellow} (en nuestro caso: 2 tratamientos vs control, no los tratamientos entre s√≠).

. . .

Para ello disponemos del [**test de Dunnet**]{.hl-yellow}

$$H_0:~\mu_{tr_{j}} = \mu_{ctrl}, \quad H_1:~\mu_{tr_{j}} \neq \mu_{ctrl}$$

* [**Dunnet**]{.hl-yellow}:
  - [**Ventajas**]{.hl-green}: el contraste con mayor potencia cuando se trata de comaprar contra un grupo contorl
  - [**Desventajas**]{.hl-red}: [**param√©trico (requiere normalidad, igualdad de varianzas e independencia entre grupos)**]{.hl-red}
  

---


## Pruebas post-hoc

Para implementarlo **primero chequeamos igualdad de varianzas entre los grupos** (por ejemplo, con `car::leveneTest()`)


```{r}
car::leveneTest(datos$weight ~ datos$group)
```

. . .

Y luego `DunnettTest()` del paquete `{DescTools}`

```{r}
DescTools::DunnettTest(x = datos$weight, g = datos$group, control = "ctrl")
```

Obtenemos que [**no hay dif. sig. entre ctrl y tratamientos**]{.hl-yellow} 

---


## üíª Tu turno {#tu-turno-3-1}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Datos**]{.hl-yellow}

Vamos a usar el fichero `cushing_syndrome.csv` que contiene datos del s√≠ndrome de Cushing, un trastorno hormonal asociado con un alto nivel de cortisol. Para cada individuo de la muestra tenemos recopiladas las tasas de excreci√≥n urinaria de dos metabolitos de esteroides (tetrahidrocortisona y pregnanetriol). La variable `type` recopila el tipo de s√≠ndrome: adenoma (a), hiperplasia bilateral (b), carcinoma (c) y desconocido (u)


### [**Ej 1**]{.hl-yellow}

üìù  Carga el fichero `cushing_syndrome.csv`

```{r}
#| code-fold: true
datos <-
  read_csv(file = "./datos/cushing_syndrome.csv") |> 
  janitor::clean_names()
datos
```

### [**Ej 2**]{.hl-yellow}

üìù Calcula la media global de `tetrahydrocortisone`. Calcula la media para cada uno de los tipos de s√≠ndrome as√≠ como su tama√±o muestral

```{r}
#| code-fold: true
#| eval: false
datos |> 
  summarise("mean_global" = mean(tetrahydrocortisone))

datos |> 
  summarise("mean_groups" = mean(tetrahydrocortisone),
            "n" = n(), .by = type)
```

### [**Ej 3**]{.hl-yellow}

üìù Calcula la varianza global de `tetrahydrocortisone`. Calcula la varianza para cada uno de los tipos de s√≠ndrome as√≠ como la varianza ponderada (cada varianza de grupo multiplicada por su proporci√≥n muestral)

```{r}
#| code-fold: true
#| eval: false
datos |> 
  summarise("var_global" = var(tetrahydrocortisone))

datos |> 
  summarise("var_groups" = var(tetrahydrocortisone),
            "n" = n(), .by = type) |> 
  mutate("var_weighted" = sum(var_groups*n)/sum(n))
```


### [**Ej 4**]{.hl-yellow}

üìù Visualiza con boxplot `tetrahydrocortisone` para cada grupo

```{r}
#| code-fold: true
#| fig-height: 3
ggplot(datos) +
  geom_boxplot(aes(x = type, y = tetrahydrocortisone,
                   fill = type, color = type), alpha = 0.6) +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  theme_minimal()
```

### [**Ej 5**]{.hl-yellow}

üìù Visualiza los puntos de `tetrahydrocortisone` junto con sus medias distinguiendo para cada uno de los grupos

```{r}
#| code-fold: true
#| fig-height: 2.8
ggplot(datos |>
         rowid_to_column(var = "id") |>
         mutate("mean" = mean(tetrahydrocortisone), .by = type)) +
  geom_point(aes(x = id, y = tetrahydrocortisone, color = type),
             size = 3, alpha = 0.6) +
  geom_line(aes(x = id, y = mean, color = type), linewidth = 2) +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  theme_minimal()
```

### [**Ej 6**]{.hl-yellow}

üìù Plantea la formulaci√≥n del ANOVA `tetrahydrocortisone` vs `type`. Ejecuta el c√≥digo que consideres y saca conclusiones: ¬øhay diferencias significativas de dicha hormona entre los s√≠ndromes con $\alpha = 0.05$ y $\alpha = 0.01$?

```{r}
#| code-fold: true
#| eval: false
aov(data = datos, formula = tetrahydrocortisone ~ type) |>
  summary()
# p-valor: 0.01 > 0.05 ==> no rechazamos H_0 ==>
# no dif sig a ese nivel de sig. ==> no se continuar√≠a

# p-valor: 0.0412 < 0.05 ==> rechazamos H_0 ==>
# hay diferencias sig. entre s√≠ndromes ==> ¬øentre cu√°les?
```

### [**Ej 7**]{.hl-yellow}

üìù En los casos en los que se haya obtenido una dif significativa, ¬øcu√°les son diferentes entre s√≠? Haz pruebas post-hoc usando Bonferroni y Tukey (realizando comprobaciones previas que consideres). ¬øCu√°l ser√≠a m√°s adecuado?

```{r}
#| code-fold: true
#| eval: false
pairwise.t.test(datos$tetrahydrocortisone, datos$type, p.adjust.method = "bonferroni", pool.sd = FALSE) 
# solo diferencia significativas a vs b (0.01 p-valor ajustado)

# ¬øvarianzas iguales?
car::leveneTest(datos$tetrahydrocortisone ~ datos$type)
aov(data = datos, formula = tetrahydrocortisone ~ type) |>
  TukeyHSD()
# solo dif significativa c vs a 
# dif medias = 16.753, IC bajo de 0.65, IC alto de 32.85
# p-valor ajustado 0.039

# en este caso mejor bonferroni porque a) no hay tama√±os iguales y b) no son normales
```


### [**Ej 8**]{.hl-yellow}

üìù Por √∫ltimo vamos a considerar que el tipo `"a"` es nuestro grupo de referencia/control. ¬øC√≥mo comparar todas vs control en lugar de todas vs todas (asumimos que son independientes)?

```{r}
#| code-fold: true
#| eval: false

# ¬øvarianzas iguales?
car::leveneTest(datos$tetrahydrocortisone ~ datos$type)

# ¬øNormalidad? ==> no parece
olsrr::ols_test_normality(datos$tetrahydrocortisone)

# Dunnet test
DescTools::DunnettTest(x = datos$tetrahydrocortisone, g = datos$type)
# nos devuelve que el √∫nico sig diferente al control es c (como antes)
# resultados "con pinzas" ya que no cumple la normalidad de momento
```


:::

---

## üê£ Caso pr√°ctico I: participaci√≥n vs candidatos {#caso-practico-3-1}

El fichero `blackturnout.csv` contiene los datos de participaci√≥n en las elecciones midterm en EEUU de los a√±os 2006, 2008 y 2010: `turnout` proporci√≥n de participaci√≥n de la poblaci√≥n negra (respecto a poblaci√≥n negra en edad de votar), `cvap` proporci√≥n de habitantes de poblaci√≥n negra del distrito  y `candidate` 1 si el candidato pertenece a la poblaci√≥n negra.


```{r}
datos <- read_csv(file = "./datos/blackturnout.csv") |> janitor::clean_names()
datos |> slice_head(n = 5)
```


Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-i-participaci%C3%B3n-vs-candidatos)

---


## üê£ Caso pr√°ctico II: atractivo vs alcohol {#caso-practico-3-2}

Los datos los tenemos cargados directamente en `beer_goggles_effect.csv` 

```{r}
datos <- read_csv(file = "./datos/beer_goggles_effect.csv") |> janitor::clean_names()
datos |> slice_head(n = 5)
```

Tenemos guardado el g√©nero de 48 estudiantes, divididos en 3 grupos de ingesta de alcohol, que han encontrado pareja que les atrae en un bar. Tambi√©n tenemos el **atractivo medido (puntuaci√≥n del 0 al 100)**, otorgada por unos "jueces" externos, de la pareja encontrada por cada participante.


Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-ii-atractivo-vs-alcohol)



# Repaso entrega I

* [**Caso pr√°ctico descriptiva**]{.hl-yellow}: [**anscombe**](https://javieralvarezliebana.quarto.pub/aprendizaje-supervisado/#caso-pr%C3%A1ctico-i-anscombe)

* [**Casos pr√°cticos cuali vs cuali**]{.hl-yellow}:     
  - [**Encuesta satisfacci√≥n**](https://javieralvarezliebana.quarto.pub/aprendizaje-supervisado/#caso-pr%C3%A1ctico-i-encuesta-de-satisfacci%C3%B3n)
  - [**Bronquitis vs tabaco**](https://javieralvarezliebana.quarto.pub/aprendizaje-supervisado/#caso-pr%C3%A1ctico-ii-bronquitis-y-tabaco)
  - [**Encuesta salud mental**](https://javieralvarezliebana.quarto.pub/aprendizaje-supervisado/#caso-pr%C3%A1ctico-iii-salud-mental)

* [**Cuanti vs cuanti**]{.hl-yellow}: ver [üíª Tu turno](#tu-turno-2-3)

* [**Casos pr√°cticos ANOVA**]{.hl-yellow}:  
  - [**participaci√≥n vs candidato**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-i-participaci%C3%B3n-vs-candidatos)
  - [**atractivo vs alcohol**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-ii-atractivo-vs-alcohol)



# Clase 6: primeras regresiones {#clase-6}

[**Historia regresi√≥n. Regresi√≥n lineal univariante**]{style="color:#444442;"}



---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente m√°s mediocre que t√∫¬ª

La [**historia de regresi√≥n**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que adem√°s de estad√≠stico fue psic√≥logo, ge√≥grafo y, por desgracia, el primer eugen√©sico (de hecho acu√±√≥ el termino)

. . .

Tambi√©n fue el primero en proponer m√©todos de clasificaci√≥n de huellas en medicina forense e incluso se le atribuye el primer mapa meteorol√≥gico de la historia

---

## Regresi√≥n y Darwin

Galton mostr√≥ fascinaci√≥n por ¬´El origen de la especies¬ª de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que √©l llama mediocres**]{.hl-yellow}

. . .

Seg√∫n Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selecci√≥n natural, as√≠ que empez√≥ a **estudiar si el talento era o no hereditario**.


. . .

¬øSu conclusi√≥n? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresi√≥n a la mediocridad

En 1886 public√≥ ¬´Regression towards mediocrity in hereditary stature¬ª, un art√≠culo que cambiar√≠a la estad√≠stica: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresi√≥n**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analiz√≥ la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones hab√≠a una [**regresi√≥n (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo m√°s bajitos, e hijos de bajitos eran algo m√°s altos.

---

## Regresi√≥n a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observ√≥ que las [**estaturas ¬´regresaban¬ª a un valor medio sino que lo hac√≠an con un patr√≥n**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estad√≠stico

La regresi√≥n lineal es el modelo m√°s simple de lo que se conoce como [**aprendizaje estad√≠stico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}


* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la informaci√≥n disponible. Ejemplos: regresi√≥n, knn, √°rboles, etc.
 



* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinci√≥n entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscar√° patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.



---


## Modelo predictivo

Dentro del marco de la [**predicci√≥n supervisada**]{.hl-yellow} un modelo tendr√° siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

. . .

* $X$ ser√°n los [**datos**]{.hl-yellow}

. . .

* $f(\cdot)$ ser√° nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

. . .

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ ser√°n nuestras [**predictoras o variables independientes**]{.hl-yellow}

. . .

* $\varepsilon$ ser√° el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} $E \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (en estad√≠stica SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo pero no a nivel individual sino su promedio $E \left[Y | \boldsymbol{X} = x \right]$**]{.hl-yellow} minimizando al m√°ximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definir√° como

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ ser√°n las [**estimaciones**]{.hl-yellow}, definidas como la estimaci√≥n del [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\widehat{f}$ ser√° el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresi√≥n lineal**]{.hl-yellow} nuestro modelo ser√° un **hiperplano lineal** (y en el caso de una variable, una simple recta):

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimaci√≥n ser√° por tanto

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ ser√° una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresi√≥n lineal [**univariante**]{.hl-yellow} tendremos por tanto $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo ser√° obtener la estimaci√≥n de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

El objetivo ser√° [**explicar el m√°ximo de informaci√≥n (variabilidad) de Y con la informaci√≥n de X**]{.hl-yellow}. Como ya se mencion√≥ en el tema de ANOVA,  una manera de [**cuantificar lo que no podemos explicar**]{.hl-green} es lo que llamamos [**varianza residual (o error cuadr√°tico medio)**]{.hl-green} (varianza del error).


. . .

Dicha cantidad la podemos estimar muestralmente como sigue (ya que la [**media muestral de los errores ser√° cero**]{.hl-purple})


$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$


¬øC√≥mo quedar√≠a la f√≥rmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## Varianza residual

$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

[**IMPORTANTE**]{.hl-green}: esa varianza residual cuantifica la variabilidad (es decir, la informaci√≥n) de la variable objetivo que **NO EXPLICA** la variable X, o lo que es lo mismo, [**cuantifica c√∫anto nos equivamos en promedio**]{.hl-yellow}, un promedio de las desviaciones (al cuadrado) entre el valor real $y$ y el valor predicho $\widehat{y}$

---


## M√©todo m√≠nimos cuadrados

El [**m√©todo de los m√≠nimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**par√°metros √≥ptimos**]{.hl-yellow} ser√°n aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## M√©todo m√≠nimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## M√©todo m√≠nimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¬øC√≥mo encontrar el m√≠nimo de una funci√≥n?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## M√©todo m√≠nimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el √≥ptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¬øC√≥mo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## M√©todo m√≠nimos cuadrados



$$\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}, \quad \widehat{\beta}_0 =  \overline{y} - \widehat{\beta}_1 \overline{x}$$


Se puede demostrar c√≥mo los [**estimadores m√≠nimos cuadrados**]{.hl-yellow} coincide con los [**estimadores de m√°xima verosimilitud**]{.hl-yellow}: condicionada a la muestra obtenida, ¬øcu√°l es el valor m√°s probable/veros√≠mil de los par√°metros? (para ello lo que se busca es maximizar la log-verosimilitud, que no es m√°s que el logaritmo de la funci√≥n de densidad condicionada a los datos, dependiendo de los dos par√°metros)

---



## Estimaci√≥n reg. univariante


Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo poblaci√≥n de lo muestral**]{.hl-yellow}

* Los par√°metros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**par√°metros poblacionales**]{.hl-yellow}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en funci√≥n de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la poblaci√≥n $\left(X, Y \right) $

---

## Estimaci√≥n reg. univariante

Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¬øCu√°l es su [**interpretaci√≥n**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: tambi√©n llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimaci√≥n tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variaci√≥n de la **estimaci√≥n $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## üíª Tu turno {#tu-turno-6-1}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Ej 1**]{.hl-yellow}

üìù Elimina ausentes de estatura y peso en la base de datos de starwars

```{r}
#| code-fold: true
datos <- 
  starwars |>
  drop_na(mass, height) 
```

### [**Ej 2**]{.hl-yellow}

üìù Para hacer un [**ajuste de regresi√≥n lineal univariante en R**]{.hl-yellow} solo necesitamos utilizar la funci√≥n `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la f√≥rmula del ajuste (y frente a x, expresado como `y ~ x`)

Realiza el ajuste de regresi√≥n `peso vs estatura`


```{r}
#| code-fold: true
# lo guardamos en un objeto
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

### [**Ej 3**]{.hl-yellow}

üìù Aplica `summary()` al objeto anterior para obtener un resumen

```{r}
#| code-fold: true
#| eval: false
ajuste_lineal |> summary()
```

### [**Ej 4**]{.hl-yellow}

üìù Interpreta cada parte de la salida (hasta la tabla de coefficients incluida)

```{r}
#| code-fold: true
# `Call: ...`: la orden que hemos ejecutado

# `Residuals: ...`: un resumen en forma de cuartiles de los residuales/errores
# (f√≠jate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).

# `Coefficients: ...`: de momento solo nos interesa la columna estimate que nos da las
# estimaciones de los par√°metros. En la fila `Intercept` siempre ir√° beta_0, y el resto de 
# filas tendr√° el nombre de la variable predictora a la que multiplica el par√°metro (en 
# este caso la fila `height` corresponde a la estimaci√≥n de beta_1)
```

### [**Ej 5**]{.hl-yellow}

üìù Con la informaci√≥n anterior determina la ecuaci√≥n del modelo

```{r}
#| code-fold: true
# En nuestro caso particular, el [**ajuste de regresi√≥n**]{.hl-yellow} viene dado por 
# Y_hat = -11.4868 + 0.6240*X, donde X es la estatura: por cada cm extra que mida un 
# personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimaci√≥n de peso para 
# un persona que mida 0 cm es de -11.4868 kg 
```

:::

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¬øSer√≠a fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .


Si te fijas la [**¬°predicci√≥n del peso es negativa!**]{.hl-red}: por muy beb√© que sea, algo pesar√°. ¬øPor qu√© sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no ser√°n fiables ya que el modelo no sabe lo que sucede fuera.


# Clase 7: inferencia {#clase-7}

[**Inferencia de la regresi√≥n**]{style="color:#444442;"}

---


## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la poblaci√≥n y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimaci√≥n**]{.hl-yellow}: $\widehat{Y}$ en funci√≥n de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresi√≥n m√≠nimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores ser√°n $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicci√≥n**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ est√© dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¬øpara qu√© necesitar√≠amos [**hip√≥tesis**]{.hl-yellow} entonces?

. . .

La raz√≥n es que, hasta ahora, lo √∫nico que hemos podido realizar es una [**estimaci√≥n puntual**]{.hl-yellow} de los par√°metros, pero dado que dichos estimadores ser√°n variables aleatorias, necesitaremos realizar [**inferencia estad√≠stica**]{.hl-yellow} sobre ellos (recuerda: los par√°metros son simples estimaciones para esa muestra de la poblaci√≥n, de forma que dada otra muestra, la recta ser√° distinta).

. . .

Para poder cuantificar la [**variabilidad y precisi√≥n de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hip√≥tesis probabil√≠sticas**]{.hl-purple}: lo interesante no es la estimaci√≥n puntual de los par√°metros a partir de la muestra sino lo que [**podamos inferir de ellos a la poblaci√≥n**]{.hl-green}

---

## Diagnosis

En el caso de la regresi√≥n lineal univariante pediremos [**4 hip√≥tesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podr√° explicar toda la informaci√≥n pero necesitamos que la [**varianza del error sea finita y constante, sin depender de $x$**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.
. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hip√≥tesis se pueden [**resumir de manera te√≥rica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versi√≥n muestral**]{.hl-purple} ser√≠a simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---


## Inferencia de los par√°metros

Las hip√≥tesis nos permiten decir que los [**par√°metros estimados siguen una distribuci√≥n (condicionada) normal**]{.hl-yellow} de [**media el par√°metro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del l√≠mite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$



---

## Inferencia de los par√°metros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos m√°s adelante porqu√© pero de momento...estos son los $SE^2$ (**varianza de nuestros par√°metros**)

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¬øQu√© [**propiedades tienen entonces los estimadores**]{.hl-yellow} $\left(\widehat{\beta}_0,\widehat{\beta}_1 \right)$?


---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimaci√≥n es el valor a estimar. $E \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisi√≥n vs tama√±o muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ est√° dividiendo. Traducci√≥n: [**a m√°s datos, mayor precision**]{.hl-green} (menos varianza tendr√°n los estimadores si repetimos la toma de muestras)


---

## Inferencia de los par√°metros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisi√≥n vs var residual**]{.hl-yellow}: cuanto m√°s grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecer√° (es decir, [**m√°s ruido implicar√° m√°s imprecisi√≥n**]{.hl-red})

. . .

* [**Precisi√≥n vs varianza de X**]{.hl-yellow}: cuanto m√°s grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecer√°, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cu√°nta [**m√°s informaci√≥n (varianza) contenga nuestra tabla, mayor precisi√≥n**]{.hl-green}.

. . .

* [**Precisi√≥n vs media X**]{.hl-yellow}: solo afecta a la estimaci√≥n de $\beta_0$, cuya [**precisi√≥n decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cu√°nto m√°s grande en media sean los datos, menos fiable ser√° la predicci√≥n para $X=0$.

---

## Inferencia de los par√°metros

Un inciso: no se si te has fijado que para calcular $\widehat{\beta}_0$ necesitas $\widehat{\beta}_1$: los [**par√°metros est√°n relacionados**]{.hl-yellow}

. . .


$$Cov \left(\widehat{\beta}_0,\widehat{\beta}_1\right) = Cov \left(\overline{y} - \widehat{\beta}_1 \overline{x} ,\widehat{\beta}_1\right) = Cov\left(\overline{y} ,\widehat{\beta}_1\right) - Cov\left(\widehat{\beta}_1 \overline{x} ,\widehat{\beta}_1\right) = - \overline{x} V \left[\widehat{\beta}_1 \right] = \frac{-\overline{x}\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$
. . .

F√≠jate que esto implica que si $\overline{x}> 0$, entonces la [**covarianza es negativa**]{.hl-yellow}: poca precisi√≥n en la estimaci√≥n de uno de los par√°metros producir√° mucha precisi√≥n en la estimaci√≥n del otro (y viceversa).

---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza poblaci√≥n del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma  donde $p$ es el n√∫mero de variables (el cuadrado de lo que R llama `Residual standard error)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 := \text{Res standard error}^2$$

donde $s_{r}^{2} = MSE$ es la varianza residual muestral o el **error cuadr√°tico medio**.

---

## Inferencia de los par√°metros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¬øQu√© suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estad√≠stico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (f√≠jate que hemos puesto $\beta_j = 0$)

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¬øDe qu√© contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la desviaci√≥n t√≠pica residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}}$

---

## Inferencia de los par√°metros



$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$


Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros par√°metros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Inferencia de los par√°metros

Podemos calcular los intervalos de confianza de los par√°metros en `R` con `confint()`

```{r}
confint(ajuste_lineal, level = 0.99)
confint(ajuste_lineal, level = 0.95)
confint(ajuste_lineal, level = 0.9)
```

---

## Inferencia de los par√°metros

[**¬øQu√© significa realmente un intervalo de confianza?**]{.hl-yellow}

. . .

La **probabilidad de que el par√°metro real** caiga dentro [**NO EXISTE**]{.hl-red}: el par√°metro es desconocido pero fijo, no aleatorio, as√≠ que no sentido calcular su probabilidad.

. . .

Si obtenemos para un par√°metro un intervalo $[-1, 1]$ al 95%, no significa que $P(parametro \in [-1, 1]) = 0.95$: significa que [**si tomamos 1000 muestras distintas de la poblaci√≥n**]{.hl-yellow} y calculamos para cada una el intervalo de confianza, [**aproximadamente 950 intervalos de confianza contendr√°n dentro el par√°metro real**]{.hl-yellow}

. . .

Un [**intervalo al 95% implica que habr√° una frecuencia esperada de 0.95**]{.hl-yellow} de que intervalos que no conocemos (porque se derivan de muestras que no hemos tomado) contengan al par√°metro real, pero [**no es la probabilidad de que tu intervalo calculado contenga a dicho par√°metro**]{.hl-red}: nos habla la precisi√≥n de nuestra metodolog√≠a de estimaci√≥n, no del par√°metro.


---

## Inferencia de los par√°metros

[**Ejemplo**]{.hl-yellow}. Dada una poblaci√≥n normal $\mu = 3$ y $\sigma = 1.2$, generamos 500 muestras distintas (tama√±o $n = 100$ cada una), de manera que para cada una aplicamos un `t.test()` para calcular un IC para la media $\mu$.

```{r}
#| code-fold: true
mu <- 3
sigma <- 1.2
m <- 500
conf_int <- tibble("id" = 1:m, "low" = NA, "high" = NA)

for (i in 1:m) {
  
  sample <- rnorm(n = 100, mean = mu, sd = sigma)
  hyp_test <- t.test(x = sample)
  conf_int[i, 2] <- hyp_test$conf.int[1]
  conf_int[i, 3] <- hyp_test$conf.int[2]
}
conf_int <-
  conf_int |> 
  mutate(true_param = mu >= low & mu <= high)

ggplot(conf_int) +
  geom_segment(aes(y = id, yend = id, x = low, xend = high,
                   color = true_param, linewidth = true_param)) +
  geom_vline(xintercept = mu) +
  scale_linewidth_manual(values = c(1.1, 0.2)) +
  theme_minimal() +
  guides(linewidth = "none") +
  labs(x = "Valores del intervalo",
       y = "id intervalo",
       color = "¬øContiene mu real?")
```

---


## Inferencia de los par√°metros

Y si tenemos inferencia, tenemos contrastes: ¬øte acuerdas de los p-valores que devuelve la tabla para cada par√°metro?

. . .

Para cada par√°metro se realiza un [**contraste de significancia**]{.hl-yellow}: ¬øcu√°nta evidencia hay en mis datos para poder decir que el [**valor estimado de mi par√°metro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estad√≠stico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIP√ìTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los par√°metros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¬øTiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, se suele rechazar la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que s√≠ lo tiene); en caso contrario no se suele rechazar (que **no es lo mismo que aceptarla**). 

---

## Inferencia de los par√°metros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores est√°n por encima de $\alpha = 0.05$ (umbral adoptado habitualmente) los que nos dice que [**no hay evidencias de los datos sean compatibles con afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¬øY si probamos a quitar $\beta_0$ (es decir, la respuesta est√° centrada)?

---

## Inferencia de los par√°metros

Para ello basta a√±adir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

F√≠jate que ahora, am√©n que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisi√≥n mayor. En este caso [**solo pod√≠a quitar uno**]{.hl-yellow} (perder√≠amos $X$), pero veremos m√°s adelante c√≥mo decidir cu√°l quitar si tuvi√©semos varias variables.



# Clase 8: predicci√≥n y simulaci√≥n {#clase-8}

[**Entendiendo mejor la regresi√≥n**]{style="color:#444442;"}

---

## Predicci√≥n en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendi√≥) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la funci√≥n `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```


---

## Simulaci√≥n

Antes de seguir con un caso real, es importante pararse a reflexionar sobre lo ya aprendido, y una de las mejores es [**con simulaci√≥n**]{.hl-yellow} 

. . .

* [**Ejercicio 1**]{.hl-yellow}: simula una muestra de tama√±o $n = 1000$ de una variable aleatoria $X \sim N(\mu=3, \sigma = 1)$. Esta variable representar√° nuestra predictora (normalmente la tendr√≠amos ya en la tabla pero aqu√≠ la hemos simulado).

. . .

* [**Ejercicio 2**]{.hl-yellow}: simula un ruido $\varepsilon$ de varianza $0.1$ que cumpla las condiciones que necesitamos en una regresi√≥n lineal

. . .

* [**Ejercicio 3**]{.hl-yellow}: con la variable predictora y el ruido simulado, simula una variable aleatoria $Y$ bajo la hip√≥tesis de que $Y$ puede ser explicada por $X$ mediante una recta de par√°metros reales $\beta_0 = -2$ y $\beta_1 = 5$.

. . .

* [**Ejercicio 4**]{.hl-yellow}: visualiza en un gr√°fico la predictora $x$ vs var objetivo $y$, en otro gr√°fico la distribuci√≥n de  $Y$ y en otro la distribuci√≥n de los errores

---

## Simulaci√≥n

Deber√≠as obtener algo **parecido a esto** (parecido != igual ya que estamos generando variables aleatorias: en promedio se parecer√°n pero a nivel individual no tienen porque coincidir)

```{r}
#| echo: false
n <- 1e3
set.seed(12345)
x <- rnorm(n = n, mean = 3, sd = 1)
eps <- rnorm(n = n, mean = 0, sd = sqrt(0.1))
y <- -2 + 5*x + eps
datos <- tibble("x" = x, "y" = y)

gg1 <-
  ggplot(datos, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()

gg2 <-
  ggplot(datos, aes(x = y)) +
  geom_density() +
  theme_minimal()

gg3 <-
  ggplot(datos, aes(x = eps)) +
  geom_density() +
  theme_minimal()

library(patchwork)
gg1 + gg2 + gg3
```

---

## Simulaci√≥n

* [**Ejercicio 4**]{.hl-yellow}: sin hacer uso de `lm` calcula el estimador insesgado de $\beta_0$ y el estimador insesgado de $\beta_1$. Haz uso de las funciones (R base o tidyverse) que consideres para realizar los c√°lculos.

. . .

* [**Ejercicio 5**]{.hl-yellow}: con dichas estimaciones de los par√°metros, calcula estimaciones/predicciones de $y_i$, los conocidos como $\widehat{y}_i$, y los errores estimados $\widehat{\varepsilon}_i$. Guarda todo en una tabla de 5 columnas: $y$, $x$, $\varepsilon$, $\widehat{y}$ y $\widehat{\varepsilon}$ 

. . .

* [**Ejercicio 6**]{.hl-yellow}: con dicha tabla haz 3 gr√°ficos. Gr√°fico 1: valores reales vs predicciones (¬øqu√© deber√≠amos observar si la estimaci√≥n fuese buena?) con el color dependiendo del error cometido. Gr√°fico 2: Q-Q plot de los errores reales simulados (los que conocemos porque los hemos generado nosotros). Gr√°fico 3:  Q-Q plot de los errores estimados (los √∫nicos que ver√≠amos si fuesen datos reales)

---

## Simulaci√≥n

Deber√≠as obtener algo **parecido a esto** (parecido != igual ya que estamos generando variables aleatorias: en promedio se parecer√°n pero a nivel individual no tienen porque coincidir)

```{r}
#| echo: false
betas <-
  datos |>
  summarise("beta_1" = cov(x, y)/var(x), "beta_0" = mean(y) - beta_1 * mean(x))
datos <-
  datos |> 
  mutate("eps" = eps,
         "y_hat" = betas$beta_0 + betas$beta_1*x,
         "eps_hat" = y - y_hat)
gg1 <-
  ggplot(datos, aes(x = y, y = y_hat, color = eps_hat)) +
  geom_point(size = 2.5, alpha = 0.8) +
  scale_color_gradient2(low = "#cf514e", mid = "grey90", high = "#376990",
                        midpoint = 0) +
  theme_minimal()

gg2 <-
  ggplot(datos, aes(sample = eps)) +
  stat_qq() +
  stat_qq_line() +
  theme_minimal()

gg3 <-
  ggplot(datos, aes(sample = eps_hat)) +
  stat_qq() +
  stat_qq_line() +
  theme_minimal()

library(patchwork)
gg1 + gg2 + gg3
```

---

## Simulaci√≥n

* [**Ejercicio 7**]{.hl-yellow}: con dichos errores estimados estima (como si no la conocieses) de manera insesgada la desviaci√≥n t√≠pica de los errores $\widehat{\sigma}_{\varepsilon}$ (el conocido como `Residual standard error`)

. . .

* [**Ejercicio 8**]{.hl-yellow}: con lo ya obtenido calcula sin hacer uso de `lm()` las estimaciones de las desviaciones t√≠picas de las variables aleatorias $\widehat{\beta}_0$ y  $\widehat{\beta}_1$ que representan nuestros estimadores (los conocidos como $SE$). Genera dos variables aleatorias ($n = 5000$) con la distribuci√≥n de ambas variables $\widehat{\beta}_0$ y  $\widehat{\beta}_1$

. . .

* [**Ejercicio 9**]{.hl-yellow}: repite el proceso inicial (generar x -->  generar errores --> generar y) con los mismos par√°metros un total de 5000 iteraciones. En cada iteraci√≥n calcula los estimadores muestrales de $\widehat{\beta}_0$ y  $\widehat{\beta}_1$, y visualiza despu√©s la distribuci√≥n de sus valores. Enfrenta esa muestra de betas con la simulaci√≥n del ejercicio anterior

---

## Simulaci√≥n

Deber√≠as obtener algo **parecido a esto**: ¬øc√≥mo interpretas las 4 gr√°ficas?

```{r}
#| echo: false
betas <-
  datos |>
  summarise("beta_1" = cov(x, y)/var(x),
            "beta_0" = mean(y) - beta_1 * mean(x),
            "res_std_error" = sqrt(sum(eps_hat^2)/(n()-2)),
            "SE_0" = sqrt(res_std_error^2/(n() * var(x))),
            "SE_1" = sqrt((res_std_error^2/n())*(1 + (mean(x)^2/var(x)))))
sim_betas <-
  tibble("beta_0_sim" = rnorm(n = 5e3, mean = betas$beta_0, sd = betas$SE_0),
         "beta_1_sim" = rnorm(n = 5e3, mean = betas$beta_1, sd = betas$SE_1))
gg1 <-
  ggplot(sim_betas |> 
           pivot_longer(cols = everything())) +
  geom_density(aes(x = value, color = name, fill = name), alpha = 0.7) +
  MetBrewer::scale_color_met_d("Renoir") +
  MetBrewer::scale_fill_met_d("Renoir") +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(title = "Betas simulados a partir de la estimaci√≥n de ambos",
       subtitle = "Lo que mi modelo estima que pasar√≠a si tomase muchas muestras")

betas <- tibble()
for (i in 1:5e3) {
  
  x <- rnorm(n = n, mean = 3, sd = 1)
  eps <- rnorm(n = n, mean = 0, sd = sqrt(0.1))
  y <- -2 + 5*x + eps
  datos <- tibble("x" = x, "y" = y)
  
  betas <-
    betas |> 
    bind_rows(datos |>
                summarise("beta_1" = cov(x, y)/var(x),
                          "beta_0" = mean(y) - beta_1 * mean(x)) |> 
                mutate("iter" = i))
  
}
gg2 <-
  ggplot(betas |> 
           pivot_longer(cols = -iter)) +
  geom_density(aes(x = value, color = name, fill = name), alpha = 0.7) +
  MetBrewer::scale_color_met_d("Renoir") +
  MetBrewer::scale_fill_met_d("Renoir") +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(title = "Betas muestrales a partir de simular datos",
       subtitle = "Lo que pasar√≠a si se pudiese tomar una muestra tras otra")

gg3 <-
  ggplot() +
  geom_point(data = tibble("betas_0" = betas$beta_0, "beta_0_sim" = sim_betas$beta_0_sim),
             aes(x = betas_0, y = beta_0_sim), color = "#6559ac") +
  labs(x = "beta_0 simulado a partir de par√°metros poblacionales",
       y = "beta_0 obtenido de muchas muestras") +
  theme_minimal()
gg4 <-
  ggplot() +
  geom_point(data = tibble("betas_1" = betas$beta_1, "beta_1_sim" = sim_betas$beta_1_sim),
             aes(x = betas_1, y = beta_1_sim), color = "#f8bd13") +
  labs(x = "beta_1 simulado a partir de par√°metros poblacionales",
       y = "beta_1 obtenido de muchas muestras") +
  theme_minimal()
library(patchwork)
gg1 + gg3 + gg2 + gg4
```

---


## üíª Tu turno {#tu-turno-8-1}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Ej 1**]{.hl-yellow}

üìù Simula una poblaci√≥n de datos $(x, y)$ (tama√±o n = 1000000, como si fuese la poblaci√≥n te√≥rica real) bajo la hip√≥tesis de que siguen el modelo lineal $y = 1.5 - 3*x + \varepsilon$. Toma las decisiones que consideres teniendo en cuenta que i) $\varepsilon$ debe cumplir las condiciones necesarias con una varianza de $0.75$; ii) $x$ se generar√° seg√∫n una normal de media 1 y varianza 1.5.

```{r}
#| code-fold: true
n <- 1e6
eps <- rnorm(n = n, mean = 0, sd = sqrt(0.75))
x <- rnorm(n = n, mean = 1, sd = sqrt(1.5))
y <- 1.5 - 3*x + eps
datos <- tibble(y, x, eps)
```


### [**Ej 2**]{.hl-yellow}

üìù Obt√©n una muestra aleatoria simple de $n = 5000$ de dicha poblaci√≥n.

```{r}
#| code-fold: true
muestra <- datos |> slice_sample(n = 5000)
```

### [**Ej 3**]{.hl-yellow}

üìù Ajusta un modelo de regresi√≥n lineal. ¬øCu√°l es la ecuaci√≥n del modelo estimado? ¬øSon los par√°metros significativos? ¬øPor qu√©? Interpreta el valor de ambos par√°metros de manera adecuada.

```{r}
#| code-fold: true
muestra <- datos |> slice_sample(n = 5000)
ajuste <- lm(data = muestra, y ~ x)
# y_hat = 1.492520 - 2.986930*X
# ambos par√°metros significativos porque al realizar
# el contraste de significaci√≥n (¬°ASUMIENDO QUE SE CUMPLEN HIP√ìTESIS!)
# salen p-valores < 0.05 ==> rechazamos H0: beta_j = 0 en ambos casos
# 1.492 es la estimaci√≥n de E[Y|X = 0].
# 2.986 es lo que desciende en promedio Y cuando X aumenta +1 
```

### [**Ej 4**]{.hl-yellow}

üìù Calcula la media de los residuales al cuadrado (varianza residual o MSE). Haciendo uso de la salida, ¬øcu√°nto estima el modelo que es su estimador insesgado?

```{r}
#| code-fold: true
MSE <- mean(ajuste$residuals^2)
estimador_insesgado_var_res <- 0.8504^2 # residual standard error^2
```


### [**Ej 5**]{.hl-yellow}

üìù Si volvi√©semos a extraer una muestra de la poblaci√≥n y estim√°semos $\beta_0$ y $\beta_1$, una y otra vez, ¬øcu√°l ser√≠a la distribuci√≥n que observar√≠amos en ambos? ¬øQu√© media tendr√≠a? ¬øQu√© variabilidad?

```{r}
#| code-fold: true
# ¬°Suponiendo ciertas las hip√≥tesis ojo!
# beta_hat_0 ~ N(1.492, sigma = 0.015626)
# beta_hat_1 ~ N(-2.987, sigma = 0.009928)
```

### [**Ej 6**]{.hl-yellow}

üìù Repite los ejercicios 1-5 con el √∫nico cambio de que la varianza de $\varepsilon$ ahora ser√° de $2.5$. Repite los ejercicios 1-5 con el √∫nico cambio de que la varianza de $\varepsilon$ ahora ser√° de $7$. ¬øQu√© observas?

```{r}
#| code-fold: true
eps <- rnorm(n = n, mean = 0, sd = sqrt(2.5))
x <- rnorm(n = n, mean = 1, sd = sqrt(1.5))
y <- 1.5 - 3*x + eps
datos <- tibble(y, x, eps)
muestra <- datos |> slice_sample(n = 5000)
ajuste <- lm(data = muestra, y ~ x)
# R2 de 0.9477 a 0.8389
# Estimaci√≥n varianza residual de 0.8504^2 a 1.579^2
# mismo modelo, pero aparenta funcionar peor
# varianza de beta_0 y beta_1 aumenta

eps <- rnorm(n = n, mean = 0, sd = sqrt(7))
x <- rnorm(n = n, mean = 1, sd = sqrt(1.5))
y <- 1.5 - 3*x + eps
datos <- tibble(y, x, eps)
muestra <- datos |> slice_sample(n = 5000)
ajuste <- lm(data = muestra, y ~ x)
# a√∫n m√°s extremo
```



### [**Ej 7**]{.hl-yellow}

üìù Repite los ejercicios 1-5 con el √∫nico cambio que ahora la muestra ser√° de $n = 50$. ¬øQu√© observas?

```{r}
#| code-fold: true
eps <- rnorm(n = n, mean = 0, sd = sqrt(0.75))
x <- rnorm(n = n, mean = 1, sd = sqrt(1.5))
y <- 1.5 - 3*x + eps
datos <- tibble(y, x, eps)
muestra <- datos |> slice_sample(n = 50)
ajuste <- lm(data = muestra, y ~ x)
# Estimaci√≥n varianza residual de 0.8504^2 a 0.9546^2
# aunque haya empeorado un poco R2 de 0.9477 a 0.9541 
# ==> para la cantidad de pocos datos que tienes, no ha
# empeorado tanto (recuerda que R2 depende de la varianza de y)
# a menos n m√°s opciones hay que de la varianza sea m√°s alta
# ya que puedes tener valores m√°s "extremos"
# varianza de beta_0 y beta_1 aumentan (menos precisi√≥n)
```

:::

---

## üê£ Caso pr√°ctico I: predicci√≥n de iris {#caso-practico-8-1}

Vamos a empezar la regresi√≥n con un ejemplo sencillo haciendo uso del archiconocido dataset `iris` donde tenemos para 150 plantas de 3 especies distintas guardado su longitud y anchura de s√©palo, y su longitud y anchura de p√©talo.

```{r}
datos <- iris |> as_tibble()
```

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#clase-8)



# Clase 9: caso pr√°ctico {#clase-9}

[**Poniendo en pr√°ctica lo aprendido**]{style="color:#444442;"}


---

## Resumen


```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
lm(data = datos, formula = mass ~ height) |> summary()
```

* `Residuals:` tabla con los cuartiles de los errores = residuales

---

## Resumen


```{r}
lm(data = datos, formula = mass ~ height) |> summary()
```

* `Coefficients:` tabla referente a la **estimaci√≥n e inferencia** de los estimadores de los par√°metros $\widehat{\beta}_j$

---

## Resumen


```{r}
lm(data = datos, formula = mass ~ height) |> summary()
```

* `Estimate` --> media de los estimadores insesgados $\widehat{\beta}_j \sim N$
* `Std. Error` --> estimador insesgado de la desv. t√≠pica de los estimadores $\widehat{\beta}_j \sim N(estimate, \sigma = std.~error)$ para el que se ha sustituido la desv t√≠pica poblacional del error por su estimador insesgado `Residual standard error`

---


## Resumen


```{r}
lm(data = datos, formula = mass ~ height) |> summary()
```

* `t value` --> estad√≠stico bajo la hip√≥tesis nula $tvalue = estimate/std.error$
* `Pr(>|t|)` --> p-valor del contraste de significaci√≥n ($H_0:~\beta_j = 0$).

---

## Caso pr√°ctico

Vamos a poner en pr√°ctica lo aprendido con el dataset `wine.csv` disponible en el campus.

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

---

## Caso pr√°ctico


El conjunto de datos est√° formado por 27 observaciones (cosechas de vino rojo Burdeos) y 7 variables

* `Year`, `Age`: a√±o de la cosecha y n√∫mero de a√±os en barrica.
* `Price`: precio en 1990-1991 de cada cosecha (en **escala log**)
* `WinterRain`: lluvia (en mm) que cay√≥ ese a√±o en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cay√≥ ese a√±o durante la cosecha.
* `FrancePop`: poblaci√≥n (miles de habitantes) de Francia.

Ver m√°s en detalles en <https://doi.org/10.1080/09332480.1995.10542468>

. . .

[**El objetivo: predecir el precio**]{.hl-yellow}


---

## Caso pr√°ctico

```{r}
datos
```

Para predecir el precio vamos a usar (de momento) una [**regresi√≥n lineal univariante**]{.hl-yellow}, donde $Y = precio$ y deberemos elegir la predictora $X$ m√°s apropiada.

---

## Pasos a seguir

1. [**An√°lisis exploratorio inicial**]{.hl-yellow}:
  - ¬øLas variables son [**num√©ricas (continuas)**]{.hl-purple}?
  - ¬øTienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¬øTienen datos ausentes?
  - ¬øC√≥mo se [**distribuyen las variables**]{.hl-purple}? Ideas: res√∫men num√©rico, histogramas/densidades, boxplots, gr√°ficos de viol√≠n, etc
  - ¬øHay [**datos at√≠picos**]{.hl-purple}?

---

## Pasos a seguir

2. [**An√°lisis de dependencia**]{.hl-yellow}:
  - ¬øQu√© [**predictora est√° m√°s correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir?
  - ¬øC√≥mo se [**relacionan las predictoras**]{.hl-purple} entre s√≠? ¬øEst√°n correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersi√≥n vs Y, corrplots, etc

. . .

3. [**Formulaci√≥n**]{.hl-yellow} del modelo

. . .

4. [**Fase de estimaci√≥n**]{.hl-yellow}:
  - ¬øCu√°nto valen los [**par√°metros estimados**]{.hl-purple}? ¬øC√≥mo queda el ajuste?
  - ¬øQu√© [**interpretaci√≥n**]{.hl-purple} tienen?


---

## Pasos a seguir

5. [**Fase de diagnosis**]{.hl-yellow} (pendiente de ver, de momento no la hagas pero apunta que hace falta para poder hacer la siguiente fase: sin condiciones no hay inferencia param√©trica)
  
. . .

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¬øQu√© [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro par√°metros?
  - ¬øLas predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¬øDebemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

---

## Pasos a seguir

7. [**Fase de evaluaci√≥n**]{.hl-yellow}: pendiente de ver, de momento no la hagas pero apunta que hace falta

8. [**Fase de predicci√≥n**]{.hl-yellow} (solo tiene sentido con todo lo anterior hecho)

---


## An√°lisis exploratorio inicial


1. [**An√°lisis exploratorio inicial**]{.hl-yellow}:
  - ¬øLas variables son [**num√©ricas (continuas)**]{.hl-purple}?
  - ¬øTienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¬øTienen datos ausentes?
  
&nbsp;

Para responder a dichas preguntas lo m√°s sencillos es hacer uso de la funci√≥n `skim()` del paquete `{skimr}`

```{r}
#| eval: false
library(skimr)
datos |> skim()
```

. . .

En este caso [**no tenemos ausentes ni problemas de codificaci√≥n**]{.hl-green}


---

## An√°lisis exploratorio inicial

- ¬øC√≥mo se [**distribuyen las variables**]{.hl-purple}?
- ¬øHay [**datos at√≠picos**]{.hl-purple}?


&nbsp;

Para ello podemos [**visualizar la distribuci√≥n de cada variable**]{.hl-yellow} haciendo uso de densidades, histogramas y/o boxplots (por ejemplo)

```{r}
#| code-fold: true
datos_tidy <-
  datos |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "values")

ggplot(datos_tidy, aes(x = values)) +
  geom_density(aes(color = variable, fill = variable),
               alpha = 0.6) +
  MetBrewer::scale_color_met_d("Renoir") +
  MetBrewer::scale_fill_met_d("Renoir") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```



---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(y = values, x = variable)) +
  geom_boxplot(aes(color = variable, fill = variable),
               alpha = 0.6) +
  MetBrewer::scale_color_met_d("Renoir") +
  MetBrewer::scale_fill_met_d("Renoir") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

[**No hay valores at√≠picos**]{.hl-green} (respecto a percentiles al menos)


---

## An√°lisis exploratorio inicial

Podemos tambi√©n visualizar con un [**scatter plot**]{.hl-yellow} todas las variables y adem√°s sus correlaciones, con el paquete `{GGally}` y la funci√≥n `ggpairs(), sin necesidad de convertir a tidydata.

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos) +
  theme_minimal()
```
   
---


## An√°lisis de dependencia

2. [**An√°lisis de dependencia**]{.hl-yellow}:
  - ¬øQu√© [**predictora est√° m√°s correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¬øExiste otro tipo de dependencia (pendiente implementar en `R`)?
  - ¬øC√≥mo se [**relacionan las predictoras**]{.hl-purple} entre s√≠? ¬øEst√°n correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersi√≥n vs Y, corrplots, etc

. . .

Este paso ser√° crucial en el [**contexto multivariante**]{.hl-yellow} pero en este caso simplemente vamos a ver como se relacionan linealmente las predictoras entre s√≠, y cu√°l de ellas es la **m√°s adecuada para predecir linealmente** `precio`

---

## An√°lisis de dependencia

El primer paso es la [**matriz de correlaciones**]{.hl-yellow} con la funci√≥n `cor()` o con la funci√≥n `correlate()` del paquete `{corrr}` (importa en tibble m√°s visual)

```{r}
library(corrr)
datos |> correlate()
```

. . .

* [**Respecto a Y**]{.hl-yellow}: predictoras con mayor cor lineal son `AGST` (m√°s calor, menos cosechas, sube el precio) y `HarvestRain` (m√°s lluvias, m√°s cosechas, baja el precio, ¬°el signo importa!)

* [**Dependencia entre predictoras**]{.hl-yellow}: las variables `Age`, `Year` y `FrancePop` presentan la misma informaci√≥n.


---

## An√°lisis de dependencia

Tambi√©n podemos usar `corrplot()` del paquete `{corrplot}`, al que le pasamos una matriz de correlaciones cl√°sica y nos la [**visualiza**]{.hl-yellow}.

```{r}
#| code-fold: true
library(corrplot)
datos |>
  cor() |>
  corrplot(method = "ellipse")
```

Puedes ver distintas opciones de visualizaci√≥n en <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>

---

## An√°lisis de dependencia

Otra opci√≥n es [**visualizar con un scatter plots todas las predictoras vs Y**]{.hl-yellow}, pivotando antes nuestro dataset (solo pivotamos las predictoras)

:::: columns
::: {.column width="65%"}

```{r}
#| code-fold: true
datos_tidy <-
  datos |>
  pivot_longer(cols = -Price, names_to = "variable",
               values_to = "values")
ggplot(datos_tidy, aes(x = values, y = Price)) + 
  geom_point(aes(color = variable), alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  MetBrewer::scale_color_met_d("Renoir") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal()
```

:::

::: {.column width="35%"}

No solo comprobamos que las rectas con m√°s pendiente son `AGST` y `HarvestRain`, adem√°s los puntos parecen poder ajustarse a una recta sin otro patr√≥n identificable.

:::
::::

Esto es importante hacerlo ya que debemos [**descartar posibles correlaciones esp√∫reas**]{.hl-red} (ver ejemplo datasaurus)

---

## Formulaci√≥n del modelo

Una vez que hemos decidido que dos predictoras usaremos, vamos por tanto a plantear [**dos posibles modelos univariantes**]{.hl-yellow}

$$Price = \beta_0 + \beta_1*AGST + \varepsilon$$
$$Price = \beta_0 + \beta_1*HarvestRain + \varepsilon$$

---

## Fase de estimaci√≥n


4. [**Fase de estimaci√≥n**]{.hl-yellow}:
  - ¬øCu√°nto valen los [**par√°metros estimados**]{.hl-purple}? ¬øC√≥mo queda el ajuste?
  - ¬øQu√© [**interpretaci√≥n**]{.hl-purple} tienen?


&nbsp;

Para ello ejecutaremos ambos modelos con `lm()`

```{r}
ajuste_AGST <- lm(data = datos, formula = Price ~ AGST)
ajuste_harvest <- lm(data = datos, formula = Price ~ HarvestRain)
```

---


## Fase de estimaci√≥n

[**Ajuste con AGST**]{.hl-yellow}

```{r}
ajuste_AGST |> summary()
```

* $\beta_0=$ `r round(ajuste_AGST$coefficients[1], 3)`: predicci√≥n del precio (escala log) cuando $AGST = 0$ (es decir, si $AGST = 0$, entonces precio es de $0.02881 = exp(-3.5469)$)

* $\beta_1=$ `r round(ajuste_AGST$coefficients[2], 3)`: lo que aumenta el precio (escala log) por cada grado de aumento (es decir, por cada unidad que aumenta la temperatura el precio se multiplica por $exp(0.6426) = 1.901418$ --> aumenta un 90%)


---

## Fase de estimaci√≥n

[**Ajuste con AGST**]{.hl-yellow}

```{r}
#| echo: false
ajuste_AGST |> summary()
```

* [**Residuales**]{.hl-yellow}: adem√°s de media cero, parecen presentar una distribuci√≥n sim√©trica con la mediana en torno al cero. Adem√°s se tiene que $\widehat{\sigma}_{\varepsilon}=\sqrt{\frac{1}{n-p-1}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}}= 0.4819$ (estimador insesgado de la desv t√≠pica residual) y $R^2 = 0.4456$ (bondad de ajuste)


---

## Fase de estimaci√≥n

[**Ajuste con harvestRain**]{.hl-yellow}


```{r}
ajuste_harvest |> summary()
```

* $\beta_0=$ `r round(ajuste_harvest$coefficients[1], 3)`: predicci√≥n del precio (esca√±a log) cuando la lluvia fue nula (es decir, si $HarvestRain = 0$, entonces precio es de $2164.308 = exp(7.679856)$ tras deshacer escala log)

* $\beta_1=$ `r round(ajuste_harvest$coefficients[2], 3)`: lo que aumenta el precio (escala log) por cada litro de lluvia de aumento (es decir, por cada unidad que aumenta la precipitaci√≥n el precio se multiplica por $exp(-0.0044) = 0.9956047$ --> disminuye un 0.5%)


---

## Fase de estimaci√≥n

[**Ajuste con harvestRain**]{.hl-yellow}

```{r}
#| echo: false
ajuste_harvest |> summary()
```

* [**Residuales**]{.hl-yellow}: adem√°s de media cero, parecen presentar una distribuci√≥n sim√©trica con la mediana en torno al cero. Adem√°s se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.5577$ (algo m√°s grande que el otro ajuste) y $R^2 = 0.2572$ (algo m√°s peque√±o que el otro ajuste) -> de momento es mejor el primer modelo.



# Clase 10: caso pr√°ctico {#clase-10}

[**Diagnosis**]{style="color:#444442;"}

---

## Fase de diagnosis

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¬øCumplen los datos las [**hip√≥tesis par√°metricas**]{.hl-purple} requeridas para poder hacer inferencia? ¬øC√≥mo modificar los datos para que se cumplan?
  - An√°lisis de residuales
  
. . .

¬øPara qu√© necesitamos comprobar las hip√≥tesis si el c√≥digo no da error?

. . .

Lo √∫nico que hemos hecho es [**obtener un modelo que SOLO sirve para la muestra**]{.hl-yellow}: si queremos extrapolar sus conclusiones a toda la poblaci√≥n e interpretar la inferencia del modelo [**necesitamos verificar ANTES las hip√≥tesis**]{.hl-yellow}. Lo haremos con `{performance}` y `{olsrr}`

---

## Fase de diagnosis

```{r}
library(performance)
check_model(ajuste_AGST)
```

---

## Diagnosis: linealidad

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$


```{r}
#| code-fold: true
check_model(ajuste_AGST)
```

Si te fijas el gr√°fico que se refiere a ello est√° [**visualizando residuales vs valores estimados**]{.hl-yellow}: est√° volviendo a plantear un segundo modelo de regresi√≥n donde ahora $\widehat{\varepsilon}_i = \gamma_0 + \gamma_1 \widehat{y}_i$

---

## Diagnosis: linealidad

```{r}
linealidad <- lm(data = tibble("fitted" = ajuste_AGST$fitted.values,
                               "residuals" = ajuste_AGST$residuals),
                 formula = residuals ~ fitted)
linealidad |> summary()
```

Si te fijas [**ambos par√°metros no son significativamente distintos de 0**]{.hl-green}: no presentan una tendencia (lineal al menos)

---

## Diagnosis: linealidad

```{r}
linealidad <- lm(data = tibble("fitted" = ajuste_AGST$fitted.values,
                               "residuals" = ajuste_AGST$residuals),
                 formula = residuals ~ fitted + I(fitted^2))
linealidad |> summary()
```

Tambi√©n podemos ver si existe una relaci√≥n de otro tipo (por ejemplo cuadr√°tica) entre residuales y valores ajustados

---

## Diagnosis: linealidad

Tambi√©n podemos [**visualizar nosotros ese scatter plot residuales vs estimaciones**]{.hl-yellow}

```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "residuals" = ajuste_AGST$residuals),
       aes(x = fitted, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

---

## Diagnosis: homocedasticidad

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.

```{r}
check_heteroscedasticity(ajuste_AGST)
```

. . .

El gr√°fico titulado `Homogeneity of variance` nos visualiza la ra√≠z cuadrada del valor absoluto de los residuos estandarizados frente a las predicciones (se conoce como [**gr√°fico de escala-localizaci√≥n**]{.hl-yellow})

---

## Diagnosis: homocedasticidad

Si visualizamos los [**residuales**]{.hl-yellow} deber√≠an estar en torno a 0, dentro de una banda constante (varianza constante)


```{r}
#| code-fold: true
ggplot(tibble("id" = 1:length(ajuste_AGST$residuals),
              "residuals" = ajuste_AGST$residuals),
       aes(x = id, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  theme_minimal()
```

---


## Diagnosis: homocedasticidad

Si visualizamos el gr√°fico de [**escala-localizaci√≥n**]{.hl-yellow} deber√≠amos obtener un diagrama de dispersi√≥n cuya recta de regresi√≥n saliese casi plana en torno al 1.


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Seg√∫n el gr√°fico no deber√≠amos asumir homocedasticidad. **¬øPor qu√© el contraste no la rechaza?**

---

## Diagnosis: homocedasticidad


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Con el poco tama√±o muestral que tenemos, es complicado tener evidencias que refuten la hip√≥tesis nula (y el gr√°fico puede estar parcialmente dise√±ado). Por eso es la [**hip√≥tesis m√°s dif√≠cil de cumplir**]{.hl-yellow}. Lo importante es que en [**la recta de regresi√≥n al dibujar los residuos no se aprecia una banda cuya anchura se modifique groseramente**]{.hl-green}, m√°s o menos constante


---

## Diagnosis: normalidad

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

Con la funci√≥n `ols_test_normality()` del paquete `{olsrr}` podemos obtener diferentes contrastes de normalidad

```{r}
library(olsrr)
ols_test_normality(ajuste_AGST)
```

Nos centraremos en los contrastes de `Shapiro-Wilk` si la muestra es peque√±a, y `Kolmogorov-Smirnov` en caso contrario: [**no se rechaza normalidad**]{.hl-yellow}


---

## Diagnosis: normalidad

Adem√°s del contraste podemos visualizar con `stat_qq()` y `stat_qq_line()` el conocido como [**Q-Q plot**]{.hl-yellow}: enfrenta los cuantiles de una muestra con los cuantiles de una normal te√≥rica, teniendo que **obtener los puntos en torno a una recta** (especilamente en el centro).


```{r}
#| code-fold: true
ggplot(tibble("residuals" = ajuste_AGST$residuals)) +
  stat_qq(aes(sample = residuals)) +
  stat_qq_line(aes(sample = residuals))+
  theme_minimal()
```


---

## Diagnosis: independencia

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

```{r}
check_autocorrelation(ajuste_AGST)
```

Por √∫ltimo, `check_autocorrelation()` comprueba como efectivamente los [**residuales/errores son independientes**]{.hl-yellow}, haciendo un test de autocorrelaci√≥n (nos tiene que salir lo contrario a una serie temporal, que el error i no depende del i-1).

---

## Diagnosis: independencia

Otra forma de verlo es [**visualizando los residuos respecto a su versi√≥n con retardo**]{.hl-yellow} (por ejemplo, $\left(\widehat{\varepsilon}_1, \widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_{n-1} \right)$ vs $\left(\widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_n \right)$ 

```{r}
#| code-fold: true
ggplot(tibble("lag1" = ajuste_AGST$residuals[-length(ajuste_AGST$residuals)],
              "residuals" = ajuste_AGST$residuals[-1]),
       aes(x = residuals, y = lag1)) +
  geom_point(size = 3, alpha = 0.7) + 
  geom_smooth(method = "lm") + 
  theme_minimal()
```

---

## Fase de diagnosis

```{r}
check_model(ajuste_AGST)
```

En nuestro caso se cumplen todas las hip√≥tesis (algunas m√°s fuertemente que otras).

[**Repite el proceso con el otro modelo**]{.hl-yellow}

---

## Fase de diagnosis

```{r}
check_predictions(ajuste_AGST)
```

Nos faltan dos gr√°ficas por comentar:

* `Posterior Predictive Checks`: [**simula distintas variables respuesta**]{.hl-yellow} suponiendo que el modelo fuese cierto (a√±adiendo ruido aleatorio) y lo compara con la muestra. [**Si lo observado se distancia mucho de las simulaciones**]{.hl-red} es que el modelo planteado no ajusta bien a la muestra.

---

## Fase de diagnosis

```{r}
check_outliers(ajuste_AGST)
```

* `Influential Observations`: nos permite identificar [**observaciones influyentes**]{.hl-yellow}, marcando aquellas (con su id de fila) que se salgan fuera de la banda definida por la conocida como **distancia de Cook** denotada como $D_i$ (realiza, para cada observaci√≥n, la suma de todos los cambios de la regresi√≥n cuando la observaci√≥n $i$ es retirada: si hay muchos cambios al cambiar una observaci√≥n, es que era muy influyente)

. . .

Diferencia dos tipos: 

* [**outliers**]{.hl-yellow}: valor at√≠pico de la **respuesta** pudiendo perturbar la varianza residual
* [**high-leverage points**]{.hl-yellow}: valor at√≠pico en alguna de las **predictoras**

---

## Fase de inferencia

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¬øQu√© [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro par√°metros?
  - ¬øLas predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¬øDebemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

&nbsp;

Una vez **verificadas las hip√≥tesis** lo que haremos ser√° [**inferir conclusiones de la poblaci√≥n en funci√≥n de la muestra**]{.hl-yellow}

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Variabilidad**]{.hl-yellow} de las estimaciones de nuestro par√°metros

  - $\widehat{SE} \left( \widehat{\beta}_0 \right)$ igual a 2.3641 por lo que $\widehat{\beta_0} \sim N(-3.5469, \sigma = 2.3641)$
  - $\widehat{SE} \left( \widehat{\beta}_1 \right)$ igual a 0.1434  por lo que que $\widehat{\beta_1} \sim N(0.6426, \sigma = 0.1434)$
  

---

## Fase de inferencia

```{r}
#| echo: false
ajuste_AGST |> summary()
```

* [**Estad√≠stico**]{.hl-yellow} del contraste

  - $\frac{\widehat{\beta}_0 - 0}{\widehat{SE} \left( \widehat{\beta}_0 \right)}$ igual a -1.5 (valor que tendr√≠as que buscar en las tablas a mano)
  - $\frac{\widehat{\beta}_1 - 0}{\widehat{SE} \left( \widehat{\beta}_1 \right)}$ igual a 4.483 (valor que tendr√≠as que buscar en las tablas a mano)
  
  
  
---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Efecto (lineal)**]{.hl-yellow}: si nos fijamos en la tabla, el p-valor de $\beta_0$ es 0.146052. Si adoptamos $\alpha = 0.05$ como suele ser habitual, el contraste $H_0:~\beta_0 = 0$ vs $H_1:~\beta_0 \neq 0$ nos dice que [**no podemos rechazar de forma significativa la hip√≥tesis nula**]{.hl-red} (no sucede con $\beta_1$, si sucediese no habr√≠a modelo)

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

[**¬øY si quitamos dicho par√°metro?**]{.hl-yellow}

. . .

Para quitarlo a√±adimos un -1 al modelo

```{r}
ajuste_AGST_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + AGST)
```
 
---

## Re-aprendiendo

```{r}
ajuste_AGST_sin_beta0 |> summary()
```

* La [**bondad de ajuste**]{.hl-yellow} ha pasado de $R^2 = 0.446$ a $R^2 = 0.9953$ (veremos que no son comparables cuando tiene o no intercepto)

* La [**variabilidad de la estimaci√≥n**]{.hl-yellow} $\widehat{SE} \left( \widehat{\beta}_1 \right)$  ha pasado de 0.143 a 0.005757 --> m√°s precisi√≥n

---

## üê£ Caso pr√°ctico I: ejercicios hoja a mano {#caso-practico-10-1}

Vamos a hacer en R el ejercicio 5 de los ejercicios de la hoja para hacer mano (reminder: deber√≠as saber hacerlos con boli y papel)

```{r}
#| eval: false
datos <- 
  tibble("temp_avg" = c(16, 17.2, 18, 17.2, 16.9, 17.1, 18.2, 17.3, 17.5, 16.6),
         "n_dias" = c(58, 82, 81, 65, 61, 48, 61, 43, 33, 36))
```

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-i-hoja-a-mano)

---

## üê£ Caso pr√°ctico II: incumpliendo hip√≥tesis {#caso-practico-10-2}

Vamos a generar distintos ejemplos que incumplen las hip√≥tesis.

&nbsp;

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#caso-pr%C3%A1ctico-ii-incumpliendo-hip%C3%B3tesis)


---

## üê£ Caso pr√°ctico III: colesterol {#caso-practico-10-3}

Vamos a intentar predecir los niveles de colesterol de 100 pacientes en funci√≥n de otras variables predictoras

```{r}
#| eval: false
datos <- read_csv(file = "./datos/colesterol.csv")
```


Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#clase-10)


# Clase 11-12: caso pr√°ctico {#clase-11}

[**Evaluaci√≥n**]{style="color:#444442;"}

---

## Comparar modelos

Aunque no hemos hablado en profundidad de las **m√©tricas de evaluaci√≥n** podemos [**comparar los modelos**]{.hl-yellow} con `compare_performance()` del paquete `{performance}`


```{r}
compare_performance(ajuste_AGST, ajuste_AGST_sin_beta0, ajuste_harvest)
```

---

## Fase de evaluaci√≥n

7. [**Fase de evaluaci√≥n**]{.hl-yellow}:
  - ¬øEs significativo el modelo? [**ANOVA: an√°lisis de la varianza**]{.hl-purple}
  - ¬øQu√© informaci√≥n de la predictora explica el modelo? [**Par√°metros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)
  - ¬øQu√© otras m√©tricas o herramientas podemos usar para [**cuantificar la calidad predictora de nuestro ajuste**]{.hl-purple}
  
. . . 

Una de las herramientas m√°s √∫tiles para evaluar nuestro modelo es [**enfrentar los valores ajustados con los valores reales**]{.hl-yellow} (dado que los conocemos al ser aprendizaje supervisado)

---

## Fase de evaluaci√≥n


```{r}
#| code-fold: true
ggplot(tibble("y" = datos$Price, "y_est" = ajuste_AGST$fitted.values),
       aes(x = y, y = y_est)) +
  geom_point(size = 1.2, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(x = "Valores reales", y = "Valores estimados")
```

En el gr√°fica podemos ver como los [**valores reales vs estimados**]{.hl-green} est√°n **muy cercanos a la diagonal**: el error cometido es muy peque√±o.

---

## Fase de evaluaci√≥n


¬øC√≥mo [**cuantificar el acierto del modelo**]{.hl-yellow}?

. . .

Y para saber c√≥mo cuantificar el acierto la primera pregunta que deber√≠amos hacernos es: ¬øcu√°l es el **objetivo de un modelo** que pretende predecir una variable continua $Y$? ¬øQu√© nos gustar√≠a que pasase si el modelo fuese perfecto?

. . .

Si construimos un **modelo predictivo para $Y$**, lo ideal ser√≠a que nuestro modelo puede capaz de [**capturar/explicar toda la variabilidad respecto a la media de Y**]{.hl-yellow} (por qu√© sube cuando sube, por qu√© baja cuando baja, etc). ¬øY qu√© usamos en estad√≠stica para hablar de **informaci√≥n**?



---

## Fase de evaluaci√≥n

Vamos a [**cuantificar la informaci√≥n de $Y$**]{.hl-yellow} (respecto a su media) como su varianza (o suma total de cuadrados $SST$), representada como

$$s_{y}^2 := \frac{1}{n} \sum_{i=1}^{n} \left(y_i - \overline{y} \right)^2 = \frac{1}{n} SST$$

. . .

¬øEl **objetivo del modelo**? Ser capaces de construir una variable predicha $\widehat{Y}$ que consiga [**capturar la mayor informaci√≥n de $Y$**]{.hl-yellow} posible. ¬øC√≥mo podr√≠amos **cuantificar la informaci√≥n que captura esa predicci√≥n** $\widehat{Y}$?


---

## Fase de evaluaci√≥n

Vamos a [**cuantificar la informaci√≥n de $\widehat{Y}$**]{.hl-yellow} de nuevo como su varianza (o suma de cuadrados explicada $SSE$), representada como

$$s_{\widehat{y}}^2 := \frac{1}{n} \sum_{i=1}^{n} \left(\widehat{y}_i - \overline{\widehat{y}} \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left(\widehat{y}_i - \overline{y} \right)^2 = \frac{1}{n} SSE$$

ya que la media de la variable predicha es (reminder: $\widehat{\beta}_0 =  \overline{y}-  \widehat{\beta}_1 \overline{x}$)

$$\overline{\widehat{y}}  = \frac{1}{n}  \sum_{i=1}^{n}\widehat{y}_i = \frac{1}{n}  \sum_{i=1}^{n} \left(\widehat{\beta}_0 + \widehat{\beta}_1 x_i \right) = \widehat{\beta}_0+\widehat{\beta}_1 \overline{x} = \overline{y}$$

---

## Fase de evaluaci√≥n


* [**Informaci√≥n a modelizar**]{.hl-yellow}: $s_{y}^2 = \frac{1}{n} SST$

* [**Informaci√≥n modelizada**]{.hl-yellow}: $s_{\widehat{y}}^2 = \frac{1}{n} SSE$

¬øQu√© querr√≠amos si nuestro **modelo fuese perfecto** en t√©rminos de acierto? El objetivo ideal es que $s_{y}^2 = s_{\widehat{y}}^2$ (**modelo explica todo lo explicable**).

. . .

Vamos a construir con ambas cantidades un [**ratio de informaci√≥n explicada**]{.hl-yellow}, tambi√©n conocido como [**bondad de ajuste o coeficiente de determinaci√≥n o $R^2$**]{.hl-yellow} (lo que `R` llama `Multiple R-squared`)


$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{y}^2}{s_{\widehat{y}}^2} = \frac{SSE}{SST} \in [0,1]$$

* Si $R^2 \to 1$ --> modelo explica casi todo lo explicable


---

## Fase de evaluaci√≥n

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{y}^2}{s_{\widehat{y}}^2} = \frac{SSE}{SST} \in [0,1]$$


Esta definici√≥n de $R^2$ es [**general para cualquier modelo de aprendizaje estad√≠stico**]{.hl-yellow} pero vamos a ver si, en el **caso de un modelo de regresi√≥n linea**, podemos relacionarlo con algo que conozcamos

. . .

Vamos a tomar la informaci√≥n que queremos explicar y vamos a realizar una descomposici√≥n similar a la que hicimos con el **ANOVA one-way (ANOVA cuali vs cuanti)** 

$$\begin{eqnarray}SST &=&  \sum_{i=1}^{n} \left(y_i - \overline{y} \right)^2 =  \sum_{i=1}^{n} \left[\left(y_i -  \widehat{y}_i \right) + \left( \widehat{y}_i - \overline{y} \right) \right] ^2 \nonumber \\ &=&  \sum_{i=1}^{n} \left(y_i - \widehat{y}_i \right)^2 + \sum_{i=1}^{n} \left(\widehat{y}_i  - \overline{y}\right)^2 + 2 \sum_{i=1}^{n}  \left(y_i - \widehat{y}_i \right)\left(\widehat{y}_i  - \overline{y}\right)  \end{eqnarray}$$

---

## Fase de evaluaci√≥n

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{y}^2}{s_{\widehat{y}}^2} = \frac{SSE}{SST} \in [0,1]$$

As√≠, asumiendo que los errores est√°n incorrelados,

$$\begin{eqnarray}SST &=&    \sum_{i=1}^{n} \left(y_i - \widehat{y}_i \right)^2 + \sum_{i=1}^{n} \left(\widehat{y}_i  - \overline{y}\right)^2 + 2 \sum_{i=1}^{n}  \left(y_i - \widehat{y}_i \right)\left(\widehat{y}_i  - \overline{y}\right) \nonumber \\ &=&  \sum_{i=1}^{n} \left(y_i - \widehat{y}_i \right)^2 + \sum_{i=1}^{n} \left(\widehat{y}_i  - \overline{y}\right)^2 =\sum_{i=1}^{n}  \widehat{\varepsilon}_{i}^2 + \sum_{i=1}^{n} \left(\widehat{y}_i  - \overline{y}\right)^2 \nonumber \\ &=& SSR + SSE \end{eqnarray}$$

. . .

[**SOLO en el caso lineal**]{.hl-yellow} tenemos que la informaci√≥n total puede descomponerse entre la [**explicada**]{.hl-green} m√°s la [**no explicada**]{.hl-red}

---

## Fase de evaluaci√≥n

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{\widehat{y}}^2}{s_{y}^2} = \frac{SSE}{SST}, \quad SST = SSE + SSR$$

Reescribiendo por tanto la expresi√≥n anterior tenemos que para el **caso lineal**

$$R^2 = \frac{s_{\widehat{y}}^2}{s_{y}^2} = \frac{SSE}{SST} = \frac{SST - SSR}{SST} = 1 - \frac{SSR}{SST} = 1 - \frac{s_{r}^2}{s_{y}^2}$$
¬øDe qu√© **depende la info no explicada $s_{r}^2$**?

. . .


$$R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{s_{r}^2}{s_{y}^2} = 1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^2}{s_{y}^2}$$

---

## Fase de evaluaci√≥n

$$R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{s_{r}^2}{s_{y}^2} = 1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^2}{s_{y}^2}$$

[**¬øDe qu√© depende?**]{.hl-yellow}

. . .

* [**M√°s predictoras (p)**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow}, ¬°incluso aunque dichas [**predictoras no sean significativas ni √∫tiles**]{.hl-red}! (lo arreglaremos usando el $R^2$ ajustado)

. . .

* [**A igualdad de errores: menos varianza de $y$, menor $R^2$**]{.hl-yellow} 

. . .

* [**M√°s ruido, menor $R^2$**]{.hl-yellow} (aunque el modelo sea igual de bueno)

. . .

* [**Ignora si el modelo cumple las hip√≥tesis**]{.hl-yellow}: un modelo con un alto $R^2$ puede dar predicciones nefastas si las incumple


---

## Resumen R2


$$R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{s_{r}^2}{s_{y}^2} = 1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^2}{s_{y}^2}$$

* [**¬øEs √∫til?**]{.hl-yellow} S√≠, nos puede servir para **comparar** entre modelos con complejidades similares aplicados a datos similares.

. . .

* [**¬øNos habla sobre la inferencia del modelo o la verificaci√≥n de sus hip√≥tesis?**]{.hl-yellow} No. Siempre **asume que se cumplen**, ignorando completamente si es cierto o no. No es un contraste.

. . .


* [**¬øDebe ser interpretado con cuidado y no de forma ¬´sagrada¬ª?**]{.hl-yellow} No. Depende de $p$ (tengan o no utilidad), de la varianza de $y$ y del ruido de los datos

---

## Resumen R2


$$R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{s_{r}^2}{s_{y}^2} = 1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^2}{s_{y}^2}$$

* [**¬øRepresenta la proporci√≥n de variabilidad de $y$ (en general)?**]{.hl-yellow} No. El coeficiente $R^2$ compara la explicabilidad del modelo **respecto a un modelo de referencia (modelo nulo)**, aquel en el que $\widehat{y} = \overline{y} = cte$ (de ah√≠ que $SSE$ - $y$ vs $\widehat{y}$ - se divida por $SST$, donde $y$ vs $\overline{y}$). Ese modelo de referencia es aquel con pendiente nula $\widehat{\beta}_1 = 0$, por lo que $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x} = \overline{y}$

---

## Resumen R2


$$R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{s_{r}^2}{s_{y}^2} = 1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^2}{s_{y}^2}$$

* [**¬øSirve la salida de $R^2$ cuando $\widehat{\beta}_0 = \beta_0 = 0$?**]{.hl-yellow} No. Dado que el modelo nulo respecto al que se compara es aquel que tiene $\widehat{\beta}_1 = 0$ donde $\widehat{\beta}_0  = \overline{y}$, el modelo de referencia ahora ser√≠a aquel con $\overline{y} = 0$. Por ello la funci√≥n `lm()` te devuelve en este caso un valor $R_{0}^2$ definido como

$$R_{0}^2 =  1 - \frac{SSR}{SST_0} = 1 - \frac{\sum_{i=1}^{n} \left(y_i - \widehat{y}_i \right)^2}{\sum_{i=1}^{n} y_{i}^2}$$

Esto hace que dicho valor puede ser **muy elevado** ya que lo compara respecto a $\widehat{y} = 0 = cte$ (malo ser√° que no lo mejore). De hecho en este caso $SST \neq SSR + SSE$ sino que $SST_0 = SSE_0 + SSR$

---

## ANOVA

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{\widehat{y}}^2}{s_{y}^2} = \frac{SSE}{SST} = 1 - \frac{SSR}{SST} $$

Al igual que pasaba en el ANOVA cualis vs cuanti, podr√≠amos plantearnos la siguiente pregunta: ¬øconocemos la [**distribuci√≥n de $SSE$ y $SSR$**]{.hl-yellow}?


. . .


Dado que de nuevo cada uno es una suma de normales al cuadrado (ergo una chi-cuadrado) se puede **demostrar** como 

$$\frac{SSE}{\sigma_{\varepsilon}^2} \sim \chi^2_{p}, \quad \frac{SSR}{\sigma_{\varepsilon}^2} \sim \chi^2_{n-p-1}$$

. . .



$$F = \frac{\frac{SSE}{p}}{\frac{SSR}{n-p-1}}  \sim F_{p, n-p-1}$$

---

## ANOVA

```{r}
ajuste_AGST |> aov() |> summary()
```


|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| x1  | p  | SSE | $\frac{SSE}{p}$ | $F-value = \frac{\frac{SSE}{p}}{\frac{SSR}{n-p-1}}$ | p-valor F test
| Residuals  | n - p - 1 | SSR | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSR}{n-p-1}$ | | |

El F-value es  $F = \frac{\frac{SSE}{p}}{\frac{SSR}{n-p-1}} \sim F_{p, n-p-1}$ asociado al [**contraste de sig global**]{.hl-yellow}

$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0~(j \geq 1)$$

---

## ANOVA

```{r}
ajuste_AGST |> aov() |> summary()
```

$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0~(j \geq 1)$$

El contraste pretende responder a: [**¬øexiste una dependencia lineal entre $Y$ y el CONJUNTO de predictoras?**]{.hl-yellow} (global, no par√°metro a par√°metro).

. . .

Si se [**rechaza**]{.hl-yellow} significa que [**existe al menos un predictor cuyo efecto LINEAL sobre Y es significativo**]{.hl-yellow}.

. . .

[**Importante**]{.hl-red}: en el caso de la reg. lineal univariante, $F-value$ y $p-value$ del ANOVA es equivalente al $t-value$ y $p-value$ del contraste de significaci√≥n para $\beta_1$ (ya que...no hay m√°s).


---

## üê£ Caso pr√°ctico I: simulaci√≥n {#caso-practico-11-1}

Intenta responder a las preguntas planteadas en el [**workbook**](https://dadosdelaplace-workbook-supervisado.share.connect.posit.cloud/#clase-11)


---



## üê£ Caso pr√°ctico II: vuelta a iris y colesterol {#caso-practico-11-2}

pendiente


# Clase 13: reg. multivariante {#clase-13}

[**Introducci√≥n a la regresi√≥n multivariante. Formulaci√≥n del modelo y estimaci√≥n**]{style="color:#444442;"}

---

## Regresi√≥n multivariante

De aqu√≠ en adelante llamaremos [**modelo multivariante**]{.hl-yellow} a todo modelo en el que $p>1$ (es decir, tenemos m√°s de una variable predictora).

$$Y = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right)$$
tal que $E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0$.

. . .

En el caso del [**modelo lineal multivariante **]{.hl-yellow}se traducir√° por tanto en

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j$$

El objetivo seguir√° siendo obtener la estimaci√≥n de los $\widehat{\beta}$ tal que minimicemos el error (la suma de errores al cuadrado).
 
--- 

## Formulaci√≥n matricial

Si tenemos $n$ observaciones y $p$ predictoras, su [**formulaci√≥n matricial te√≥rica**]{.hl-yellow} la podemos expresar mediante la **matriz de dise√±o**

$$\mathbf{X} =\begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}, \quad Y_i =  \beta_0 + \displaystyle \sum_{j=1}^{p} \beta_j X_{ij} + \varepsilon_i$$

tal que


$$\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \beta_0 \\ \vdots \\ \beta_p \end{pmatrix}_{(p+1)\times1} + \begin{pmatrix} \varepsilon_1 \\  \vdots \\ \varepsilon_n \end{pmatrix}_{n\times1}$$

---

## Formulaci√≥n matricial

Dicho modelo te√≥rico ser√° estimado tal que

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$

---

## M√≠nimos cuadrados

Como pasaba en el modelo univariante, nuestro objetivo ser√° encontrar que [**vector de par√°metros minimiza la suma de errores al cuadrado**]{.hl-yellow}

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)\right]^2$$


¬øC√≥mo quedar√≠a **matricialmente**?

. . .


$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = \varepsilon^{T} \varepsilon$$

por lo que, de todos los $\boldsymbol{\beta}$ posibles, el [**vector √≥ptimo de par√°metros**]{.hl_yellow} $\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p\right)$ ser√° aquel que minimice $SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}= \varepsilon^{T} \varepsilon$

---

## M√≠nimos cuadrados

Para hallar el m√≠nimo de la funci√≥n $SSE \left(\boldsymbol{\beta} \right)$ (donde $\boldsymbol{\beta}$ es el argumento) calcularemos la derivada respecto a $\boldsymbol{\beta}$ e igualaremos a 0

. . .

Teniendo en cuenta que $\left(A B \right)^T = B^T A^T$

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} \nonumber \\  &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}}\right)^{T} + \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}}  \nonumber \\  &=&  \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}}\right)^{T} -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} \nonumber \\  &=&  \left( \left(\mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}}\right)^{T} - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X}\end{eqnarray}$$

---

## M√≠nimos cuadrados

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}} \right)^T -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X}\end{eqnarray}$$ 
¬øCu√°nto vale $\frac{\partial \left(  - \boldsymbol{\beta}^{T} A  \right)}{\partial \boldsymbol{\beta}}$ para una matriz $A$ de constantes cualesquiera?

. . .


$$\begin{eqnarray}\frac{\partial \left(  - \boldsymbol{\beta}^{T} A  \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial}{\partial \boldsymbol{\beta}}\left(-\beta_0, \ldots, -\beta_p\right)\begin{pmatrix} a_{11} & \ldots & a_{1q} \\ \vdots & \ddots & \vdots \\
a_{p1} & \ldots & a_{pq}
\end{pmatrix} = - \frac{\partial}{\partial \boldsymbol{\beta}} \left(\sum_{i=1}^{p} \beta_i a_{i1}, \ldots, \sum_{i=1}^{p} \beta_i a_{iq}\right) \nonumber \\ &=& - \begin{pmatrix} \frac{\partial}{\partial \beta_1} \sum_{i=1}^{p} \beta_i a_{i1} & \ldots & \frac{\partial}{\partial \beta_p} \sum_{i=1}^{p} \beta_i a_{i1} \\ \vdots & \ddots & \vdots \\
\frac{\partial}{\partial \beta_1} \sum_{i=1}^{p} \beta_i a_{iq} & \ldots & \frac{\partial}{\partial \beta_p} \sum_{i=1}^{p} \beta_i a_{iq}
\end{pmatrix} \nonumber \\ &=& - \begin{pmatrix} a_{11} & \ldots & a_{p1} \\ \vdots & \ddots & \vdots \\
a_{1q} & \ldots &  a_{pq}
\end{pmatrix} = -A^{T}\end{eqnarray}$$


---


## M√≠nimos cuadrados

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}} \right)^T -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X}\end{eqnarray}$$  

Usando dicho resultado tenemos por tanto que

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}} \right)^T -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} \nonumber \\ &=& - \left(\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)\right)^T  -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} \nonumber \\ &=& -2\left(\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)\right)^T = 0 \end{eqnarray}$$ 

. . .

Eso es equivalente a decir que $\widehat{\boldsymbol{\beta}}$ ser√° aquel vector que

$$\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \widehat{\boldsymbol{\beta}} \right) = 0$$ 

---

## M√≠nimos cuadrados

$$\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \widehat{\boldsymbol{\beta}} \right) = 0$$ 

Despejando tenemos que

$$\mathbf{X}^{T} \mathbf{Y} - \mathbf{X}^{T}\mathbf{X} \widehat{\boldsymbol{\beta}}  = 0 \Rightarrow \widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T} \mathbf{Y} $$

. . .

[**IMPORTANTE**]{.hl-yellow}: para que exista soluci√≥n (√∫nica) necesitamos que $\mathbf{X}^{T}\mathbf{X}$ sea invertible, es decir, que $\left| \mathbf{X}^{T}\mathbf{X} \right| \neq 0$. 


---

## Estimaci√≥n multivariante

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$

As√≠ la estimaci√≥n muestral ser√° $\widehat{\mathbf{Y}} = X \widehat{\boldsymbol{\beta}} = X\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T} \mathbf{Y}$ tal que $H:=X\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}$ se conoce
como [**hat matrix o matriz de proyecci√≥n**]{.hl-yellow} (ya que hace que las estimaciones $\widehat{y}$ sean en realidad los valores y proyectados verticalmente sobre el plano de regresi√≥n ajustado).

---

## Deberes

Demuestra que $\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T} \mathbf{Y}$, cuando $p = 1$, coincide con las expresiones ya conocidas

$$\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^2}$$
$$\widehat{\beta}_0 =  \overline{y} - \widehat{\beta}_1 \overline{x}$$