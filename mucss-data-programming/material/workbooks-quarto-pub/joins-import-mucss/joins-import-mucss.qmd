---
title: "Joins and imports: how to combine, import and export files in R. Factors and lists"
subtitle: "Practical workbooks of Data Programming in Master in Computational Social Science  (2024-2025)"
author: "Javier √Ålvarez Li√©bana"
format:
  html:
    theme: [default, style.scss]
    toc: true
    toc-title: √çndice
    toc-depth: 5
    toc-location: left
    number-sections: true
embed-resources: true
execute: 
  echo: true
---

```{r}
#| echo: false
setwd(dir = getwd())
```


## Joins

When working with data we will not always have the information in a single table, and sometimes, we will be interested in [**cross-referencing**]{.hl-yellow} information from different sources.


For this we will use a classic of every language that handles data: the famous [**join**]{.hl-yellow}, a tool that will allow us to [**cross one or several tables**]{.hl-yellow}, making use of a [**identifying column**]{.hl-yellow} of each one of them.

```{r}
#| eval: false
table_1 |>
  xxx_join(table_2, by = id)
```



* `inner_join()`: only [**records with id in both**]{.hl-yellow} tables survive.

* `full_join()`: keeps [**all records in both**]{.hl-yellow} tables.

* `left_join()`: keeps [**all the records of the first table**]{.hl-yellow}, and looks for which ones have id also in the second one (in case of [**not having it, it fills with NA**]{.hl-yellow} the fields of the 2nd table).

* `right_join()`: keeps [**all records in the second table**]{.hl-yellow}, and searches which ones have id also in the first one.

![](img/sql-joins.jpg)    



Let's test the various joins with a simple example

```{r}
library(tidyverse)
tb_1 <- tibble("key" = 1:3, "val_x" = c("x1", "x2", "x3"))
tb_2 <- tibble("key" = c(1, 2, 4), "val_y" = c("y1", "y2", "y3"))
```


```{r}
tb_1
```


```{r}
tb_2
```



### left_join()

Imagine that we want to [**incorporate**]{.hl-yellow} to `tb_1` the [**information from table_2**]{.hl-yellow}, identifying the records by the key column (`by = "key"`, the column it has to cross): we want to keep all the records of the first table and look for which ones have the same value in `key` also in the second one.


```{r}
tb_1 |> 
  left_join(tb_2, by = "key")
```


![](img/left_join.jpg)




```{r}
tb_1 |>
  left_join(tb_2, by = "key")
```

Notice that the [**records in the first one whose key was not found in the second one**]{.hl-yellow} has given them the value of [**absent**]{.hl-yellow}.

### right_join()

The `right_join()` will perform the opposite operation: we will now [**incorporate**]{.hl-yellow} to `tb_2` the [**information from table_2**]{.hl-yellow}, identifying the records by the `key` column: we want to keep all the records of the second one and look for which ones have id (same value in `key`) also in the first table.


```{r}
tb_1 |> 
  right_join(tb_2, by = "key")
```



![](img/right_join.jpg)


```{r}
tb_1 |>
  right_join(tb_2, by = "key")
```

Notice that now the [**records of the second one whose key was not found in the first one**]{.hl-yellow} are the ones given the value of [**absent**]{.hl-yellow}.

### keys and suffixes

The key columns we will use for the crossover [**will not always be named the same**]{.hl-yellow}.

```{r}
tb_1 <- tibble("key_1" = 1:3, "val_x" = c("x1", "x2", "x3"))
tb_2 <- tibble("key_2" = c(1, 2, 4), "val_y" = c("y1", "y2", "y3"))
```


* `by = c("key_2" = "key_2")`: we will indicate in which column of each table are the keys that we are going to cross.


```{r}
# Left
tb_1  |> 
  left_join(tb_2, by = c("key_1" = "key_2"))
```


```{r}
# Right
tb_1 |> 
  right_join(tb_2, by = c("key_1" = "key_2"))
```




We can also [**cross over several columns at the same time**]{.hl-yellow} (it will interpret as equal record the one that has the same set of keys), with `by = c("var1_t1" = "var1_t2", "var2_t1" = "var2_t2", ...)`. Let's modify the previous example

```{r}
tb_1 <- tibble("k_11" = 1:3, "k_12" = c("a", "b", "c"),  "val_x" = c("x1", "x2", "x3"))
tb_2 <- tibble("k_21" = c(1, 2, 4), "k_22" = c("a", "b", "e"), "val_y" = c("y1", "y2", "y3"))
```


```{r}
# Left
tb_1 |> 
  left_join(tb_2,
            by = c("k_11" = "k_21", "k_12" = "k_22"))
```



```{r}
# Right
tb_1 |> 
  right_join(tb_2,
             by = c("k_11" = "k_21", "k_12" = "k_22"))
```



It could also happen that when crossing two tables, there are [**columns of values named the same**]{.hl-yellow}

```{r}
tb_1 <- tibble("key_1" = 1:3, "val" = c("x1", "x2", "x3"))
tb_2 <- tibble("key_2" = c(1, 2, 4), "val" = c("y1", "y2", "y3"))
```


```{r}
# Left
tb_1 |>
  left_join(tb_2, by = c("key_1" = "key_2"))
```

Notice that [**default adds the suffixes**]{.hl-yellow} `.x` and `.y` to tell us which table they come from. This [**suffix can be specified**]{.hl-yellow} in the optional argument `suffix = ...`, which allows us to [**distinguish the variables**]{.hl-yellow} of one table from another.

```{r}
# Left
tb_1 |>
  left_join(tb_2, by = c("key_1" = "key_2"),
            suffix = c("_table1", "_table2"))
```

### full_join()

The two previous cases form what is known as [**outer joins**]{.hl-yellow}: crosses where observations are kept that appear in at least one table. The third outer join is known as `full_join()` which will [**keep observations from both**]{.hl-yellow} tables, [**adding rows**]{.hl-yellow} that do not match the other table.


```{r}
tb_1  |> 
  full_join(tb_2, by = c("key_1" = "key_2"))
```


![](img/full_join.jpg)



### inner_join()

Opposite the outer join is what is known as [**inner join**]{.hl-yellow}, with `inner_join()`: a join in which only the [**observations that appear in both tables**]{.hl-yellow} are kept, only those records that are patched are kept.


```{r}
tb_1 |> 
  inner_join(tb_2,  by = c("key_1" = "key_2"))
```


![](img/inner_join.png)




Note that in terms of records, `inner_join` if it is commutative, **we don't care about the order of the tables**: the only thing that changes is the order of the columns it adds.



```{r}
tb_1 |> 
  inner_join(tb_2, by = c("key_1" = "key_2"))
```


```{r}
tb_2 |> inner_join(tb_1, by = c("key_2" = "key_1"))
```

### semi/anti_join()

Finally we have two interesting tools to [**filter (not cross) records**]{.hl-yellow}: `semi_join()` and `anti_join()`. The [**semi join**]{.hl-yellow} leaves us in the [**first table the records whose key is also in the second table**]{.hl-yellow} (like an inner join but without adding the info from the second table). And the second one, the anti join, does just the opposite (those that are not).




```{r}
# semijoin
tb_1 |> 
  semi_join(tb_2, by = c("key_1" = "key_2"))
```

```{r}
# antijoin
tb_1 |> 
  anti_join(tb_2, by = c("key_1" = "key_2"))
```



### üíª It's your turn

For the exercises we will use the tables available in the package `{nycflights13}`.

```{r}
library(nycflights13)
```

* **airlines**: name of airlines (with their abbreviation).
* **airports**: airport data (names, longitude, latitude, altitude, etc).
* **flights**, **planes**: flight and aircraft data.
* **weather**: hourly weather data.



::: panel-tabset

### [**Exercise 1**]{.hl-yellow}

üìù From package `{nycflights13}` incorporates into the `flights` table the airline data in `airlines`. We want to maintain all flight records, adding the airline information to the airlines table.


```{r}
#| code-fold: true
#| eval: false
flights_airlines <-
  flights |> 
  left_join(airlines, by = "carrier")
flights_airlines
```

### [**Exercise 2**]{.hl-yellow}

üìù To the table obtained from the crossing of the previous section, include the data of the aircraft in `planes`, but including only those flights for which we have information on their aircraft (and vice versa). 

```{r}
#| code-fold: true
#| eval: false

flights_airlines_planes <- 
  flights_airlines |> 
  inner_join(planes, by = "tailnum")
flights_airlines_planes
```


### [**Exercise 3**]{.hl-yellow}

üìù Repeat the previous exercise but keeping both `year` variables (in one is the year of flight, in the other is the year of construction of the aircraft), and distinguishing them from each other


```{r}
#| code-fold: true
#| eval: false

flights_airlines_planes <- 
  flights_airlines |>
  inner_join(planes, by = "tailnum",
             suffix = c("_flight", "_build_aircraft"))
flights_airlines_planes
```


### [**Exercise 4**]{.hl-yellow}

üìù To the table obtained from the previous exercise includes the longitude and latitude of the airports in `airports`, distinguishing between the latitude/longitude of the airport at destination and at origin.


```{r}
#| code-fold: true
#| eval: false

flights_airlines_planes |> 
  left_join(airports |> select(faa, lat, lon),
            by = c("origin" = "faa")) |> 
  rename(lat_origin = lat, lon_origin = lon) |> 
  left_join(airports |> select(faa, lat, lon),
            by = c("dest" = "faa")) |> 
  rename(lat_dest = lat, lon_dest = lon)
```

### [**Exercise 5**]{.hl-yellow}

üìù Filter from `airports` only those airports from which flights depart. Repeat the process filtering only those airports where flights arrive.


```{r}
#| code-fold: true
#| eval: false

airports |> 
  semi_join(flights, by = c("faa" = "origin"))
airports |> 
  semi_join(flights, by = c("faa" = "dest"))
```

### [**Exercise 6**]{.hl-yellow}

üìù How many flights do we not have information about the aircraft? Eliminate flights that do not have an aircraft ID (other than NA) beforehand.


```{r}
#| code-fold: true
#| eval: false

flights |>
  drop_na(tailnum) |>
  anti_join(planes, by = "tailnum") |>
  count(tailnum, sort = TRUE) 
```

:::


## üê£ Case study I: Beatles and Rolling Stones

We will use to practice simple joins the `band_members` and `band_instruments` datasets already included in the `{dplyr}` package.

```{r}
library(dplyr)
band_members
band_instruments
```

In the first one we have a series of artists and the band they belong to; in the second one we have a series of artists and the instrument they play. Beyond performing the requested actions, try to visualize which final table you would have

### Question 1

> Given the table `band_members`, incorporate the information of what instrument each member plays (`band_instruments`) of the ones you have in that table.



```{r}
#| code-fold: true
left_join_band <-
  band_members |> 
  left_join(band_instruments, by = "name")
```

### Question 2

> Given the `band_members` and `band_instruments` tables, what kind of join should you do to have a complete table, with no absentees, where all band members have their instrument information, and every instrument has a member associated with it?

```{r}
#| code-fold: true
inner_join_band <-
  band_members |>
  inner_join(band_instruments, by = "name")
```

### Question 3

> Given the `band_instruments` table, how to incorporate the information of who plays each instrument (in case we know it)?

```{r}
#| code-fold: true
right_join_band <-
  band_members |>
  right_join(band_instruments, by = "name")

# other option
left_join_instruments <-
  band_instruments |> 
  left_join(band_members, by = "name")
```

### Question 4

> Given the `band_members` and `band_instruments` tables, what kind of join should you do to have a table with all the info, both members and instruments, even if there are members whose instrument you do not know, and instruments whose carrier you do not know?

```{r}
#| code-fold: true
full_join_band <-
  band_members |>
  full_join(band_instruments, by = "name")
```

## üê£ Case study II: income by municipalities

In the file `municipios.csv` we have stored the information of the municipalities of Spain as of 2019.

* The variable `LAU_code` represents the code as local administrative unit in the EU (see more at <https://ec.europa.eu/eurostat/web/nuts/local-administrative-units>).

* The variable `codigo_ine` is formed by joining the province code and the community code (each province has a code from 1 to 52, it does not depend on the ccaa).

```{r}
# 2019 data
mun_data <- read_csv(file = "./datos/municipios.csv")
```

On the other hand, in the file `renta_mun` we have the average per capita income of each administrative unit (municipalities, districts, provinces, autonomous communities,...) for different years.

```{r}
renta_mun <- read_csv(file = "./datos/renta_mun.csv")
```

Before we start let's [**normalize variable names**]{.hl-yellow} using `clean_names()` from the `{janitor}` package.

```{r}
mun_data <-
  mun_data |> 
  janitor::clean_names()
renta_mun <-
  renta_mun |> 
  janitor::clean_names()
```

### Question 1

> Convert to tidydata `renta_mun` obtaining a table of 4 columns: `unit`, `year`, `income` and `codigo_ine` (no absent and each data of the correct type)

```{r}
#| code-fold: true
renta_mun_tidy <-
  renta_mun |> 
  pivot_longer(cols = contains("x"), names_to = "year",
               values_to = "income", names_prefix = "x",
               names_transform = list(year = as.numeric),
               values_drop_na = TRUE)
```

### Question 2

> If you look at the table above we have data from different administrative units that are not always municipalities. Knowing that all municipalities have a 5-character code, filter only those records that correspond to municipal units.

```{r}
#| code-fold: true
renta_mun_tidy <-
  renta_mun_tidy |>
  filter(str_detect(codigo_ine, pattern = "[0-9]{5}") & 
           str_length(codigo_ine) == 5)
```

### Question 3

> Then properly separate the unit variable into two columns: one with the code (which you already have so one of the two should be removed) and the name. Remove any remaining spaces (take a look at the `{stringr}` package options).

```{r}
#| code-fold: true
renta_mun_tidy <-
  renta_mun_tidy |>
  separate(col = "unidad", into = c("cod_rm", "name"), sep = 5) |> 
  select(-cod_rm) |> 
  mutate(name = str_trim(name)) 
```

### Question 4

> In which year was the median income higher? And lower? What was the median income of municipalities in Spain in 2019?

```{r}
summary_renta <-
  renta_mun_tidy |> 
  summarise("mean_income" = mean(income, na.rm = TRUE),
            .by = year)
summary_renta |>
  slice_min(mean_income, n = 1)
summary_renta |>
  slice_max(mean_income, n = 1)

renta_mun_tidy |> 
  filter(year == 2019) |> 
  summarise("median_income" = median(income, na.rm = TRUE))
```


### Question 5

> Do whatever you consider to obtain the province with the highest average income in 2019 and the one with the lowest. Be sure to get its name.

```{r}
#| code-fold: true
summary_by_prov <-
  renta_mun_tidy |> 
  filter(year == 2019) |> 
  left_join(mun_data, by = "codigo_ine", suffix = c("", "_rm")) |> 
  select(-contains("rm")) |> 
  summarise("mean_by_prov" = mean(income, na.rm = TRUE),
            .by = c("cpro", "ine_prov_name"))

summary_by_prov |> 
  slice_max(mean_by_prov, n = 1)

summary_by_prov |> 
  slice_min(mean_by_prov, n = 1)
```

### Question 6

> Obtain from each ccaa the name of the municipality with the highest income in 2019.

```{r}
#| code-fold: true
renta_mun_tidy |> 
  filter(year == 2019) |> 
  left_join(mun_data, by = "codigo_ine", suffix = c("", "_rm")) |> 
  select(-contains("rm")) |> 
  slice_max(income, by = "codauto")
```

## Import and export files


So far we have only used data already loaded in packages but many times [**we will need to import data externally**]{.hl-yellow}. One of the main [**strengths**]{.hl-yellow} of `R` is that we can import data very easily in different formats:


* [**R native formats**]{.hl-yellow}: `.rda`, `.RData` and `.rds` formats.

* [**Rectangular (tabular) data**]{.hl-yellow}: `.csv` and `.tsv` formats

* [**Untabulated data**]{.hl-yellow}: `.txt` format.

* [**Data in excel**]{.hl-yellow}: `.xls` and `.xlsx` formats

* [**Data from SAS/Stata/SPSS**]{.hl-yellow}: `.sas7bdat`, `.sav` and `.dat` formats

* [**Data from Google Drive**]{.hl-yellow}

* [**Data from API's**]{.hl-yellow}: aemet, catastro, twitter, spotify, etc.


### R native formats

The [**simplest**]{.hl-yellow} files to import into `R` (and which usually take up less disk space) are its own [**native extensions**]{.hl-yellow}: files in `.RData`, `.rda` and `.rds` formats. To load the former we simply need to [**use the native**]{.hl-yellow} function `load()` by providing it the file path.

* `RData` file: we are going to import the file `world_bank_pop.RData`, which includes the dataset `world_bank_pop`


```{r}
load("./datos/world_bank_pop.RData")
world_bank_pop
```


* `.rda` file: we will import the airquality dataset from `airquality.rda`

```{r}
load("./datos/airquality.rda")
airquality |> as_tibble()
```


Note that files loaded with `load()` are [**automatically loaded into the environment**]{.hl-yellow} (with the originally saved name), and not only datasets can be loaded: `load()` allows us to load multiple objetcs (not only a tabular data)

Native `.rda` and `.RData` files are a properly way to save your environment.

```{r}
load(file = "./datos/multiple_objects.rda")
```


* `.rds` files: for this type we must use `readRDS()`, and we need to incorporate a [**argument `file`**]{.hl-yellow} with the path. In this case we are going to import [**lung cancer data**]{.hl-purple} from the North Central Cancer Treatment Group. Note that now [**.rds import files are a unique database**]{.hl-yellow}

```{r}
lung_cancer <-
  readRDS(file = "./datos/NCCTG_lung_cancer.rds") |>
  as_tibble()
lung_cancer
```


::: callout-important

## Important

The [**paths**]{.hl-yellow} must always be [**without spaces, √±, or accents**]{.hl-yellow}. 

:::



### Tabular data: readr

The `{readr}` package within the `{tidyverse}` environment contains several useful functions for [**loading rectangular data (without formatting)**]{.hl-yellow}.



* `read_csv()`: `.csv` files whose [**separator is comma**]{.hl-purple}
* `read_csv2()`: [**semicolon**]{.hl-purple}
* `read_tsv()`: [**tabulator**]{.hl-purple}.
* `read_table()`: [**space**]{.hl-purple}.
* `read_delim()`: generic function for [**character delimited files**]{.hl-purple}.



![](img/data-import-readr.png)


All of them need as **argument the file path** plus **other optional** (skip header or not, decimals, etc). See more at <https://readr.tidyverse.org/>


#### .csv, .tsv

The main advantage of `{readr}` is that it [**automates formatting**]{.hl-yellow} to go from a flat (unformatted) file to a tibble (in rows and columns, with formatting).


* File `.csv`: with `read_csv()` we will load [**comma separated**]{.hl-purple} files, passing as [**argument the path**]{.hl-yellow} in `file = ...`. Let's import the `chickens.csv` dataset (about cartoon chickens, why not). If you look at the output it gives us the type of variables.

```{r}
library(readr)
chickens <- read_csv(file = "./datos/chickens.csv")
chickens
```



The [**variable format**]{.hl-yellow} will normally be done [**automatically**]{.hl-yellow} by `read_csv()`, and we can query it with `spec()`.

```{r}
spec(chickens)
```



Although it usually does it well automatically we can [**specify the format explicitly**]{.hl-yellow} in `col_types = list()` (in list format, with `col_xxx()` for each type of variable, for example `eggs_laid` will be imported as character). 


```{r}
chickens <-
  read_csv(file = "./datos/chickens.csv",
           col_types = list(col_character(), col_character(),
                            col_character(), col_character()))
chickens
```


We can even indicate that [**variables we want to select**]{.hl-yellow} (without occupying memory), by indicating it in `col_select = ...` (in list format, with `col_select = ...`).


```{r}
chickens <-
  read_csv(file = "./datos/chickens.csv",
           col_select = c(chicken, sex, eggs_laid))
chickens
```

#### .txt

What happens when the [**separator is not correct**]{.hl-red}?


If we use `read_csv()` it expects the separator between columns to be a comma but, as you can see with the following `.txt`, it interprets everything as a single column: [**has no comma and does not know where to separate**]{.hl-yellow}

```{r}
datos_txt <- read_csv(file = "./datos/massey-rating.txt")
dim(datos_txt)
as_tibble(datos_txt)
```


To do this we have.

* `read_csv2()` when the [**separator is semicolon**]{.hl-yellow}, `read_tsv()` when the [**is a tab**]{.hl-yellow} and `read_table()` when the [**is a space**]{.hl-yellow}.

* `read_delim()` in general.

```{r}
datos_txt <- read_table(file = "./datos/massey-rating.txt")
as_tibble(datos_txt)
```


### Excel data (.xls, .xlsx)

Another key import package will be the `{readxl}` package for [**importing data from Excel**]{.hl-yellow}. Three functions will be key:

* `read_xls()` specific to `.xls`, `read_xlsx()` specific to `.xlsx`.
* `read_excel()`: for both `.xls` and `.xlsx`.


We are going to import `deaths.xlsx` with celebrity death records.

```{r}
library(readxl)
deaths <- read_xlsx(path = "./datos/deaths.xlsx")
deaths
```



```{r}
deaths |> slice(1:6)
```

One thing that is [**very common misfortune**]{.hl-yellow} is that there is some kind of comment or text at the beginning of the file, having to [**skip those rows**]{.hl-yellow}.


We can [**skip these rows**]{.hl-yellow} directly in the load with `skip = ...` (indicating the number of rows to skip).

```{r}
deaths <- read_xlsx(path = "./datos/deaths.xlsx", skip = 4)
deaths
```


In addition with `col_names = ...` we can already rename the columns in the import (if [**provide names assumes 1st line already as a data**]{.hl-yellow})

```{r}
#| code-line-numbers: "2-3"
deaths <-
  read_xlsx(path = "./datos/deaths.xlsx", skip = 5,
            col_names = c("name", "profession", "age", "kids", "birth", "death"))
deaths
```


Sometimes [**Excel dates are incorrectly formatted**]{.hl-red} (surprise): we can use `convertToDate()` from the `{openxlsx}` package to convert it.


```{r}
library(openxlsx)
deaths$death <- convertToDate(deaths$death)
deaths
```
   

We can also [**load an Excel with several sheets**]{.hl-yellow}: to [**indicate the sheet**]{.hl-yellow} (either by its name or by its number) we will use the argument `sheet = ...`.

```{r}
mtcars <- read_xlsx(path = "./datos/datasets.xlsx", sheet = "mtcars")
mtcars
```

We can even indicate the [**range of cells**]{.hl-yellow} to load with `range = ...`.

```{r}
iris <- read_xlsx(path = "./datos/datasets.xlsx", sheet = "iris", range = "C1:E4")
iris
```

### Import from SAS/STATA/SPSS

The `{haven}` package within the tidyverse orbit will allow us to [**import files from the 3 most important payment software**]{.hl-yellow}: SAS, SPSS and Stata.

```{r}
library(haven)

# SAS
iris_sas <- read_sas(data_file = "./datos/iris.sas7bdat")

# SPSS
iris_spss <- read_sav(file = "./datos/iris.sav")

# Stata
iris_stata <- read_dta(file = "./datos/iris.dta")
```


### Export

In the same way that we can import we can also [**export**]{.hl-yellow}

* exported in `.RData` (recommended option for variables stored in `R`). Remember that this extension [**can only be used in `R`**]{.hl-yellow}. To do so, just use `save(object, file = path)`.

```{r}
table <- tibble("a" = 1:4, "b" = 1:4)
save(table, file = "./datos/table.RData")
rm(table) # eliminar
load("./datos/table.RData")
table
```


* exported in `.RData` multiple objects

```{r}
table <- tibble("a" = 1:4, "b" = 1:4)
a <- 1
b <- c("javi", "sandra")
save(table, a, b, file = "./datos/mult_obj.RData")
rm(list = c("a", "b", "table"))
load("./datos/mult_obj.RData")
table
```



* exported in `.csv`. To do this we simply use `write_csv(object, file = path)`.

```{r}
write_csv(table, file = "./datos/table.csv")
read_csv(file = "./datos/table.csv")
```

### Import from new sources

#### Website

One of the main advantages of `R` is that we can make use of all the previous functions of [**import but directly from a web**]{.hl-yellow}, without the need to perform the manual download: instead of passing it the local path we will indicate the [**link**]{.hl-yellow}. For example, we are going to download the covid data from ISCIII (<https://cnecovid.isciii.es/covid19/#documentaci%C3%B3n-y-datos>)

```{r}
#| eval: false
covid_data <-
  read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv")
covid_data
```

```{r}
#| echo: false
covid_data <-
  read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv", n_max = 500)
covid_data
```

#### Wikipedia

The `{rvest}` package, one of the most useful of `{tidyverse}` allows us to import directly from an `html`. For example, to export wikipedia tables just `read_html()` to import the html, `html_element("table")` to extract the table objects, and `html_table()` to convert the html table to `tibble`.

```{r}
library(rvest)
wiki_jump <- 'https://en.wikipedia.org/wiki/Men%27s_long_jump_world_record_progression'
wiki_jump |> read_html() |> 
  html_element("table") |> 
  html_table()
```

#### Google Drive

Another option available (especially if we work with other people working) is to [**import from a Google Drive spreadsheet**]{.hl-yellow}, making use of `read_sheet()` from the `{googlesheets4}` package.

The first time you will be asked for a tidyverse permission to interact with your drive

```{r}
library(googlesheets4)
google_sheet <-
  read_sheet("https://docs.google.com/spreadsheets/d/1Uz38nHjl3bmftxDpcXj--DYyPo1I39NHVf-xjeg1_wI/edit?usp=sharing")
google_sheet
```

#### API (owid)

Another interesting option is the [**data download from an API**]{.hl-yellow}: an intermediary between an app or data provider and our `R`. For example, let's load the `{owidR}` library, which allows us to download data from the web <https://ourworldindata.org/>. For example, the `owid_covid()` function loads without realizing it more than 400 000 records with more than 60 variables from 238 countries.

```{r}
#| eval: false
library(owidR)
owid_covid() |> as_tibble()
```

```{r}
#| echo: false
library(owidR)
owid_covid() |> as_tibble() |> slice(1:7)
```

#### API (aemet)

In many occasions to connect to the API we will first have to [**register and obtain a key**]{.hl-yellow}, this is the case of the `{climaemet}` package to access [**Spanish meteorological data**]{.hl-yellow} (<https://opendata.aemet.es/centrodedescargas/inicio>).


Once we have the API key we register it in our RStudio to be able to use it in the future.

```{r}
#| eval: false
library(climaemet)

# Api key
apikey <- "eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqYXZhbHYwOUB1Y20uZXMiLCJqdGkiOiI4YTU1ODUxMS01MTE3LTQ4MTYtYmM4OS1hYmVkNDhiODBkYzkiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTY2NjQ2OTcxNSwidXNlcklkIjoiOGE1NTg1MTEtNTExNy00ODE2LWJjODktYWJlZDQ4YjgwZGM5Iiwicm9sZSI6IiJ9.HEMR77lZy2ASjmOxJa8ppx2J8Za1IViurMX3p1reVBU"

aemet_api_key(apikey, install = TRUE)
```


```{r}
#| echo: false
library(climaemet)
```


With this package we can do a [**search for stations**]{.hl-yellow} to know both its postal code and its identifier code within the AEMET network

```{r}
stations <- aemet_stations()
stations
```

For example, to get data from the station of the airport of El Prat, Barcelona, the code to provide is `"0076"`, obtaining **hourly data**

```{r}
aemet_last_obs("0076")
```


#### API (US census)

One of the most useful tools in recent years is known as `{tidycensus}`: a tool for [**facilitating the process of downloading census data**]{.hl-yellow} for the United States from `R`.

```{r}
library(tidycensus)
```

* `get_decennial()`: to access the [**census data (US Decennial Census)**]{.hl-yellow}, done every 10 years (years 2000, 2010 and 2020).

* `get_acs()`: to access the [**annual and quinquennial (5 years) ACS (American Community Survey)**]{.hl-yellow} (census != survey)

* `get_estimates()`: to access the [**annual population, birth and death estimates**]{.hl-yellow} (census != survey)

* `get_pums()`: to access the [**microdata (unaggregated data) of the ACS (anonymized at the individual level)**]{.hl-yellow}

* `get_flows()`: to access the [**migration flow**]{.hl-yellow} data.

portar from API (US census)

For example, we will download the **census data** (`get_decennial()`) at the state level (`geography = ‚Äústate‚Äù`) for the population (variable `variables = ‚ÄúP001001‚Äù`) for the year 2010 (see variables in `tidycensus::load_variables()`).

```{r}
total_population_10 <-
  get_decennial(geography = "state", 
  variables = "P001001",
  year = 2010)
total_population_10
```


#### Other options


* `{chessR}`: data from chess matches. See <https://github.com/JaseZiv/chessR>

* `{spotifyr}`: data from Spotify. See <https://www.rcharlie.com/spotifyr/>

* `{gtrendsR}`: data from Google Trends. See <https://github.com/PMassicotte/gtrendsR>

* `{scholar}`: data from <https://github.com/jkeirstead/scholar>

### üíª Your turn 

[**Try to solve the following exercises without looking at the solutions**]{style="color:#444442;"}

::: panel-tabset
### [**Exercise 1**]{.hl-yellow}

üìù The `who` dataset we have used in previous exercises, export it to a native `R` format in the `data` folder of the project

```{r}
#| code-fold: true
#| eval: false
library(tidyr)
save(who, file = "./datos/who.RData")
```

### [**Exercise 2**]{.hl-yellow}

üìù Loads the `who` dataset but from the data folder (import the file created in the previous exercise)

```{r}
#| code-fold: true
#| eval: false
load("./datos/who.RData")
```

### [**Exercise 3**]{.hl-yellow}

üìù Repeats the same (export and import) in 4 formats: `.csv`, `.xlsx`, `.sav` (spss) and `.dta` (stata)

```{r}
#| code-fold: true
#| eval: false

# csv
library(readr)
write_csv(who, file = "./datos/who.csv")
who_data <- read_csv(file = "./datos/who.csv")

# excel
library(openxlsx)
write.xlsx(who, file = "./datos/who.xlsx")
who_data <- read_xlsx(path = "./datos/who.xlsx")

# sas y stata
library(haven)
write_sav(who, path = "./datos/who.sav")
who_data <- read_spss(path = "./datos/who.sav")

write_dta(who, path = "./datos/who.dta")
who_data <- read_dta(path = "./datos/who.dta")
```

### [**Exercise 4**]{.hl-yellow}

üìù Repeat the loading of `who.csv` but only select the first 4 columns already in the load.

```{r}
#| code-fold: true
#| eval: false
who_select <-
  read_csv(file = "./datos/who.csv",
           col_select = c("country", "iso2", "iso3", "year"))
```


:::



## üê£ Case study I: CIS survey

[üìä Data](https://drive.google.com/drive/folders/18Ok6Epqcimszqguj_JTTLuSbs5Ot5Srd?usp=sharing)

We are going to put into practice the loading and preprocessing of a file generated by one of the most used software (SPSS). The file contains data from the CIS (Centro de Investigaciones Sociol√≥gicas) barometer ¬´Perceptions on equality between men and women and gender stereotypes¬ª whose sample work was carried out from November 6 to 14 (4000 interviews of both sexes over 16 years old in 1174 municipalities and 50 provinces).

### Question 1

> Load the file extension `.sav` that you have in the subfolder `CIS-feminism` inside the data folder. After loading normalize variable names with `{janitor}` package.

```{r}
#| code-fold: true
library(haven)
data <-
  read_sav(file = "./data/CIS-feminismo/3428.sav") |> 
  janitor::clean_names()
```

### Question 2

> Calculate using tidyverse the number of distinct values of each variable and eliminate those that only have a constant value. First get the name of those variables and then use them inside a `select()`.

```{r}
#| code-fold: true
rm_variables <-
  data |> 
  summarise(across(everything(), n_distinct)) |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "n_values") |> 
  filter(n_values <= 1) |> 
  pull(variable)
data <-
  data |> 
  select(-rm_variables)
```

### Question 3

> Perform the same action to detect those variables that have a different value for each individual (all values are different). These columns are important, they will be the ids of the respondent, but one is enough (if there are several, delete one of them).

```{r}
#| code-fold: true
rm_variables <-
  data |> 
  summarise(across(everything(), n_distinct)) |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "n_values") |> 
  filter(n_values == nrow(data)) |> 
  pull(variable)
data <-
  data |> 
  # all of them except one of them
  select(-rm_variables[-1])
```

### Question 4

> To understand what the variables mean, you have in the folder `CIS-feminismos` the questionnaire that was made to each respondent in `cues3428.pdf` and the sampling data sheet in `FT3428.pdf`. In these documents you will see how, for example, the variables of the questionnaire almost all begin with `p...`, although we also have other types of variables related to household chores (`tarhog...`), children (`hijomenor...`), childcare (`cuidadohijos...`), care tasks in general (`tareascuid...`) and other variables related to the ideological scale, religion, voting memory, etc. In addition, all variables starting with `ia_xxx` refer to codes regarding possible incidences in the data collection and the variables `peso...` to the type of weighting used in the sampling. Proceed to eliminate these last two type of variables.

```{r}
#| code-fold: true
data <-
  data |> 
  select(-contains("ia_"), -contains("peso"))
```

# Question 5

> Calculate the number of interviews per autonomous community (`ccaa`) and extract the 5 with the fewest interviews.

```{r}
#| code-fold: true
#| eval: false
data |>
  count(ccaa, sort = TRUE)

data |>
  count(ccaa) |> 
  slice_min(n = 5, n)
```


### Question 6

> Use the `{datapasta}` package (see <https://javieralvarezliebana.es/docencia/mucss-data-programming/slides/#/data-pasta>) to copy from the [INE page](https://www.ine.es/jaxiT3/Datos.htm?t=2853) the population of each province and import it into a tibble

```{r}
#| code-fold: true
# this code should be automatically generated by de addin in R Studio
# after installing the R package
population <- 
  tibble::tribble(~Total, ~`47.385.107`,
                    "01 Andaluc√≠a",   "8.472.407",
                       "02 Arag√≥n",   "1.326.261",
      "03 Asturias, Principado de",   "1.011.792",
               "04 Balears, Illes",   "1.173.008",
                     "05 Canarias",   "2.172.944",
                    "06 Cantabria",     "584.507",
              "07 Castilla y Le√≥n",   "2.383.139",
         "08 Castilla - La Mancha",   "2.049.562",
                     "09 Catalu√±a",   "7.763.362",
         "10 Comunitat Valenciana",   "5.058.138",
                  "11 Extremadura",   "1.059.501",
                      "12 Galicia",   "2.695.645",
         "13 Madrid, Comunidad de",   "6.751.251",
            "14 Murcia, Regi√≥n de",   "1.518.486",
  "15 Navarra, Comunidad Foral de",     "661.537",
                   "16 Pa√≠s Vasco",   "2.213.993",
                    "17 Rioja, La",     "319.796",
                        "18 Ceuta",      "83.517",
                      "19 Melilla",      "86.261"
  )
```

### Question 7

> Rename properly the variables of `population` datasets, converts the population to number and separates the variable of the autonomous communities in two, one with the code and another with the name (according to INE), properly processing and trimming the names. See before exercises about `{stringr}` package.

```{r}
#| code-fold: true
population <-
  population |> 
  rename(ccaa = Total, pop = `47.385.107`) |> 
  # First to convert to numeric we need to remove dots
  mutate("pop" = as.numeric(str_replace_all(pop, "\\.", ""))) |> 
  # if we use " " as sep, we split also some names
  separate(col = ccaa, into = c("cod_INE", "name"), sep = 2) |> 
  # From stringr package, str_squish() removes whitespace at the start 
  # and end, and replaces all internal whitespace with a single space.
  mutate(name = str_squish(name))
```

### Question 8

> Add the population information to our CIS table. Then calculate a summary table with the ratio between the percentage of population represented by each community (of the total of Spain) and the percentage of respondents of each community (with respect to the total). Which are the 3 most over-represented and which are the 3 least? Think that if the ratio is higher than 1, it implies that there is a lower proportion of respondents from that community than would correspond to it by population (under-represented)


```{r}
#| code-fold: true
#| eval: false
data <-
  data |> 
  # you need first to convert cod_INE as numeric since both 
  # key columns should be of same type
  left_join(population |> mutate("cod_INE" = as.numeric(cod_INE)),
            by = c("ccaa" = "cod_INE")) |> 
  relocate(name, pop, .after = "ccaa")

prop_surv_pop <-
  data |>
  summarise("prop_surv" = n() / sum(nrow(data)),
            "pop" = unique(pop),
            .by = ccaa) |> 
  mutate("prop_pop" = pop / sum(pop),
         "ratio" = prop_pop / prop_surv)

# overrepresented
prop_surv_pop |> 
  slice_min(ratio, n = 3)

# underrepresented
prop_surv_pop |> 
  slice_max(ratio, n = 3)
```


## Lists

We have already seen that lists are an object in R that allows us to store [**collections of variables of different type**]{.hl-yellow} (as with `data.frame` and `tibble`) but also [**different lengths**]{.hl-purple}, with totally heterogeneous structures (even a list can have inside it another list).

```{r}
name <- "Javi"
age <- 34
marks <- c(7, 8, 5, 3, 10, 9)
parents <- c("Paloma", "Goyo")

list_var <- list("name" = name, "age" = age, "marks" = marks, "parents" = parents)
```


```{r}
list_var$name
list_var$marks
```



---

## Lists


We can also make [**lists with other lists inside**]{.hl-yellow}, so that to access each level we must use the `[[]]` operator.

```{r}
list_of_lists <- list("list_1" = list_var[1:2], "list_2" = list_var[3:4])
names(list_of_lists)
```

```{r}
names(list_of_lists[[1]])
```

```{r}
list_of_lists[[1]][[1]]
```

. . .

We are allowed to store [**n-dimensional data**]{.hl-yellow}!

---

## Lists

One of the disadvantages is that a list [**cannot be vectorized**]{.hl-yellow} immediately, so any arithmetic operation applied to a list will give [**error**]{.hl-red}.

```{r}
#| error: true
data <- list("a" = 1:5, "b" = 10:20)
data / 2
```

. . .

For this purpose, one of the common (but outdated) options is to make use of the `lapply()` family.

```{r}
lapply(data, FUN = function(x) { x / 2})
```

By default, the output of `lapply()`is always a [**list of equal length**]{.hl-yellow}.

---

## Lists


A more flexible and versatile option is to make use of the `{purrr}` package of the `{tidyverse}` environment.

```{r}
library(purrr)
```

This package is intended to mimic the [**functional programming**]{.hl-yellow} of other languages such as Scala or Hadoop's [**map-reduce strategy**]{.hl-yellow} (from Google).

![](img/purrr.png)

---

## Lists

The simplest function of the `{purrr}` package is the `map()` function, which [**applies a vectorized function**]{.hl-yellow} to each of the elements of a list. Let's see a first example applied to vectors

. . .

`map()` allows us to [**"map" each list**]{.hl-yellow} and apply the function element by element (if applicable).

```{r}
x <- list("x1" = 1:4, "x2" = 11:20)
map(x, sqrt) 
```

. . .


::: callout-warning
## Be careful

With vectors we have a default vectorization because `R` performs element-by-element operations. Note that, by default, the [**output of `map` is a list**]{.hl-yellow}.
:::


---

## Lists

Let's look at another example. Define in a list two samples from 2 normal distributions, of different sample size and different mean. Compute the mean of each one.

```{r}
#| code-fold: true
x <- list(rnorm(n = 1500, mean = 0, sd = 0.7),
          rnorm(n = 2800, mean = 2, sd = 1.5))
map(x, mean)
```

---

## Lists

What if we want to calculate the mean of their squared values?

```{r}
#| code-fold: true
map(x, function(x) { mean(x^2) })
```


---

## Lists

In addition to being [**more readable and efficient**]{.hl-yellow}, with `{purrr}` we can [**decide the output format**]{.hl-yellow} after the operation

* output as [**numeric (double) vector**]{.hl-purple} with `map_dbl()`
* output as [**numeric (int) vector**]{.hl-purple} with `map_int()`
* output as [**character vector**]{.hl-purple} with `map_chr()`
* output as [**logical vector**]{.hl-purple} with `map_lgl()`

```{r}
map_dbl(x, mean)
map_chr(x, function(x) { glue("Mean is {round(mean(x), 5)}") })
```

---

## Lists

```{r}
c(x[[1]][3], x[[2]][3])
```


Also, if you pass it a [**number**]{.hl-yellow} instead of a function, it will return the [**ith element of each list**]{.hl-yellow}.

```{r}
map_dbl(x, 3)
```

---


## Lists

```{r}
list_dummy <- list("a" = dplyr::starwars, "b" = tidyr::billboard)
```


We can also use `pluck()` to [**access to the i-th element of a list**]{.hl-yellow}

```{r}
list_dummy |> 
  pluck(1)
```


---


## Lists

We also have the option of generalizing it to be able to use functions that [**need two arguments in the form of a list**]{.hl-yellow} (binary operations), with `map2()`


```{r}
x <- list("a" = 1:3, "b" = 4:6)
y <- list("c" = c(-1, 4, 0), "b" = c(5, -4, -1))
map2(x, y, function(x, y) { x^2 + y^2})
```

---

## Lists

We can obtain the output in the form of `data.frame` by adding `list_rbind()` or `list_cbind()`, which [**converts a list into a table**]{.hl-yellow}.


```{r}
x <- c("a", "b", "c")
y <- 1:3
map2(x, y, function(x, y) { tibble(x, y) }) |> list_rbind()
```

---

## Lists

We can generalize it further with `pmap_xxx()` which allows us to use [**multiple arguments (multiple lists)**]{.hl-yellow}.


```{r}
x <- list(1, 1, 1)
y <- list(10, 20, 30)
z <- list(100, 200, 300)
pmap_dbl(list(x, y, z), sum)
```

---

## Lists

We have other types of iterators that, although they assume inputs, do not return anything, as `walk()` (just one input argument), `walk2()` (two arguments) and `pwalk()` (multiple arguments), all [**invisibly return**]{.hl-yellow}, just call a function for its [**side effects**]{.hl-yellow} rather than its return value.

```{r}
list("a" = 1:3, "b" = 4:6) |>
  map2(list("a" = 11:13, "b" = 14:16),
       function(x, y) { x + y }) |> 
  walk(print)
```

---

## üíª It's your turn {#tu-turno-8-1}


::: panel-tabset

### [**Exercise 1**]{.hl-yellow}

üìù Define a list of 4 elements of different types and access the second of them (I will include one that is a tibble so that you can see that in a list there is room for everything).

```{r}
#| code-fold: true
#| eval: false
list_example <-
  list("name" = "Javier", "cp" = 28019,
       "siblings" = TRUE,
       "marks" = tibble("maths" = c(7.5, 8, 9),
                        "lang" = c(10, 5, 6)))
list_example
```

### [**Exercise 2**]{.hl-yellow}

üìù From the list above, access the elements that occupy places 1 and 4 of the list defined above.

```{r}
#| code-fold: true
#| eval: false

list_example[c(1, 4)]

list_example$name
list_example$marks

list_example[c("name", "marks")]
```



### [**Exercise 3**]{.hl-yellow}

üìù  Load the `starwars` dataset from the `{dplyr}` package and access the second movie that appears in `starwars$films` (for each character). Determine which ones do not appear in more than one movie.

```{r}
second_film <- map(starwars$films, 2)
map_lgl(second_film, is.null)
```

:::


---


## Quali: factors

In statistics, when we talk about [**qualitative variables**]{.hl-yellow}, we will call **levels or modalities** the **different values** that these data can take. For example, in the case of the `sex` variable of the `starwars` set, we have 4 allowed levels: `female`, `hermaphroditic`, `male` and `none` (in addition to missing data).

```{r}
starwars |> count(sex)
```



---

## Quali: factors

These kinds of variables are known in `R` as [**factors**]{.hl-yellow}, and the fundamental package to deal with them is `{forcats}` (from the `{tidyverse}` environment). 


![](img/factors.jpg)


---

## Quali: factors

This package allows us to set the [**levels**]{.hl-yellow} (stored internally as `levels`) that a given categorical variable takes so that no mistakes, errors in data collection and generation can be generated. It also makes their analysis less computationally expensive when doing searches and comparisons, giving them a [**different treatment than normal text strings**]{.hl-yellow}.

. . .

Let's see a simple example simulating a `party` variable taking the values `"PP"`, `"PSOE"` and `"SUMAR"` (of size 15)

```{r}
set.seed(1234567)
party <- sample(x = c("PP", "PSOE", "SUMAR"), size = 15, replace = TRUE)
party
```

The `party` variable is currently of [**type text**]{.hl-yellow}, of type `chr`, something we can check with `class(state)`.

```{r}
class(party)
```

---

## Quali: factors

From a statistical and computational point of view, for `R` this variable right now would be equivalent to a named variable. But statistically [**a variable as a string**]{.hl-yellow} is not the same as a categorical variable that [**can only take those 3 levels**]{.hl-yellow}. How to [**convert to factor**]{.hl-yellow}? 

. . .

By making use of the `as_factor()` function from the `{forcats}` package.

```{r}
library(tidyverse)
party_fct <- tibble("id" = 1:length(party),
                    "party" = as_factor(party))
party_fct
```

---

## Quali: factors

Not only the class of the variable  has  changed, but now, below the saved value, the sentence `Levels: ...` appears: these are the [**modalities or levels**]{.hl-yellow} of our qualitative. 

```{r}
party_fct |> pull(party)
```

---


## Quali: factors

Imagine that we are defining the database of deputies of the Congress and that the deputies of the PP did not attend that day to the plenary session: although our variable does not take that value THAT DAY, the state `PSOE` is a [**allowed level in the database**]{.hl-yellow} (so even if we eliminate it, because it is a factor, the level remains, we do not have it now but it is an allowed level).


```{r}
party_fct |> 
  filter(party %in% c("PP", "SUMAR")) |> 
  pull(party)
```

---

## Quali: factors

With `factor()` function we can [**explicitly specify**]{.hl-yellow} the names of the modalities and using `levels = ...` we can explicitly tell it the [**"order" of the modalities**]{.hl-yellow} 


```{r}
party_fct <-
  tibble(id = 1:length(party),
         party = factor(party, levels = c("SUMAR", "PP", "PSOE")))
party_fct |> pull(party)
```


---


## Quali: factors

The previous "order" is just in the sense which it will be counted/plotted first) but [**we don't have (yet) an ordinal variable**]{.hl-purple}


```{r}
party_fct$party < "SUMAR"
```

. . .

What if we want to [**define a qualitative variable ORDINAL**]{.hl-yellow}? Inside `factor()` we must indicate that `ordered = TRUE`.

```{r}
marks <- c("A", "E", "F", "B", "A+", "A", "C", "C", "D", "B", "A", "C", "C", "E", "F", "D", "A+")
marks_database <-
  tibble("student" = 1:length(marks),
         "marks" =
           factor(marks, levels = c("F", "E", "D", "C", "B", "A", "A+"),
                  ordered = TRUE))
marks_database |> pull(marks)
```

---

## Quali: factors

What changes? If you notice now, although the variable is still qualitative, we can [**make comparisons and order the records**]{.hl-yellow} because there is a [**hierarchy**]{.hl-purple} between the modalities.

```{r}
marks_database |> filter(marks >= "B")
```

---

## Quali: factors


:::: columns
::: {.column width="45%"}

If we want to tell it to [**drop an unused level**]{.hl-yellow} at that moment (and that we want to [**exclude from the definition**]{.hl-purple}) we can do it with `fct_drop()`.

```{r}
marks_database |> 
  filter(marks %in% c("F", "E", "D", "C", "B", "A")) |> 
  pull(marks)
```

:::

::: {.column width="55%"}


```{r echo = FALSE,  out.width = "100%", fig.align = "left"}
knitr::include_graphics("./img/drop_factor.jpg")
``` 

:::

::::

```{r}
marks_database |> 
  filter(marks %in% c("F", "E", "D", "C", "B", "A")) |> 
  mutate(marks = fct_drop(marks)) |>  
  pull(marks)
```

---

## Quali: factors


:::: columns
::: {.column width="45%"}


Just as we can delete levels we can [**expand existing levels**]{.hl-yellow} (even if there is no data for that level at that time) with `fct_expand()`.

:::

::: {.column width="55%"}


```{r echo = FALSE,  out.width = "100%", fig.align = "left"}
knitr::include_graphics("./img/factor_expand.jpg")
``` 

:::
::::

```{r}
marks_database |> 
  mutate(marks = fct_expand(marks, c("F-", "A+", "A++"))) %>% 
  pull(marks)
```

---

## Quali: factors

:::: columns
::: {.column width="45%"}

In addition with `fct_explicit_na()` we can [**assign a level to the missing values**]{.hl-yellow} to be included in the analysis and visualizations.

:::

::: {.column width="55%"}


```{r echo = FALSE,  out.width = "100%", fig.align = "left"}
knitr::include_graphics("./img/factor_explicit.jpg")
``` 

:::
::::

```{r}
fct_explicit_na(factor(c("a", "b", NA)))
```

---

## Quali: factors

Even once defined we can [**reorder the levels**]{.hl-yellow} with `fct_relevel()`.


```{r}
marks_database_expand <- 
  marks_database |>  
  mutate(marks = fct_expand(marks, c("F-", "A+", "A++"))) |> 
  pull(marks)

marks_database_expand |> 
  fct_relevel(c("F-", "F", "E", "D", "C", "B", "A", "A+", "A++"))
  
```


---

## Quali: factors

:::: columns
::: {.column width="55%"}

This way of working with qualitative variables allows us to give a [**theoretical definition**]{.hl-yellow} of our database, and we can even count values that do not yet exist (but could), making use of `fct_count()`.

:::

::: {.column width="45%"}


```{r echo = FALSE,  out.width = "70%", fig.align = "left"}
knitr::include_graphics("./img/fct_count.jpg")
``` 

:::
::::

```{r}
marks_database |> 
  mutate(marks = fct_expand(marks, c("F-", "A+", "A++"))) |> 
  pull(marks) |> 
  fct_count()
```


---

## Quali: factors

The levels can also be [**sorted by frequency**]{.hl-yellow} with `fct_infreq()`.

```{r}
marks_database |> 
  mutate(marks = fct_infreq(marks)) |> 
  pull(marks)

marks_database |> 
  mutate(marks = fct_infreq(marks)) |> 
  pull(marks) |> 
  fct_count()
```

---

## Quali: factors


Sometimes we will want to [**group levels**]{.hl-yellow}, for example, not allowing levels that [**do not happen a minimum number of times**]{.hl-yellow} with `fct_lump_min(..., min = ..)` (observations that do not meet this will go to a **generic level** called `Other`, although this can be changed with the `other_level` argument). 

:::: columns
::: {.column width="50%"}


```{r}
marks_database |> 
  pull(marks) %>% 
  fct_lump_min(min = 3)
```

:::

::: {.column width="50%"}


```{r}
marks_database |> 
  pull(marks) |>
  fct_lump_min(min = 3,
               other_level = "Less frequent")
```

:::
::::

---

## Quali: factors


We can do something equivalent but based on its [**relative frequency**]{.hl-yellow} with `fct_lump_prop()`.


```{r}
marks_database |> 
  pull(marks) |> 
  fct_lump_prop(prop = 0.15,
                other_level = "less frequent")
```

---


## Quali: factors

We can apply this to our datasets to [**recategorize variables**]{.hl-yellow} very quickly.

```{r}
starwars |>  
  drop_na(species) |> 
  mutate(species =
           fct_lump_min(species, min = 3,
                        other_level = "Others")) |>  
  count(species)
```

---

## Quali: factors



With `fct_reorder()` we can also indicate that we want to [**order the factors**]{.hl-yellow} according to a function applied to another variable.


```{r}
starwars_factor <- 
  starwars |>  
  drop_na(height, species) |> 
  mutate(species =
           fct_lump_min(species, min = 3,
                        other_level = "Others"))
```

:::: columns
::: {.column width="50%"}


```{r}
starwars_factor |>  pull(species)
```

:::

::: {.column width="50%"}


```{r}
starwars_factor |> 
  mutate(species = fct_reorder(species, height, mean)) |> 
  pull(species)
```

:::
::::


---

## üíª It's your turn {#tu-turno-8-2}


::: panel-tabset

### [**Exercise 1**]{.hl-yellow}

üìù Given the variable `months` defined below (defined as a character vector), convert this variable to factor (just that)

```{r}
months <- c("Jan", "Feb", "Mar", "Apr")
```

```{r}
#| code-fold: true
#| eval: false
months <- c("Jan", "Feb", "Mar", "Apr")
months_fct <- as_factor(months)
months_fct
```

### [**Exercise 2**]{.hl-yellow}

üìù Given the variable `months` defined below converts this variable to a factor but indicating the levels correctly.

```{r}
months <- c(NA, "Apr", "Jan", "Oct", "Jul", "Jan", "Sep", NA, "Feb", "Dic",
           "Jul", "Mar", "Jan", "Mar", "Feb", "Apr", "May", "Oct", "Sep",  NA,
           "Dic", "Jul", "Nov", "Feb", "Oct", "Jun", "Sep", "Oct", "Oct", "Sep")
```



```{r}
#| code-fold: true
#| eval: false
months_fct <-
  factor(months,
         levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dic"))
months_fct
```

  
### [**Exercise 3**]{.hl-yellow}

üìù Count how many values there are for each month but keep in mind that they are factors (maybe there are unused levels and you should get a 0 for them).


```{r}
#| code-fold: true
#| eval: false
meses_fct |>  fct_count()
```

### [**Exercise 4**]{.hl-yellow}

üìù Since there are missing values, it indicates that the absentee is a thirteenth level labeled `"missing"`.

```{r}
#| code-fold: true
#| eval: false
months_fct <- 
  months_fct |> 
  fct_explicit_na(na_level = "missing")
months_fct
```

### [**Exercise 5**]{.hl-yellow}

üìù Removes unused levels.


```{r}
#| code-fold: true
#| eval: false
months_fct <- 
  months_fct %>%
  fct_drop()
months_fct
```


### [**Exercise 6**]{.hl-yellow}

üìù Sort the levels by frequency of occurrence.


```{r}
#| code-fold: true
#| eval: false
months_fct |> 
  fct_infreq()
```

### [**Exercise 7**]{.hl-yellow}

üìù Group levels so that any level that does not appear at least 7% of the time is grouped in a level called `"other months"`.
  
```{r}
#| code-fold: true
#| eval: false
months_fct <-
  months_fct |> 
  fct_lump_prop(prop = 0.07, other_level = "other months")
months_fct 
```

:::

## üê£ Case study II: CIS survey

[üìä Data](https://drive.google.com/drive/folders/18Ok6Epqcimszqguj_JTTLuSbs5Ot5Srd?usp=sharing)

We are going to work again with file contains data from the CIS (Centro de Investigaciones Sociol√≥gicas) barometer ¬´Perceptions on equality between men and women and gender stereotypes¬ª whose sample work was carried out from November 6 to 14 (4000 interviews of both sexes over 16 years old in 1174 municipalities and 50 provinces).


> F√≠jate que muchas columnas son del tipo `<dbl+lbl>`, un nuevo tipo de dato heredado de SPSS para poder disponer del valor num√©rico (por ejemplo, `1` para Andaluc√≠a) pero tambi√©n de la etiqueta (`"Andaluc√≠a"`). Puedes ver m√°s en el paquete `{labelled}` pero de momento vamos a usar `to_factor()` para poder trabajar con esas etiquetas si queremos.

```{r}
data |> 
  # numeric mode
  count(ccaa == 1)

data |> 
  # labels mode
  count(labelled::to_factor(ccaa) == "Andaluc√≠a")
```


> Las variables `escfeminis` y `sitconv` en realidad corresponde a la pregunta `P16` y `P18` del cuestionario. Ren√≥mbralas como `p16` y  `p18`. Las variables `tarhogentrev_1`, `tarhogentrev_2`, `tarhogentrev_8`, `tarhogentrev_9`, `tarhogentrev_hh`, `tarhogentrev_mm` hablan de la misma pregunta: tiempo dedicado a las tareas del hogar. 

* La variable `tarhogentrev_1` guarda un `1` si el entrevistado respondi√≥ en horas (en caso contrario `2`)

* La variable `tarhogentrev_2` guarda un `1` si el entrevistado respondi√≥ en minutos (en caso contrario `2`)

* La variable `tarhogentrev_8` guarda un `1` si el entrevistado respondi√≥ ¬´no s√© decir¬ª (en caso contrario `2`)

* La variable `tarhogentrev_9` guarda un `1` si el entrevistado respondi√≥ no quiso responder.

* Las variables `tarhogentrev_hh` y `tarhogentrev_mm` guardan las cantidades (en horas o en minutos, o `99` o `98` si no sabe o no quiso responder)

Usa la informaci√≥n de estas 5 variables de manera que dicha informaci√≥n est√© contenida en una sola variable `p19` con el n√∫mero de minutos dedicados (si no se puede saber `NA`)

```{r}
data <- 
  data |> 
  rename(p16 = escfeminis, p18 = sitconv) |> 
  mutate("p19" =
           if_else(tarhogentrev_1 == 1, tarhogentrev_hh * 60,
                   if_else(tarhogentrev_2 == 1, tarhogentrev_mm, NA)),
         .after = p18) |>
  select(-contains("tarhogentrev"))
```


> Realiza la misma recodificaci√≥n con las variables `tarhogparej_1`, `tarhogparej_2`, `tarhogparej_8`, `tarhogparej_9`, `tarhogparej_hh`, `tarhogparej_mm` (tiempo dedicado a las tareas del hogar de la pareja) y agrupa su info en `p20`


```{r}
data <- 
  data |> 
  mutate("p20" =
           if_else(tarhogparej_1 == 1, tarhogparej_hh * 60,
                   if_else(tarhogparej_2 == 1, tarhogparej_mm, NA)),
         .after = p19) |>
  select(-contains("tarhogparej"))
```

> La variable `hijomenor_1`  contiene un `1` si tiene hijos y el n√∫mero de fijos est√° en `hijomenor_n`; si `hijomenor_97` contiene un `1` deber√≠amos apuntar `0` en n√∫mero de hijos; si `hijomenor_99` es `1` deber√≠amos imputar `NA`. Resume la info de las 4 variables en una sola variable llamada `p21`

```{r}
data <- 
  data |> 
  mutate("p21" =
           if_else(hijomenor_1 == 1, hijomenor_n,
                   if_else(hijomenor_97 == 1, 0, NA)),
         .after = p20) |>
  select(-contains("hijomenor"))
```

> Las variables siguientes (`cuidadohijos_xxx` y `ciudadohijospar_xxx`) codifican algo similar respecto al tiempo dedicado al cuidado de hijos (del entrevistado y su pareja). Resume la info en dos variables `p21a` y `p21b` como hemos hecho con `p19` y `p20`

```{r}
data <- 
  data |> 
  mutate("p21a" =
           if_else(cuidadohijos_1 == 1, cuidadohijos_hh * 60,
                   if_else(cuidadohijos_2 == 1, cuidadohijos_mm, NA)),
         "p21b" =
           if_else(cuidadohijospar_1 == 1, cuidadohijospar_hh * 60,
                   if_else(cuidadohijospar_2 == 1, cuidadohijospar_mm, NA)),
         .after = p21) |>
  select(-contains("cuidadohijos"))
```

> Haz lo mismo con `tareascuid_xxx` respecto al tiempo empleado a cuidado y atenci√≥n de una persona enferma y/o dependiente en un d√≠a laborable, y guarda la info en `p22a`. Piensa que en `p22` est√° guardado si tiene a su cargo una persona enferma o dependiente; si `p22` es `No` (`2`) o `N.C.` (`9`) el valor de `p22a` deber√≠a ser `NA`.

```{r}
data <- 
  data |> 
  mutate("p22a" =
           if_else(p22 %in% c(2, 9), NA,
                   if_else(tareascui_1 == 1, tareascui_hh * 60,
                           if_else(tareascui_2 == 1, tareascui_mm, NA))),
         .after = p22) |>
  select(-contains("tareascui"))
```

> Por √∫ltimo las variables `escideol`, `participaciong`, `recuvotog`, `escuela`, `nivelestentrev`, `religion`, `practicarelig6`, `sitlab`, `relalab`, `cno11`, `tipojor`, `ingreshog`, `clasesocial`, `paisnac`, `paisnac2` deber√≠an ser renombradas como `p24`, `p25`, `p25a`, `p26`, `p26a`, `p27`, `p27a`, `p28`, `p28a`, `p28b`, `p28c`, `p29`, `p30`, `p31` y `p31a`. 

> La variable `sinceridad` recopila la sinceridad subjetiva que el encuestador ha percibido en el entrevistado y las variables `recuvotogr` y `recuerdo` capturan informaci√≥n de recuerdo de voto que ya tenemos recoficada en otros sitios. Tras eliminarlas moveremos las variables `estudios`, `cno11r`, `clasesub` (estudios, ocupaci√≥n  seg√∫n codificaci√≥n de Seguridad Social e identificaci√≥n subjetiva de clase) antes de las preguntas encuestadas

As√≠ deber√≠amos acabar con un dataset de 107 columnas de las cuales las primeras 14 columnas dedicadas a caracterizar sociol√≥gicamente al entrevistado.

```{r}
data <-
  data |> 
  rename(p24 = escideol, p25 = participaciong, p25a = recuvotog,
         p26 = escuela, p26a = nivelestentrev, p27 = religion,
         p27a = practicarelig6, p28 = sitlab, p28a = relalab,
         p28b = cno11, p28c = tipojor, p29 = ingreshog,
         p30 = clasesocial, p31 = paisnac, p31a = paisnac2) |> 
  select(-(sinceridad:recuerdo)) |> 
  relocate(estudios, cno11r, clasesub, .before = p0)
```



> F√≠jate que hay variables como `p0` y `p1` que aunque est√©n codificadas de manera num√©rica en realidad deber√≠amos tenerlas solo de forma cualitativa. Por ejemplo, si miras el cuestionario, la variable `p0` corresponde a la pregunta `P0.c` que dice

¬´En primer lugar quisiera preguntarle si tiene Ud

* La nacionalidad espa√±ola --> 1
* La nacionalidad espa√±ola y otra --> 2
* Otra nacionalidad --> 3

```{r}
data |> count(p0)
```

Si dej√°semos la variable tal cual, de manera num√©rica, estamos asumiendo dos cosas incorrectas:

1) Hay una jerarqu√≠a (tener nacionalidad espa√±ola es mejor o peor que tener otra nacionalidad)

2) Esa jerarqu√≠a adem√°s se cuantifica de manera num√©rica: dad que 2 es el doble 1, implica que, de alguna forma, tener dos nacionalidades "el doble de algo" que tener solo la nacionalidad espa√±ola

Si te fijas en el cuestionario