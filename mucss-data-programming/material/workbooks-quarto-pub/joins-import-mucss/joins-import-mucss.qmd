---
title: "Joins and imports: how to combine, import and export files in R. Factors and lists"
subtitle: "Practical workbooks of Data Programming in Master in Computational Social Science  (2024-2025)"
author: "Javier √Ålvarez Li√©bana"
format:
  html:
    theme: [default, style.scss]
    toc: true
    toc-title: √çndice
    toc-depth: 5
    toc-location: left
    number-sections: true
embed-resources: true
execute: 
  echo: true
---

```{r}
#| echo: false
setwd(dir = getwd())
```


## Joins

When working with data we will not always have the information in a single table, and sometimes, we will be interested in [**cross-referencing**]{.hl-yellow} information from different sources.


For this we will use a classic of every language that handles data: the famous [**join**]{.hl-yellow}, a tool that will allow us to [**cross one or several tables**]{.hl-yellow}, making use of a [**identifying column**]{.hl-yellow} of each one of them.

```{r}
#| eval: false
table_1 |>
  xxx_join(table_2, by = id)
```



* `inner_join()`: only [**records with id in both**]{.hl-yellow} tables survive.

* `full_join()`: keeps [**all records in both**]{.hl-yellow} tables.

* `left_join()`: keeps [**all the records of the first table**]{.hl-yellow}, and looks for which ones have id also in the second one (in case of [**not having it, it fills with NA**]{.hl-yellow} the fields of the 2nd table).

* `right_join()`: keeps [**all records in the second table**]{.hl-yellow}, and searches which ones have id also in the first one.

![](img/sql-joins.jpg)    



Let's test the various joins with a simple example

```{r}
library(tidyverse)
tb_1 <- tibble("key" = 1:3, "val_x" = c("x1", "x2", "x3"))
tb_2 <- tibble("key" = c(1, 2, 4), "val_y" = c("y1", "y2", "y3"))
```


```{r}
tb_1
```


```{r}
tb_2
```



### left_join()

Imagine that we want to [**incorporate**]{.hl-yellow} to `tb_1` the [**information from table_2**]{.hl-yellow}, identifying the records by the key column (`by = "key"`, the column it has to cross): we want to keep all the records of the first table and look for which ones have the same value in `key` also in the second one.


```{r}
tb_1 |> 
  left_join(tb_2, by = "key")
```


![](img/left_join.jpg)




```{r}
tb_1 |>
  left_join(tb_2, by = "key")
```

Notice that the [**records in the first one whose key was not found in the second one**]{.hl-yellow} has given them the value of [**absent**]{.hl-yellow}.

### right_join()

The `right_join()` will perform the opposite operation: we will now [**incorporate**]{.hl-yellow} to `tb_2` the [**information from table_2**]{.hl-yellow}, identifying the records by the `key` column: we want to keep all the records of the second one and look for which ones have id (same value in `key`) also in the first table.


```{r}
tb_1 |> 
  right_join(tb_2, by = "key")
```



![](img/right_join.jpg)


```{r}
tb_1 |>
  right_join(tb_2, by = "key")
```

Notice that now the [**records of the second one whose key was not found in the first one**]{.hl-yellow} are the ones given the value of [**absent**]{.hl-yellow}.

### keys and suffixes

The key columns we will use for the crossover [**will not always be named the same**]{.hl-yellow}.

```{r}
tb_1 <- tibble("key_1" = 1:3, "val_x" = c("x1", "x2", "x3"))
tb_2 <- tibble("key_2" = c(1, 2, 4), "val_y" = c("y1", "y2", "y3"))
```


* `by = c("key_2" = "key_2")`: we will indicate in which column of each table are the keys that we are going to cross.


```{r}
# Left
tb_1  |> 
  left_join(tb_2, by = c("key_1" = "key_2"))
```


```{r}
# Right
tb_1 |> 
  right_join(tb_2, by = c("key_1" = "key_2"))
```




We can also [**cross over several columns at the same time**]{.hl-yellow} (it will interpret as equal record the one that has the same set of keys), with `by = c("var1_t1" = "var1_t2", "var2_t1" = "var2_t2", ...)`. Let's modify the previous example

```{r}
tb_1 <- tibble("k_11" = 1:3, "k_12" = c("a", "b", "c"),  "val_x" = c("x1", "x2", "x3"))
tb_2 <- tibble("k_21" = c(1, 2, 4), "k_22" = c("a", "b", "e"), "val_y" = c("y1", "y2", "y3"))
```


```{r}
# Left
tb_1 |> 
  left_join(tb_2,
            by = c("k_11" = "k_21", "k_12" = "k_22"))
```



```{r}
# Right
tb_1 |> 
  right_join(tb_2,
             by = c("k_11" = "k_21", "k_12" = "k_22"))
```



It could also happen that when crossing two tables, there are [**columns of values named the same**]{.hl-yellow}

```{r}
tb_1 <- tibble("key_1" = 1:3, "val" = c("x1", "x2", "x3"))
tb_2 <- tibble("key_2" = c(1, 2, 4), "val" = c("y1", "y2", "y3"))
```


```{r}
# Left
tb_1 |>
  left_join(tb_2, by = c("key_1" = "key_2"))
```

Notice that [**default adds the suffixes**]{.hl-yellow} `.x` and `.y` to tell us which table they come from. This [**suffix can be specified**]{.hl-yellow} in the optional argument `suffix = ...`, which allows us to [**distinguish the variables**]{.hl-yellow} of one table from another.

```{r}
# Left
tb_1 |>
  left_join(tb_2, by = c("key_1" = "key_2"),
            suffix = c("_table1", "_table2"))
```

### full_join()

The two previous cases form what is known as [**outer joins**]{.hl-yellow}: crosses where observations are kept that appear in at least one table. The third outer join is known as `full_join()` which will [**keep observations from both**]{.hl-yellow} tables, [**adding rows**]{.hl-yellow} that do not match the other table.


```{r}
tb_1  |> 
  full_join(tb_2, by = c("key_1" = "key_2"))
```


![](img/full_join.jpg)



### inner_join()

Opposite the outer join is what is known as [**inner join**]{.hl-yellow}, with `inner_join()`: a join in which only the [**observations that appear in both tables**]{.hl-yellow} are kept, only those records that are patched are kept.


```{r}
tb_1 |> 
  inner_join(tb_2,  by = c("key_1" = "key_2"))
```


![](img/inner_join.png)




Note that in terms of records, `inner_join` if it is commutative, **we don't care about the order of the tables**: the only thing that changes is the order of the columns it adds.



```{r}
tb_1 |> 
  inner_join(tb_2, by = c("key_1" = "key_2"))
```


```{r}
tb_2 |> inner_join(tb_1, by = c("key_2" = "key_1"))
```

### semi/anti_join()

Finally we have two interesting tools to [**filter (not cross) records**]{.hl-yellow}: `semi_join()` and `anti_join()`. The [**semi join**]{.hl-yellow} leaves us in the [**first table the records whose key is also in the second table**]{.hl-yellow} (like an inner join but without adding the info from the second table). And the second one, the anti join, does just the opposite (those that are not).




```{r}
# semijoin
tb_1 |> 
  semi_join(tb_2, by = c("key_1" = "key_2"))
```

```{r}
# antijoin
tb_1 |> 
  anti_join(tb_2, by = c("key_1" = "key_2"))
```



### üíª It's your turn

For the exercises we will use the tables available in the package `{nycflights13}`.

```{r}
library(nycflights13)
```

* **airlines**: name of airlines (with their abbreviation).
* **airports**: airport data (names, longitude, latitude, altitude, etc).
* **flights**, **planes**: flight and aircraft data.
* **weather**: hourly weather data.



::: panel-tabset

### [**Exercise 1**]{.hl-yellow}

üìù From package `{nycflights13}` incorporates into the `flights` table the airline data in `airlines`. We want to maintain all flight records, adding the airline information to the airlines table.


```{r}
#| code-fold: true
#| eval: false
flights_airlines <-
  flights |> 
  left_join(airlines, by = "carrier")
flights_airlines
```

### [**Exercise 2**]{.hl-yellow}

üìù To the table obtained from the crossing of the previous section, include the data of the aircraft in `planes`, but including only those flights for which we have information on their aircraft (and vice versa). 

```{r}
#| code-fold: true
#| eval: false

flights_airlines_planes <- 
  flights_airlines |> 
  inner_join(planes, by = "tailnum")
flights_airlines_planes
```


### [**Exercise 3**]{.hl-yellow}

üìù Repeat the previous exercise but keeping both `year` variables (in one is the year of flight, in the other is the year of construction of the aircraft), and distinguishing them from each other


```{r}
#| code-fold: true
#| eval: false

flights_airlines_planes <- 
  flights_airlines |>
  inner_join(planes, by = "tailnum",
             suffix = c("_flight", "_build_aircraft"))
flights_airlines_planes
```


### [**Exercise 4**]{.hl-yellow}

üìù To the table obtained from the previous exercise includes the longitude and latitude of the airports in `airports`, distinguishing between the latitude/longitude of the airport at destination and at origin.


```{r}
#| code-fold: true
#| eval: false

flights_airlines_planes |> 
  left_join(airports |> select(faa, lat, lon),
            by = c("origin" = "faa")) |> 
  rename(lat_origin = lat, lon_origin = lon) |> 
  left_join(airports |> select(faa, lat, lon),
            by = c("dest" = "faa")) |> 
  rename(lat_dest = lat, lon_dest = lon)
```

### [**Exercise 5**]{.hl-yellow}

üìù Filter from `airports` only those airports from which flights depart. Repeat the process filtering only those airports where flights arrive.


```{r}
#| code-fold: true
#| eval: false

airports |> 
  semi_join(flights, by = c("faa" = "origin"))
airports |> 
  semi_join(flights, by = c("faa" = "dest"))
```

### [**Exercise 6**]{.hl-yellow}

üìù How many flights do we not have information about the aircraft? Eliminate flights that do not have an aircraft ID (other than NA) beforehand.


```{r}
#| code-fold: true
#| eval: false

flights |>
  drop_na(tailnum) |>
  anti_join(planes, by = "tailnum") |>
  count(tailnum, sort = TRUE) 
```

:::


## üê£ Case study I: Beatles and Rolling Stones

We will use to practice simple joins the `band_members` and `band_instruments` datasets already included in the `{dplyr}` package.

```{r}
library(dplyr)
band_members
band_instruments
```

In the first one we have a series of artists and the band they belong to; in the second one we have a series of artists and the instrument they play. Beyond performing the requested actions, try to visualize which final table you would have

### Question 1

> Given the table `band_members`, incorporate the information of what instrument each member plays (`band_instruments`) of the ones you have in that table.



```{r}
#| code-fold: true
left_join_band <-
  band_members |> 
  left_join(band_instruments, by = "name")
```

### Question 2

> Given the `band_members` and `band_instruments` tables, what kind of join should you do to have a complete table, with no absentees, where all band members have their instrument information, and every instrument has a member associated with it?

```{r}
#| code-fold: true
inner_join_band <-
  band_members |>
  inner_join(band_instruments, by = "name")
```

### Question 3

> Given the `band_instruments` table, how to incorporate the information of who plays each instrument (in case we know it)?

```{r}
#| code-fold: true
right_join_band <-
  band_members |>
  right_join(band_instruments, by = "name")

# other option
left_join_instruments <-
  band_instruments |> 
  left_join(band_members, by = "name")
```

### Question 4

> Given the `band_members` and `band_instruments` tables, what kind of join should you do to have a table with all the info, both members and instruments, even if there are members whose instrument you do not know, and instruments whose carrier you do not know?

```{r}
#| code-fold: true
full_join_band <-
  band_members |>
  full_join(band_instruments, by = "name")
```

## üê£ Case study II: income by municipalities

In the file `municipios.csv` we have stored the information of the municipalities of Spain as of 2019.

* The variable `LAU_code` represents the code as local administrative unit in the EU (see more at <https://ec.europa.eu/eurostat/web/nuts/local-administrative-units>).

* The variable `codigo_ine` is formed by joining the province code and the community code (each province has a code from 1 to 52, it does not depend on the ccaa).

```{r}
# 2019 data
mun_data <- read_csv(file = "./datos/municipios.csv")
```

On the other hand, in the file `renta_mun` we have the average per capita income of each administrative unit (municipalities, districts, provinces, autonomous communities,...) for different years.

```{r}
renta_mun <- read_csv(file = "./datos/renta_mun.csv")
```

Before we start let's [**normalize variable names**]{.hl-yellow} using `clean_names()` from the `{janitor}` package.

```{r}
mun_data <-
  mun_data |> 
  janitor::clean_names()
renta_mun <-
  renta_mun |> 
  janitor::clean_names()
```

### Question 1

> Convert to tidydata `renta_mun` obtaining a table of 4 columns: `unit`, `year`, `income` and `codigo_ine` (no absent and each data of the correct type)

```{r}
#| code-fold: true
renta_mun_tidy <-
  renta_mun |> 
  pivot_longer(cols = contains("x"), names_to = "year",
               values_to = "income", names_prefix = "x",
               names_transform = list(year = as.numeric),
               values_drop_na = TRUE)
```

### Question 2

> If you look at the table above we have data from different administrative units that are not always municipalities. Knowing that all municipalities have a 5-character code, filter only those records that correspond to municipal units.

```{r}
#| code-fold: true
renta_mun_tidy <-
  renta_mun_tidy |>
  filter(str_detect(codigo_ine, pattern = "[0-9]{5}") & 
           str_length(codigo_ine) == 5)
```

### Question 3

> Then properly separate the unit variable into two columns: one with the code (which you already have so one of the two should be removed) and the name. Remove any remaining spaces (take a look at the `{stringr}` package options).

```{r}
#| code-fold: true
renta_mun_tidy <-
  renta_mun_tidy |>
  separate(col = "unidad", into = c("cod_rm", "name"), sep = 5) |> 
  select(-cod_rm) |> 
  mutate(name = str_trim(name)) 
```

### Question 4

> In which year was the median income higher? And lower? What was the median income of municipalities in Spain in 2019?

```{r}
summary_renta <-
  renta_mun_tidy |> 
  summarise("mean_income" = mean(income, na.rm = TRUE),
            .by = year)
summary_renta |>
  slice_min(mean_income, n = 1)
summary_renta |>
  slice_max(mean_income, n = 1)

renta_mun_tidy |> 
  filter(year == 2019) |> 
  summarise("median_income" = median(income, na.rm = TRUE))
```


### Question 5

> Do whatever you consider to obtain the province with the highest average income in 2019 and the one with the lowest. Be sure to get its name.

```{r}
#| code-fold: true
summary_by_prov <-
  renta_mun_tidy |> 
  filter(year == 2019) |> 
  left_join(mun_data, by = "codigo_ine", suffix = c("", "_rm")) |> 
  select(-contains("rm")) |> 
  summarise("mean_by_prov" = mean(income, na.rm = TRUE),
            .by = c("cpro", "ine_prov_name"))

summary_by_prov |> 
  slice_max(mean_by_prov, n = 1)

summary_by_prov |> 
  slice_min(mean_by_prov, n = 1)
```

### Question 6

> Obtain from each ccaa the name of the municipality with the highest income in 2019.

```{r}
#| code-fold: true
renta_mun_tidy |> 
  filter(year == 2019) |> 
  left_join(mun_data, by = "codigo_ine", suffix = c("", "_rm")) |> 
  select(-contains("rm")) |> 
  slice_max(income, by = "codauto")
```

## Import and export files


So far we have only used data already loaded in packages but many times [**we will need to import data externally**]{.hl-yellow}. One of the main [**strengths**]{.hl-yellow} of `R` is that we can import data very easily in different formats:


* [**R native formats**]{.hl-yellow}: `.rda`, `.RData` and `.rds` formats.

* [**Rectangular (tabular) data**]{.hl-yellow}: `.csv` and `.tsv` formats

* [**Untabulated data**]{.hl-yellow}: `.txt` format.

* [**Data in excel**]{.hl-yellow}: `.xls` and `.xlsx` formats

* [**Data from SAS/Stata/SPSS**]{.hl-yellow}: `.sas7bdat`, `.sav` and `.dat` formats

* [**Data from Google Drive**]{.hl-yellow}

* [**Data from API's**]{.hl-yellow}: aemet, catastro, twitter, spotify, etc.


### R native formats

The [**simplest**]{.hl-yellow} files to import into `R` (and which usually take up less disk space) are its own [**native extensions**]{.hl-yellow}: files in `.RData`, `.rda` and `.rds` formats. To load the former we simply need to [**use the native**]{.hl-yellow} function `load()` by providing it the file path.

* `RData` file: we are going to import the file `world_bank_pop.RData`, which includes the dataset `world_bank_pop`


```{r}
load("./datos/world_bank_pop.RData")
world_bank_pop
```


* `.rda` file: we will import the airquality dataset from `airquality.rda`

```{r}
load("./datos/airquality.rda")
airquality |> as_tibble()
```


Note that files loaded with `load()` are [**automatically loaded into the environment**]{.hl-yellow} (with the originally saved name), and not only datasets can be loaded: `load()` allows us to load multiple objetcs (not only a tabular data)

Native `.rda` and `.RData` files are a properly way to save your environment.

```{r}
load(file = "./datos/multiple_objects.rda")
```


* `.rds` files: for this type we must use `readRDS()`, and we need to incorporate a [**argument `file`**]{.hl-yellow} with the path. In this case we are going to import [**lung cancer data**]{.hl-purple} from the North Central Cancer Treatment Group. Note that now [**.rds import files are a unique database**]{.hl-yellow}

```{r}
lung_cancer <-
  readRDS(file = "./datos/NCCTG_lung_cancer.rds") |>
  as_tibble()
lung_cancer
```


::: callout-important

## Important

The [**paths**]{.hl-yellow} must always be [**without spaces, √±, or accents**]{.hl-yellow}. 

:::



### Tabular data: readr

The `{readr}` package within the `{tidyverse}` environment contains several useful functions for [**loading rectangular data (without formatting)**]{.hl-yellow}.



* `read_csv()`: `.csv` files whose [**separator is comma**]{.hl-purple}
* `read_csv2()`: [**semicolon**]{.hl-purple}
* `read_tsv()`: [**tabulator**]{.hl-purple}.
* `read_table()`: [**space**]{.hl-purple}.
* `read_delim()`: generic function for [**character delimited files**]{.hl-purple}.



![](img/data-import-readr.png)


All of them need as **argument the file path** plus **other optional** (skip header or not, decimals, etc). See more at <https://readr.tidyverse.org/>


#### .csv, .tsv

The main advantage of `{readr}` is that it [**automates formatting**]{.hl-yellow} to go from a flat (unformatted) file to a tibble (in rows and columns, with formatting).


* File `.csv`: with `read_csv()` we will load [**comma separated**]{.hl-purple} files, passing as [**argument the path**]{.hl-yellow} in `file = ...`. Let's import the `chickens.csv` dataset (about cartoon chickens, why not). If you look at the output it gives us the type of variables.

```{r}
library(readr)
chickens <- read_csv(file = "./datos/chickens.csv")
chickens
```



The [**variable format**]{.hl-yellow} will normally be done [**automatically**]{.hl-yellow} by `read_csv()`, and we can query it with `spec()`.

```{r}
spec(chickens)
```



Although it usually does it well automatically we can [**specify the format explicitly**]{.hl-yellow} in `col_types = list()` (in list format, with `col_xxx()` for each type of variable, for example `eggs_laid` will be imported as character). 


```{r}
chickens <-
  read_csv(file = "./datos/chickens.csv",
           col_types = list(col_character(), col_character(),
                            col_character(), col_character()))
chickens
```


We can even indicate that [**variables we want to select**]{.hl-yellow} (without occupying memory), by indicating it in `col_select = ...` (in list format, with `col_select = ...`).


```{r}
chickens <-
  read_csv(file = "./datos/chickens.csv",
           col_select = c(chicken, sex, eggs_laid))
chickens
```

#### .txt

What happens when the [**separator is not correct**]{.hl-red}?


If we use `read_csv()` it expects the separator between columns to be a comma but, as you can see with the following `.txt`, it interprets everything as a single column: [**has no comma and does not know where to separate**]{.hl-yellow}

```{r}
datos_txt <- read_csv(file = "./datos/massey-rating.txt")
dim(datos_txt)
as_tibble(datos_txt)
```


To do this we have.

* `read_csv2()` when the [**separator is semicolon**]{.hl-yellow}, `read_tsv()` when the [**is a tab**]{.hl-yellow} and `read_table()` when the [**is a space**]{.hl-yellow}.

* `read_delim()` in general.

```{r}
datos_txt <- read_table(file = "./datos/massey-rating.txt")
as_tibble(datos_txt)
```


### Excel data (.xls, .xlsx)

Another key import package will be the `{readxl}` package for [**importing data from Excel**]{.hl-yellow}. Three functions will be key:

* `read_xls()` specific to `.xls`, `read_xlsx()` specific to `.xlsx`.
* `read_excel()`: for both `.xls` and `.xlsx`.


We are going to import `deaths.xlsx` with celebrity death records.

```{r}
library(readxl)
deaths <- read_xlsx(path = "./datos/deaths.xlsx")
deaths
```



```{r}
deaths |> slice(1:6)
```

One thing that is [**very common misfortune**]{.hl-yellow} is that there is some kind of comment or text at the beginning of the file, having to [**skip those rows**]{.hl-yellow}.


We can [**skip these rows**]{.hl-yellow} directly in the load with `skip = ...` (indicating the number of rows to skip).

```{r}
deaths <- read_xlsx(path = "./datos/deaths.xlsx", skip = 4)
deaths
```


In addition with `col_names = ...` we can already rename the columns in the import (if [**provide names assumes 1st line already as a data**]{.hl-yellow})

```{r}
#| code-line-numbers: "2-3"
deaths <-
  read_xlsx(path = "./datos/deaths.xlsx", skip = 5,
            col_names = c("name", "profession", "age", "kids", "birth", "death"))
deaths
```


Sometimes [**Excel dates are incorrectly formatted**]{.hl-red} (surprise): we can use `convertToDate()` from the `{openxlsx}` package to convert it.


```{r}
library(openxlsx)
deaths$death <- convertToDate(deaths$death)
deaths
```
   

We can also [**load an Excel with several sheets**]{.hl-yellow}: to [**indicate the sheet**]{.hl-yellow} (either by its name or by its number) we will use the argument `sheet = ...`.

```{r}
mtcars <- read_xlsx(path = "./datos/datasets.xlsx", sheet = "mtcars")
mtcars
```

We can even indicate the [**range of cells**]{.hl-yellow} to load with `range = ...`.

```{r}
iris <- read_xlsx(path = "./datos/datasets.xlsx", sheet = "iris", range = "C1:E4")
iris
```

### Import from SAS/STATA/SPSS

The `{haven}` package within the tidyverse orbit will allow us to [**import files from the 3 most important payment software**]{.hl-yellow}: SAS, SPSS and Stata.

```{r}
library(haven)

# SAS
iris_sas <- read_sas(data_file = "./datos/iris.sas7bdat")

# SPSS
iris_spss <- read_sav(file = "./datos/iris.sav")

# Stata
iris_stata <- read_dta(file = "./datos/iris.dta")
```


### Export

In the same way that we can import we can also [**export**]{.hl-yellow}

* exported in `.RData` (recommended option for variables stored in `R`). Remember that this extension [**can only be used in `R`**]{.hl-yellow}. To do so, just use `save(object, file = path)`.

```{r}
table <- tibble("a" = 1:4, "b" = 1:4)
save(table, file = "./datos/table.RData")
rm(table) # eliminar
load("./datos/table.RData")
table
```


* exported in `.RData` multiple objects

```{r}
table <- tibble("a" = 1:4, "b" = 1:4)
a <- 1
b <- c("javi", "sandra")
save(table, a, b, file = "./datos/mult_obj.RData")
rm(list = c("a", "b", "table"))
load("./datos/mult_obj.RData")
table
```



* exported in `.csv`. To do this we simply use `write_csv(object, file = path)`.

```{r}
write_csv(table, file = "./datos/table.csv")
read_csv(file = "./datos/table.csv")
```

### Import from new sources

#### Website

One of the main advantages of `R` is that we can make use of all the previous functions of [**import but directly from a web**]{.hl-yellow}, without the need to perform the manual download: instead of passing it the local path we will indicate the [**link**]{.hl-yellow}. For example, we are going to download the covid data from ISCIII (<https://cnecovid.isciii.es/covid19/#documentaci%C3%B3n-y-datos>)

```{r}
#| eval: false
covid_data <-
  read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv")
covid_data
```

```{r}
#| echo: false
covid_data <-
  read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv", n_max = 500)
covid_data
```

#### Wikipedia

The `{rvest}` package, one of the most useful of `{tidyverse}` allows us to import directly from an `html`. For example, to export wikipedia tables just `read_html()` to import the html, `html_element("table")` to extract the table objects, and `html_table()` to convert the html table to `tibble`.

```{r}
library(rvest)
wiki_jump <- 'https://en.wikipedia.org/wiki/Men%27s_long_jump_world_record_progression'
wiki_jump |> read_html() |> 
  html_element("table") |> 
  html_table()
```

#### Google Drive

Another option available (especially if we work with other people working) is to [**import from a Google Drive spreadsheet**]{.hl-yellow}, making use of `read_sheet()` from the `{googlesheets4}` package.

The first time you will be asked for a tidyverse permission to interact with your drive

```{r}
library(googlesheets4)
google_sheet <-
  read_sheet("https://docs.google.com/spreadsheets/d/1Uz38nHjl3bmftxDpcXj--DYyPo1I39NHVf-xjeg1_wI/edit?usp=sharing")
google_sheet
```

#### API (owid)

Another interesting option is the [**data download from an API**]{.hl-yellow}: an intermediary between an app or data provider and our `R`. For example, let's load the `{owidR}` library, which allows us to download data from the web <https://ourworldindata.org/>. For example, the `owid_covid()` function loads without realizing it more than 400 000 records with more than 60 variables from 238 countries.

```{r}
#| eval: false
library(owidR)
owid_covid() |> as_tibble()
```

```{r}
#| echo: false
library(owidR)
owid_covid() |> as_tibble() |> slice(1:7)
```

#### API (aemet)

In many occasions to connect to the API we will first have to [**register and obtain a key**]{.hl-yellow}, this is the case of the `{climaemet}` package to access [**Spanish meteorological data**]{.hl-yellow} (<https://opendata.aemet.es/centrodedescargas/inicio>).


Once we have the API key we register it in our RStudio to be able to use it in the future.

```{r}
#| eval: false
library(climaemet)

# Api key
apikey <- "eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqYXZhbHYwOUB1Y20uZXMiLCJqdGkiOiI4YTU1ODUxMS01MTE3LTQ4MTYtYmM4OS1hYmVkNDhiODBkYzkiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTY2NjQ2OTcxNSwidXNlcklkIjoiOGE1NTg1MTEtNTExNy00ODE2LWJjODktYWJlZDQ4YjgwZGM5Iiwicm9sZSI6IiJ9.HEMR77lZy2ASjmOxJa8ppx2J8Za1IViurMX3p1reVBU"

aemet_api_key(apikey, install = TRUE)
```


```{r}
#| echo: false
library(climaemet)
```


With this package we can do a [**search for stations**]{.hl-yellow} to know both its postal code and its identifier code within the AEMET network

```{r}
stations <- aemet_stations()
stations
```

For example, to get data from the station of the airport of El Prat, Barcelona, the code to provide is `"0076"`, obtaining **hourly data**

```{r}
aemet_last_obs("0076")
```


#### API (US census)

One of the most useful tools in recent years is known as `{tidycensus}`: a tool for [**facilitating the process of downloading census data**]{.hl-yellow} for the United States from `R`.

```{r}
library(tidycensus)
```

* `get_decennial()`: to access the [**census data (US Decennial Census)**]{.hl-yellow}, done every 10 years (years 2000, 2010 and 2020).

* `get_acs()`: to access the [**annual and quinquennial (5 years) ACS (American Community Survey)**]{.hl-yellow} (census != survey)

* `get_estimates()`: to access the [**annual population, birth and death estimates**]{.hl-yellow} (census != survey)

* `get_pums()`: to access the [**microdata (unaggregated data) of the ACS (anonymized at the individual level)**]{.hl-yellow}

* `get_flows()`: to access the [**migration flow**]{.hl-yellow} data.

portar from API (US census)

For example, we will download the **census data** (`get_decennial()`) at the state level (`geography = ‚Äústate‚Äù`) for the population (variable `variables = ‚ÄúP001001‚Äù`) for the year 2010 (see variables in `tidycensus::load_variables()`).

```{r}
total_population_10 <-
  get_decennial(geography = "state", 
  variables = "P001001",
  year = 2010)
total_population_10
```


#### Other options


* `{chessR}`: data from chess matches. See <https://github.com/JaseZiv/chessR>

* `{spotifyr}`: data from Spotify. See <https://www.rcharlie.com/spotifyr/>

* `{gtrendsR}`: data from Google Trends. See <https://github.com/PMassicotte/gtrendsR>

* `{scholar}`: data from <https://github.com/jkeirstead/scholar>

### üíª Your turn 

[**Try to solve the following exercises without looking at the solutions**]{style="color:#444442;"}

::: panel-tabset
### [**Exercise 1**]{.hl-yellow}

üìù The `who` dataset we have used in previous exercises, export it to a native `R` format in the `data` folder of the project

```{r}
#| code-fold: true
#| eval: false
library(tidyr)
save(who, file = "./datos/who.RData")
```

### [**Exercise 2**]{.hl-yellow}

üìù Loads the `who` dataset but from the data folder (import the file created in the previous exercise)

```{r}
#| code-fold: true
#| eval: false
load("./datos/who.RData")
```

### [**Exercise 3**]{.hl-yellow}

üìù Repeats the same (export and import) in 4 formats: `.csv`, `.xlsx`, `.sav` (spss) and `.dta` (stata)

```{r}
#| code-fold: true
#| eval: false

# csv
library(readr)
write_csv(who, file = "./datos/who.csv")
who_data <- read_csv(file = "./datos/who.csv")

# excel
library(openxlsx)
write.xlsx(who, file = "./datos/who.xlsx")
who_data <- read_xlsx(path = "./datos/who.xlsx")

# sas y stata
library(haven)
write_sav(who, path = "./datos/who.sav")
who_data <- read_spss(path = "./datos/who.sav")

write_dta(who, path = "./datos/who.dta")
who_data <- read_dta(path = "./datos/who.dta")
```

### [**Exercise 4**]{.hl-yellow}

üìù Repeat the loading of `who.csv` but only select the first 4 columns already in the load.

```{r}
#| code-fold: true
#| eval: false
who_select <-
  read_csv(file = "./datos/who.csv",
           col_select = c("country", "iso2", "iso3", "year"))
```


:::



## üê£ Case study I: CIS survey

[üìä Data](https://drive.google.com/drive/folders/18Ok6Epqcimszqguj_JTTLuSbs5Ot5Srd?usp=sharing)

We are going to put into practice the loading and preprocessing of a file generated by one of the most used software (SPSS). The file contains data from the CIS (Centro de Investigaciones Sociol√≥gicas) barometer ¬´Perceptions on equality between men and women and gender stereotypes¬ª whose sample work was carried out from November 6 to 14 (4000 interviews of both sexes over 16 years old in 1174 municipalities and 50 provinces).

### Question 1

> Load the file extension `.sav` that you have in the subfolder `CIS-feminism` inside the data folder. After loading normalize variable names with `{janitor}` package.

```{r}
#| code-fold: true
library(haven)
data <-
  read_sav(file = "./datos/CIS-feminismo/3428.sav") |> 
  janitor::clean_names()
```

### Question 2

> Calculate using tidyverse the number of distinct values of each variable and eliminate those that only have a constant value. First get the name of those variables and then use them inside a `select()`.

```{r}
#| code-fold: true
rm_variables <-
  data |> 
  summarise(across(everything(), n_distinct)) |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "n_values") |> 
  filter(n_values <= 1) |> 
  pull(variable)
data <-
  data |> 
  select(-rm_variables)
```

### Question 3

> Perform the same action to detect those variables that have a different value for each individual (all values are different). These columns are important, they will be the ids of the respondent, but one is enough (if there are several, delete one of them).

```{r}
#| code-fold: true
rm_variables <-
  data |> 
  summarise(across(everything(), n_distinct)) |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "n_values") |> 
  filter(n_values == nrow(data)) |> 
  pull(variable)
data <-
  data |> 
  # all of them except one of them
  select(-rm_variables[-1])
```

### Question 4

> To understand what the variables mean, you have in the folder `CIS-feminismos` the questionnaire that was made to each respondent in `cues3428.pdf` and the sampling data sheet in `FT3428.pdf`. In these documents you will see how, for example, the variables of the questionnaire almost all begin with `p...`, although we also have other types of variables related to household chores (`tarhog...`), children (`hijomenor...`), childcare (`cuidadohijos...`), care tasks in general (`tareascuid...`) and other variables related to the ideological scale, religion, voting memory, etc. In addition, all variables starting with `ia_xxx` refer to codes regarding possible incidences in the data collection and the variables `peso...` to the type of weighting used in the sampling. Proceed to eliminate these last two type of variables.

```{r}
#| code-fold: true
data <-
  data |> 
  select(-contains("ia_"), -contains("peso"))
```

### Question 5

> Calculate the number of interviews per autonomous community (`ccaa`) and extract the 5 with the fewest interviews.

```{r}
#| code-fold: true
#| eval: false
data |>
  count(ccaa, sort = TRUE)

data |>
  count(ccaa) |> 
  slice_min(n = 5, n)
```


### Question 6

> Use the `{datapasta}` package (see <https://javieralvarezliebana.es/docencia/mucss-data-programming/slides/#/data-pasta>) to copy from the [INE page](https://www.ine.es/jaxiT3/Datos.htm?t=2853) the population of each province and import it into a tibble

```{r}
#| code-fold: true
# this code should be automatically generated by de addin in R Studio
# after installing the R package
population <- 
  tibble::tribble(~Total, ~`47.385.107`,
                    "01 Andaluc√≠a",   "8.472.407",
                       "02 Arag√≥n",   "1.326.261",
      "03 Asturias, Principado de",   "1.011.792",
               "04 Balears, Illes",   "1.173.008",
                     "05 Canarias",   "2.172.944",
                    "06 Cantabria",     "584.507",
              "07 Castilla y Le√≥n",   "2.383.139",
         "08 Castilla - La Mancha",   "2.049.562",
                     "09 Catalu√±a",   "7.763.362",
         "10 Comunitat Valenciana",   "5.058.138",
                  "11 Extremadura",   "1.059.501",
                      "12 Galicia",   "2.695.645",
         "13 Madrid, Comunidad de",   "6.751.251",
            "14 Murcia, Regi√≥n de",   "1.518.486",
  "15 Navarra, Comunidad Foral de",     "661.537",
                   "16 Pa√≠s Vasco",   "2.213.993",
                    "17 Rioja, La",     "319.796",
                        "18 Ceuta",      "83.517",
                      "19 Melilla",      "86.261"
  )
```

### Question 7

> Rename properly the variables of `population` datasets, converts the population to number and separates the variable of the autonomous communities in two, one with the code and another with the name (according to INE), properly processing and trimming the names. See before exercises about `{stringr}` package.

```{r}
#| code-fold: true
population <-
  population |> 
  rename(ccaa = Total, pop = `47.385.107`) |> 
  # First to convert to numeric we need to remove dots
  mutate("pop" = as.numeric(str_replace_all(pop, "\\.", ""))) |> 
  # if we use " " as sep, we split also some names
  separate(col = ccaa, into = c("cod_INE", "name"), sep = 2) |> 
  # From stringr package, str_squish() removes whitespace at the start 
  # and end, and replaces all internal whitespace with a single space.
  mutate(name = str_squish(name))
```

### Question 8

> Add the population information to our CIS table. Then calculate a summary table with the ratio between the percentage of population represented by each community (of the total of Spain) and the percentage of respondents of each community (with respect to the total). Which are the 3 most over-represented and which are the 3 least? Think that if the ratio is higher than 1, it implies that there is a lower proportion of respondents from that community than would correspond to it by population (under-represented)


```{r}
#| code-fold: true
#| eval: false
data <-
  data |> 
  # you need first to convert cod_INE as numeric since both 
  # key columns should be of same type
  left_join(population |> mutate("cod_INE" = as.numeric(cod_INE)),
            by = c("ccaa" = "cod_INE")) |> 
  relocate(name, pop, .after = "ccaa")

prop_surv_pop <-
  data |>
  summarise("prop_surv" = n() / sum(nrow(data)),
            "pop" = unique(pop),
            .by = ccaa) |> 
  mutate("prop_pop" = pop / sum(pop),
         "ratio" = prop_pop / prop_surv)

# overrepresented
prop_surv_pop |> 
  slice_min(ratio, n = 3)

# underrepresented
prop_surv_pop |> 
  slice_max(ratio, n = 3)
```


## Lists

We have already seen that lists are an object in R that allows us to store [**collections of variables of different type**]{.hl-yellow} (as with `data.frame` and `tibble`) but also [**different lengths**]{.hl-purple}, with totally heterogeneous structures (even a list can have inside it another list).

```{r}
name <- "Javi"
age <- 34
marks <- c(7, 8, 5, 3, 10, 9)
parents <- c("Paloma", "Goyo")

list_var <- list("name" = name, "age" = age, "marks" = marks, "parents" = parents)
```


```{r}
list_var$name
list_var$marks
```



We can also make [**lists with other lists inside**]{.hl-yellow}, so that to access each level we must use the `[[]]` operator.

```{r}
list_of_lists <- list("list_1" = list_var[1:2], "list_2" = list_var[3:4])
names(list_of_lists)
```

```{r}
names(list_of_lists[[1]])
```

```{r}
list_of_lists[[1]][[1]]
```


We are allowed to store [**n-dimensional data**]{.hl-yellow}!

&nbsp;

One of the disadvantages is that a list [**cannot be vectorized**]{.hl-yellow} immediately, so any arithmetic operation applied to a list will give [**error**]{.hl-red}.

```{r}
#| error: true
data <- list("a" = 1:5, "b" = 10:20)
data / 2
```

For this purpose, one of the common (but outdated) options is to make use of the `lapply()` family.

```{r}
lapply(data, FUN = function(x) { x / 2})
```

By default, the output of `lapply()`is always a [**list of equal length**]{.hl-yellow}.



A more flexible and versatile option is to make use of the `{purrr}` package of the `{tidyverse}` environment.

```{r}
library(purrr)
```

This package is intended to mimic the [**functional programming**]{.hl-yellow} of other languages such as Scala or Hadoop's [**map-reduce strategy**]{.hl-yellow} (from Google).

![](img/purrr.png)



The simplest function of the `{purrr}` package is the `map()` function, which [**applies a vectorized function**]{.hl-yellow} to each of the elements of a list. Let's see a first example applied to vectors


* `map()` allows us to [**"map" each list**]{.hl-yellow} and apply the function element by element (if applicable).

```{r}
x <- list("x1" = 1:4, "x2" = 11:20)
map(x, sqrt) 
```


::: callout-warning
## Be careful

With vectors we have a default vectorization because `R` performs element-by-element operations. Note that, by default, the [**output of `map` is a list**]{.hl-yellow}.
:::


Let's look at another example. Define in a list two samples from 2 normal distributions, of different sample size and different mean. Compute the mean of each one.

```{r}
#| code-fold: true
x <- list(rnorm(n = 1500, mean = 0, sd = 0.7),
          rnorm(n = 2800, mean = 2, sd = 1.5))
map(x, mean)
```

What if we want to calculate the mean of their squared values?

```{r}
#| code-fold: true
map(x, function(x) { mean(x^2) })
```


In addition to being [**more readable and efficient**]{.hl-yellow}, with `{purrr}` we can [**decide the output format**]{.hl-yellow} after the operation

* output as [**numeric (double) vector**]{.hl-purple} with `map_dbl()`
* output as [**numeric (int) vector**]{.hl-purple} with `map_int()`
* output as [**character vector**]{.hl-purple} with `map_chr()`
* output as [**logical vector**]{.hl-purple} with `map_lgl()`

```{r}
library(glue)
map_dbl(x, mean)
map_chr(x, function(x) { glue("Mean is {round(mean(x), 5)}") })
```


```{r}
c(x[[1]][3], x[[2]][3])
```


Also, if you pass it a [**number**]{.hl-yellow} instead of a function, it will return the [**ith element of each list**]{.hl-yellow}.

```{r}
map_dbl(x, 3)
```

```{r}
list_dummy <- list("a" = dplyr::starwars, "b" = tidyr::billboard)
```


We can also use `pluck()` to [**access to the i-th element of a list**]{.hl-yellow}

```{r}
list_dummy |> 
  pluck(1)
```


We also have the option of generalizing it to be able to use functions that [**need two arguments in the form of a list**]{.hl-yellow} (binary operations), with `map2()`


```{r}
x <- list("a" = 1:3, "b" = 4:6)
y <- list("c" = c(-1, 4, 0), "b" = c(5, -4, -1))
map2(x, y, function(x, y) { x^2 + y^2})
```

We can obtain the output in the form of `data.frame` by adding `list_rbind()` or `list_cbind()`, which [**converts a list into a table**]{.hl-yellow}.


```{r}
x <- c("a", "b", "c")
y <- 1:3
map2(x, y, function(x, y) { tibble(x, y) }) |> list_rbind()
```


We can generalize it further with `pmap_xxx()` which allows us to use [**multiple arguments (multiple lists)**]{.hl-yellow}.


```{r}
x <- list(1, 1, 1)
y <- list(10, 20, 30)
z <- list(100, 200, 300)
pmap_dbl(list(x, y, z), sum)
```


We have other types of iterators that, although they assume inputs, do not return anything, as `walk()` (just one input argument), `walk2()` (two arguments) and `pwalk()` (multiple arguments), all [**invisibly return**]{.hl-yellow}, just call a function for its [**side effects**]{.hl-yellow} rather than its return value.

```{r}
list("a" = 1:3, "b" = 4:6) |>
  map2(list("a" = 11:13, "b" = 14:16),
       function(x, y) { x + y }) |> 
  walk(print)
```


### üíª It's your turn 


::: panel-tabset

### [**Exercise 1**]{.hl-yellow}

üìù Define a list of 4 elements of different types and access the second of them (I will include one that is a tibble so that you can see that in a list there is room for everything).

```{r}
#| code-fold: true
#| eval: false
list_example <-
  list("name" = "Javier", "cp" = 28019,
       "siblings" = TRUE,
       "marks" = tibble("maths" = c(7.5, 8, 9),
                        "lang" = c(10, 5, 6)))
list_example
```

### [**Exercise 2**]{.hl-yellow}

üìù From the list above, access the elements that occupy places 1 and 4 of the list defined above.

```{r}
#| code-fold: true
#| eval: false

list_example[c(1, 4)]

list_example$name
list_example$marks

list_example[c("name", "marks")]
```



### [**Exercise 3**]{.hl-yellow}

üìù  Load the `starwars` dataset from the `{dplyr}` package and access the second movie that appears in `starwars$films` (for each character). Determine which ones do not appear in more than one movie.

```{r}
second_film <- map(starwars$films, 2)
map_lgl(second_film, is.null)
```

:::



## Quali: factors (forcats package)

In statistics, when we talk about [**qualitative variables**]{.hl-yellow}, we will call **levels or modalities** the **different values** that these data can take. For example, in the case of the `sex` variable of the `starwars` set, we have 4 allowed levels: `female`, `hermaphroditic`, `male` and `none` (in addition to missing data).

```{r}
starwars |> count(sex)
```


These kinds of variables are known in `R` as [**factors**]{.hl-yellow}, and the fundamental package to deal with them is `{forcats}` (from the `{tidyverse}` environment). 


![](img/factors.jpg)


This package allows us to set the [**levels**]{.hl-yellow} (stored internally as `levels`) that a given categorical variable takes so that no mistakes, errors in data collection and generation can be generated. It also makes their analysis less computationally expensive when doing searches and comparisons, giving them a [**different treatment than normal text strings**]{.hl-yellow}.


Let's see a simple example simulating a `party` variable taking the values `"PP"`, `"PSOE"` and `"SUMAR"` (of size 15)

```{r}
set.seed(1234567)
party <- sample(x = c("PP", "PSOE", "SUMAR"), size = 15, replace = TRUE)
party
```

The `party` variable is currently of [**type text**]{.hl-yellow}, of type `chr`, something we can check with `class(state)`.

```{r}
class(party)
```


From a statistical and computational point of view, for `R` this variable right now would be equivalent to a named variable. But statistically [**a variable as a string**]{.hl-yellow} is not the same as a categorical variable that [**can only take those 3 levels**]{.hl-yellow}. How to [**convert to factor**]{.hl-yellow}? 


By making use of the `as_factor()` function from the `{forcats}` package.

```{r}
library(tidyverse)
party_fct <- tibble("id" = 1:length(party),
                    "party" = as_factor(party))
party_fct
```


Not only the class of the variable  has  changed, but now, below the saved value, the sentence `Levels: ...` appears: these are the [**modalities or levels**]{.hl-yellow} of our qualitative. 

```{r}
party_fct |> pull(party)
```


Imagine that we are defining the database of deputies of the Congress and that the deputies of the PP did not attend that day to the plenary session: although our variable does not take that value THAT DAY, the state `PSOE` is a [**allowed level in the database**]{.hl-yellow} (so even if we eliminate it, because it is a factor, the level remains, we do not have it now but it is an allowed level).


```{r}
party_fct |> 
  filter(party %in% c("PP", "SUMAR")) |> 
  pull(party)
```


With `factor()` function we can [**explicitly specify**]{.hl-yellow} the names of the modalities and using `levels = ...` we can explicitly tell it the [**"order" of the modalities**]{.hl-yellow} 


```{r}
party_fct <-
  tibble(id = 1:length(party),
         party = factor(party, levels = c("SUMAR", "PP", "PSOE")))
party_fct |> pull(party)
```


The previous "order" is just in the sense which it will be counted/plotted first) but [**we don't have (yet) an ordinal variable**]{.hl-purple}


```{r}
party_fct$party < "SUMAR"
```


What if we want to [**define a qualitative variable ORDINAL**]{.hl-yellow}? Inside `factor()` we must indicate that `ordered = TRUE`.

```{r}
marks <- c("A", "E", "F", "B", "A+", "A", "C", "C", "D", "B", "A", "C", "C", "E", "F", "D", "A+")
marks_database <-
  tibble("student" = 1:length(marks),
         "marks" =
           factor(marks, levels = c("F", "E", "D", "C", "B", "A", "A+"),
                  ordered = TRUE))
marks_database |> pull(marks)
```

What changes? If you notice now, although the variable is still qualitative, we can [**make comparisons and order the records**]{.hl-yellow} because there is a [**hierarchy**]{.hl-purple} between the modalities.

```{r}
marks_database |> filter(marks >= "B")
```


If we want to tell it to [**drop an unused level**]{.hl-yellow} at that moment (and that we want to [**exclude from the definition**]{.hl-purple}) we can do it with `fct_drop()`.

```{r}
marks_database |> 
  filter(marks %in% c("F", "E", "D", "C", "B", "A")) |> 
  pull(marks)
```


```{r echo = FALSE,  out.width = "100%", fig.align = "left"}
knitr::include_graphics("./img/drop_factor.jpg")
``` 



```{r}
marks_database |> 
  filter(marks %in% c("F", "E", "D", "C", "B", "A")) |> 
  mutate(marks = fct_drop(marks)) |>  
  pull(marks)
```


Just as we can delete levels we can [**expand existing levels**]{.hl-yellow} (even if there is no data for that level at that time) with `fct_expand()`.



```{r echo = FALSE,  out.width = "100%", fig.align = "left"}
knitr::include_graphics("./img/factor_expand.jpg")
``` 

```{r}
marks_database |> 
  mutate(marks = fct_expand(marks, c("F-", "A+", "A++"))) %>% 
  pull(marks)
```


In addition with `fct_explicit_na()` we can [**assign a level to the missing values**]{.hl-yellow} to be included in the analysis and visualizations.



```{r echo = FALSE,  out.width = "100%", fig.align = "left"}
knitr::include_graphics("./img/factor_explicit.jpg")
``` 


```{r}
fct_explicit_na(factor(c("a", "b", NA)))
```


Even once defined we can [**reorder the levels**]{.hl-yellow} with `fct_relevel()`.


```{r}
marks_database_expand <- 
  marks_database |>  
  mutate(marks = fct_expand(marks, c("F-", "A+", "A++"))) |> 
  pull(marks)

marks_database_expand |> 
  fct_relevel(c("F-", "F", "E", "D", "C", "B", "A", "A+", "A++"))
  
```


This way of working with qualitative variables allows us to give a [**theoretical definition**]{.hl-yellow} of our database, and we can even count values that do not yet exist (but could), making use of `fct_count()`.


```{r echo = FALSE,  out.width = "70%", fig.align = "left"}
knitr::include_graphics("./img/fct_count.jpg")
``` 


```{r}
marks_database |> 
  mutate(marks = fct_expand(marks, c("F-", "A+", "A++"))) |> 
  pull(marks) |> 
  fct_count()
```



The levels can also be [**sorted by frequency**]{.hl-yellow} with `fct_infreq()`.

```{r}
marks_database |> 
  mutate(marks = fct_infreq(marks)) |> 
  pull(marks)

marks_database |> 
  mutate(marks = fct_infreq(marks)) |> 
  pull(marks) |> 
  fct_count()
```


Sometimes we will want to [**group levels**]{.hl-yellow}, for example, not allowing levels that [**do not happen a minimum number of times**]{.hl-yellow} with `fct_lump_min(..., min = ..)` (observations that do not meet this will go to a **generic level** called `Other`, although this can be changed with the `other_level` argument). 



```{r}
marks_database |> 
  pull(marks) %>% 
  fct_lump_min(min = 3)
```


```{r}
marks_database |> 
  pull(marks) |>
  fct_lump_min(min = 3,
               other_level = "Less frequent")
```



We can do something equivalent but based on its [**relative frequency**]{.hl-yellow} with `fct_lump_prop()`.


```{r}
marks_database |> 
  pull(marks) |> 
  fct_lump_prop(prop = 0.15,
                other_level = "less frequent")
```


We can apply this to our datasets to [**recategorize variables**]{.hl-yellow} very quickly.

```{r}
starwars |>  
  drop_na(species) |> 
  mutate(species =
           fct_lump_min(species, min = 3,
                        other_level = "Others")) |>  
  count(species)
```




With `fct_reorder()` we can also indicate that we want to [**order the factors**]{.hl-yellow} according to a function applied to another variable.


```{r}
starwars_factor <- 
  starwars |>  
  drop_na(height, species) |> 
  mutate(species =
           fct_lump_min(species, min = 3,
                        other_level = "Others"))
```



```{r}
starwars_factor |>  pull(species)
```


```{r}
starwars_factor |> 
  mutate(species = fct_reorder(species, height, mean)) |> 
  pull(species)
```



### üíª It's your turn


::: panel-tabset

### [**Exercise 1**]{.hl-yellow}

üìù Given the variable `months` defined below (defined as a character vector), convert this variable to factor (just that)

```{r}
months <- c("Jan", "Feb", "Mar", "Apr")
```

```{r}
#| code-fold: true
#| eval: false
months <- c("Jan", "Feb", "Mar", "Apr")
months_fct <- as_factor(months)
months_fct
```

### [**Exercise 2**]{.hl-yellow}

üìù Given the variable `months` defined below converts this variable to a factor but indicating the levels correctly.

```{r}
months <- c(NA, "Apr", "Jan", "Oct", "Jul", "Jan", "Sep", NA, "Feb", "Dic",
           "Jul", "Mar", "Jan", "Mar", "Feb", "Apr", "May", "Oct", "Sep",  NA,
           "Dic", "Jul", "Nov", "Feb", "Oct", "Jun", "Sep", "Oct", "Oct", "Sep")
```



```{r}
#| code-fold: true
#| eval: false
months_fct <-
  factor(months,
         levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dic"))
months_fct
```

  
### [**Exercise 3**]{.hl-yellow}

üìù Count how many values there are for each month but keep in mind that they are factors (maybe there are unused levels and you should get a 0 for them).


```{r}
#| code-fold: true
#| eval: false
meses_fct |>  fct_count()
```

### [**Exercise 4**]{.hl-yellow}

üìù Since there are missing values, it indicates that the absentee is a thirteenth level labeled `"missing"`.

```{r}
#| code-fold: true
#| eval: false
months_fct <- 
  months_fct |> 
  fct_explicit_na(na_level = "missing")
months_fct
```

### [**Exercise 5**]{.hl-yellow}

üìù Removes unused levels.


```{r}
#| code-fold: true
#| eval: false
months_fct <- 
  months_fct %>%
  fct_drop()
months_fct
```


### [**Exercise 6**]{.hl-yellow}

üìù Sort the levels by frequency of occurrence.


```{r}
#| code-fold: true
#| eval: false
months_fct |> 
  fct_infreq()
```

### [**Exercise 7**]{.hl-yellow}

üìù Group levels so that any level that does not appear at least 7% of the time is grouped in a level called `"other months"`.
  
```{r}
#| code-fold: true
#| eval: false
months_fct <-
  months_fct |> 
  fct_lump_prop(prop = 0.07, other_level = "other months")
months_fct 
```

:::

## üê£ Case study II: CIS survey

[üìä Data](https://drive.google.com/drive/folders/18Ok6Epqcimszqguj_JTTLuSbs5Ot5Srd?usp=sharing)

We are going to work again with file contains data from the CIS (Centro de Investigaciones Sociol√≥gicas) barometer ¬´Perceptions on equality between men and women and gender stereotypes¬ª whose sample work was carried out from November 6 to 14 (4000 interviews of both sexes over 16 years old in 1174 municipalities and 50 provinces).

### Question 1

> Run the entire code of the previous case study

```{r}
library(haven)
data <-
  read_sav(file = "./datos/CIS-feminismo/3428.sav") |> 
  janitor::clean_names()

rm_variables <-
  data |> 
  summarise(across(everything(), n_distinct)) |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "n_values") |> 
  filter(n_values <= 1) |> 
  pull(variable)
data <-
  data |> 
  select(-rm_variables)

rm_variables <-
  data |> 
  summarise(across(everything(), n_distinct)) |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "n_values") |> 
  filter(n_values == nrow(data)) |> 
  pull(variable)
data <-
  data |> 
  # all of them except one of them
  select(-rm_variables[-1])

data <-
  data |> 
  select(-contains("ia_"), -contains("peso"))

population <- 
  tibble::tribble(~Total, ~`47.385.107`,
                    "01 Andaluc√≠a",   "8.472.407",
                       "02 Arag√≥n",   "1.326.261",
      "03 Asturias, Principado de",   "1.011.792",
               "04 Balears, Illes",   "1.173.008",
                     "05 Canarias",   "2.172.944",
                    "06 Cantabria",     "584.507",
              "07 Castilla y Le√≥n",   "2.383.139",
         "08 Castilla - La Mancha",   "2.049.562",
                     "09 Catalu√±a",   "7.763.362",
         "10 Comunitat Valenciana",   "5.058.138",
                  "11 Extremadura",   "1.059.501",
                      "12 Galicia",   "2.695.645",
         "13 Madrid, Comunidad de",   "6.751.251",
            "14 Murcia, Regi√≥n de",   "1.518.486",
  "15 Navarra, Comunidad Foral de",     "661.537",
                   "16 Pa√≠s Vasco",   "2.213.993",
                    "17 Rioja, La",     "319.796",
                        "18 Ceuta",      "83.517",
                      "19 Melilla",      "86.261"
  )

population <-
  population |> 
  rename(ccaa = Total, pop = `47.385.107`) |> 
  # First to convert to numeric we need to remove dots
  mutate("pop" = as.numeric(str_replace_all(pop, "\\.", ""))) |> 
  # if we use " " as sep, we split also some names
  separate(col = ccaa, into = c("cod_INE", "name"), sep = 2) |> 
  # From stringr package, str_squish() removes whitespace at the start 
  # and end, and replaces all internal whitespace with a single space.
  mutate(name = str_squish(name))

data <-
  data |> 
  # you need first to convert cod_INE as numeric since both 
  # key columns should be of same type
  left_join(population |> mutate("cod_INE" = as.numeric(cod_INE)),
            by = c("ccaa" = "cod_INE")) |> 
  relocate(name, pop, .after = "ccaa")
```
### Question 2

> Notice that many columns are of type `<dbl+lbl>`, a new data type inherited from SPSS to be able to have the numeric value (e.g. `1` for the autonomous community ccaa of Andalusia) but also the label (`¬´Andaluc√≠a¬ª`). You can see more in the `{labelled}` package but for now we are going to use `to_factor()` to be able to work with those labels if we want.

```{r}
data |> 
  # numeric mode
  count(ccaa == 1)

data |> 
  # labels mode
  count(labelled::to_factor(ccaa) == "Andaluc√≠a")
```


> The variables `escfeminis` and `sitconv` actually correspond to question `P16` and `P18` of the questionnaire. Rename them as `p16` and `p18`.

```{r}
#| code-fold: true
data <- 
  data |> 
  rename(p16 = escfeminis, p18 = sitconv)
```

### Question 3

The variables `tarhogentrev_1`, `tarhogentrev_2`, `tarhogentrev_8`, `tarhogentrev_9`, `tarhogentrev_hh`, `tarhogentrev_mm` refer to the same question: time spent on household chores.

* The variable `tarhogentrev_1` stores a `1` if the respondent answered in hours (otherwise `2`)

* The variable `tarhogentrev_2` stores a `1` if the respondent answered in minutes (otherwise `2`)

* The variable `tarhogentrev_8` stores a `1` if the respondent answered ‚Äúcan't say‚Äù (otherwise `2`)

* The variable `tarhogentrev_9` stores a `1` if the respondent did not want to answer.

* The variables `tarhogentrev_hh` and `tarhogentrev_mm` store the amounts (in hours or minutes, or `99` or `98` if the respondent did not know or did not want to answer).

> Use the information from these 5 variables in such a way that the information is contained in a single variable `p19` with the number of minutes spent (if `NA` cannot be known) and delete all the others

```{r}
#| code-fold: true
data <- 
  data |>
  mutate("p19" =
           if_else(tarhogentrev_1 == 1, tarhogentrev_hh * 60,
                   if_else(tarhogentrev_2 == 1, tarhogentrev_mm, NA)),
         .after = p18) |>
  select(-contains("tarhogentrev"))
```

### Question 4

> Perform the same recoding with the variables `tarhogparej_1`, `tarhogparej_2`, `tarhogparej_8`, `tarhogparej_9`, `tarhogparej_hh`, `tarhogparej_mm` (time spent on household chores of the couple) and group their info in `p20`.


```{r}
data <- 
  data |> 
  mutate("p20" =
           if_else(tarhogparej_1 == 1, tarhogparej_hh * 60,
                   if_else(tarhogparej_2 == 1, tarhogparej_mm, NA)),
         .after = p19) |>
  select(-contains("tarhogparej"))
```

### Question 5

> The variable `hijomenor_1` contains a `1` if it has children and the number of fixed is in `hijomenor_n`; if `hijomenor_97` contains a `1` we should impute `0` in number of children; if `hijomenor_99` is `1` we should impute `NA`. Collects the info of the 4 variables in a single variable called `p21`.

```{r}
data <- 
  data |> 
  mutate("p21" =
           if_else(hijomenor_1 == 1, hijomenor_n,
                   if_else(hijomenor_97 == 1, 0, NA)),
         .after = p20) |>
  select(-contains("hijomenor"))
```

### Question 6

> The following variables (`cuidadohijos_xxx` and `cuidadohijospar_xxx`) code something similar with respect to time spent on childcare (of the respondent and his/her partner). Summarize the information in two variables `p21a` and `p21b` as we have done with `p19` and `p20`.


```{r}
data <- 
  data |> 
  mutate("p21a" =
           if_else(cuidadohijos_1 == 1, cuidadohijos_hh * 60,
                   if_else(cuidadohijos_2 == 1, cuidadohijos_mm, NA)),
         "p21b" =
           if_else(cuidadohijospar_1 == 1, cuidadohijospar_hh * 60,
                   if_else(cuidadohijospar_2 == 1, cuidadohijospar_mm, NA)),
         .after = p21) |>
  select(-contains("cuidadohijos"))
```

### Question 7

> Do the same with `tareascuid_xxx` regarding the time spent on care and attention of a sick and/or dependent person in a working day, and store the info in `p22a`. Think that in `p22` is stored if you have a sick or dependent person in your care; if `p22` is `No` (`2`) or `N.C.` (`9`) the value of `p22a` should be `NA`.

```{r}
data <- 
  data |> 
  mutate("p22a" =
           if_else(p22 %in% c(2, 9), NA,
                   if_else(tareascui_1 == 1, tareascui_hh * 60,
                           if_else(tareascui_2 == 1, tareascui_mm, NA))),
         .after = p22) |>
  select(-contains("tareascui"))
```

### Question 8

> Finally the variables `escideol`, `participaciong`, `recuvotog`, `escuela`, `nivelestentrev`, `religion`, `practicarelig6`, `sitlab`, `relalab`, `cno11`, `tipojor`, `ingreshog`, `clasesocial`, `paisnac`, `paisnac2` should be renamed as `p24`, `p25`, `p25a`, `p26`, `p26a`, `p27`, `p27a`, `p28`, `p28a`, `p28b`, `p28c`, `p29`, `p30`, `p31` and `p31a`. In addition the variable `sinceridad` collects the subjective sincerity that the pollster has perceived in the respondent and the variables `recuvotogr` and `recuerdo` capture vote recall information that we already have collected elsewhere. Rename the former, delete the latter and move the variables `estudios`, `cno11r`, `clasesub` (studies, occupation according to Social Security coding and subjective class identification) before the surveyed questions.


```{r}
#| code-fold: true
data <-
  data |> 
  rename(p24 = escideol, p25 = participaciong, p25a = recuvotog,
         p26 = escuela, p26a = nivelestentrev, p27 = religion,
         p27a = practicarelig6, p28 = sitlab, p28a = relalab,
         p28b = cno11, p28c = tipojor, p29 = ingreshog,
         p30 = clasesocial, p31 = paisnac, p31a = paisnac2) |> 
  select(-(sinceridad:recuerdo)) |> 
  relocate(estudios, cno11r, clasesub, .before = p0)
data
```


### Question 9

Note that there are variables in the questionnaire, such as `p0` and `p1`, that although they are coded numerically, in reality we should have them only qualitatively. For example, if you look at the questionnaire, the variable `p0` corresponds to the question `P0.c` which says ‚ÄúFirst of all I would like to ask you if you have...

* Spanish nationality --> 1

* Spanish nationality and other --> 2

* Other nationality --> 3

```{r}
data |> count(p0)
```

If we were to leave the variable as it is, numerically, we would be assuming two incorrect things:

1) There is a hierarchy (having Spanish nationality is better or worse than having another nationality).

2) This hierarchy is also quantified numerically: since 2 is double 1, it implies that, in some way, having two nationalities is ‚Äútwice as much‚Äù as having only Spanish nationality

If you look at the questionnaire, all the variables that are `p{number}` (without the low slash `_`) are of this type. The others respond to a more complex questionnaire (a series of questions to be assessed on a scale that is qualitative ordinal and we will consider that it can be converted to an ordinal factor).

> Convert the `p{number}` to qualitative (nominal) with its label and the `p{number}_x` to ordinal qualitative (with its number). In the latter, note that ‚Äúdon't know/no answer‚Äù responses are coded as `98` and `99` (and should be NA).

```{r}
#| code-fold: true
library(labelled)
data <-
  data |> 
  # with $ to specify "not more than..."
  mutate(across(matches("p[0-9]{1}$|p[0-9]{2}$"), to_factor),
         across(matches("p[0-9]{1}_|p[0-9]{2}_"),
                function(x) { factor(if_else(x %in% c(98, 99), NA, x),
                                     ordered = TRUE) } ))
```

> In some compound questions with only 5 options, the `NA` are coded as `7`, `8` or `9`.

```{r}
data <-
  data |> 
  # with $ to specify "not more than..."
  mutate(across(matches("p[0-9]{1}_|p[0-9]{2}_"),
                function(x) { factor(if_else(x %in% c(7, 8, 9), NA, x),
                                     ordered = TRUE) } ))
```

### Question 10

> Calculate a two-dimensional table of relative frequencies between the variable `sex` and the variable `p4` (degree of inequality between men and women in Spain) normalizing as you consider to answer the following question: what percentage of women believe that inequalities are very large or quite large? And of men?

```{r}
#| code-fold: true
#| eval: false
table_freq <- table(to_factor(data$sexo), data$p4)
prop.table(table_freq, margin = 1)
# 67% of women
# 48.7% of men
```


### Question 11

> Calculate a measure of association between these variables and detail conclusions.

```{r}
#| code-fold: true
#| eval: false
data |> 
  summarise("sig_chisq" = chisq.test(to_factor(sexo), p4)$p.value)
# tiny p-valor --> enough evidences againts null hyp --> 
# answers to p4 depends on sex
```

### Question 12

> Create a simple graph in `ggplot()` that allows to represent the variable `p4` in function of sexes, allowing to appreciate the differences in them.

```{r}
#| code-fold: true
#| eval: false
# first option: geom_bar with abs freqs
ggplot(data |>
         mutate("sexo" = to_factor(sexo))) +
  geom_bar(aes(x = p4, fill = p4), alpha = 0.7) +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~sexo) +
  theme_minimal()

# second option: geom_col with rel freqs
ggplot(data |>
         mutate("sexo" = to_factor(sexo)) |> 
         count(sexo, p4) |> 
         mutate("porc" = n/sum(n))) +
  geom_col(aes(x = p4, y = porc, fill = p4), alpha = 0.7) +
  scale_y_continuous(labels = scales::label_percent()) +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~sexo) +
  theme_minimal()

# third option: horizontal bars
ggplot(data |>
         mutate("sexo" = to_factor(sexo)) |> 
         count(sexo, p4) |> 
         mutate("porc" = n/sum(n)) |> 
         mutate("porc" = if_else(sexo == "Hombre", -porc, porc))) +
  geom_col(aes(y = p4, x = porc, fill = p4), alpha = 0.7) +
  scale_x_continuous(labels = scales::label_percent()) +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~sexo, scales = "free_x") +
  theme_minimal()

# fourth option: fill geom bar
ggplot(data |>
         mutate("sexo" = to_factor(sexo))) +
  geom_bar(aes(x = sexo, fill = p4), position = "fill",
           alpha = 0.7) +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
```

### Question 13

> Performs the same analysis but regroups `p4` earlier into only 3 levels: grandes (`Muy grandes` and `Bastante grandes`), peque√±as (`Peque√±as` and `Casi inexistentes`) and `NS/NC`.

```{r}
#| code-fold: true
#| eval: false
data <-
  data |> 
  mutate("p4" =
           fct_collapse(data$p4,
                        "grandes" = c("Muy grandes", "Bastante grandes"), "NS/NC" = c("N.S.", "N.C."),
                        other_level = "peque√±as"))

table_freq <- table(to_factor(data$sexo), data$p4)
prop.table(table_freq, margin = 1)
# 67% of women
# 48.7% of men

data |>
  summarise("sig_chisq" = chisq.test(to_factor(sexo), p4)$p.value)

ggplot(data |>
         mutate("sexo" = to_factor(sexo))) +
  geom_bar(aes(x = sexo, fill = p4), position = "fill",
           alpha = 0.7) +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
```

### Question 14

> Question `p16` captures, on a scale of 0 to 10 (where 0 means ‚Äúnot at all feminist‚Äù and 10 means ‚Äúvery feminist‚Äù) the self-placement of the respondents. How is this variable distributed by gender? Make a frequency table

```{r}
#| code-fold: true
#| eval: false
prop.table(table(to_factor(data$sexo), data$p16), margin = 1)
```

> What % of men and women declare themselves above 6?

```{r}
#| code-fold: true
#| eval: false
data |> 
  mutate("p16" = factor(p16, ordered = TRUE)) |> 
  group_by(sexo) |> 
  count(p16 > 7) |> 
  mutate("porc" = 100*n/sum(n)) |> 
  ungroup()
# 48.3% between men
# 62.3% between women
```

> Question `p19` captured minutes spent on household chores. Converts to numeric (‚ÄúNo minutes‚Äù as 0).

```{r}
#| code-fold: true
data <-
  data |>
  mutate("p19" = as.numeric(as.character(p19)))

```

> After that, first calculate the average by sex disaggregated, and then the average disaggregated by sex and `p16`. What conclusions do you draw? Make a graph if you consider it

```{r}
#| code-fold: true
mean_sex <-
  data |> 
  summarise("avg_min" = mean(p19, na.rm = TRUE), .by = sexo)
# 175 minutes in women vs 129 in men

mean_sex_feminism <-
  data |> 
  summarise("avg_min" = mean(p19, na.rm = TRUE),
            .by = c(sexo, p16))
ggplot(mean_sex_feminism) +
  geom_col(aes(x = p16, y = avg_min, fill = avg_min),
           alpha = 0.7) +
  scale_fill_viridis_c(option = "magma") +
  facet_wrap(~to_factor(sexo)) +
  theme_minimal()
# We can see how in men the average minutes remains cte
# without depending on the self-localization in the
# feminism scale
```

### Question 15

> We will calculate in a new variable the difference in minutes spent at home between the respondent (`p19`) and his/her partner (`p20`).

```{r}
#| code-fold: true
data <- 
  data |>
  mutate("p20" = as.numeric(as.character(p20))) |> 
  mutate("diff_min" = p19 - p20)
```

> Repeat the previous analysis using this difference in minutes.

```{r}
#| code-fold: true
mean_sex <-
  data |> 
  filter(p17a %in% c(1, 2) & as.numeric(p17a) != as.numeric(sexo)) |>
  summarise("avg_diff_min" = mean(diff_min, na.rm = TRUE), .by = sexo)
# +63.7 minutes in women vs -40.6 in men

mean_sex_feminism <-
  data |> 
  filter(p17a %in% c(1, 2) & as.numeric(p17a) != as.numeric(sexo)) |>
  summarise("avg_diff_min" = mean(diff_min, na.rm = TRUE),
            .by = c(sexo, p16))

ggplot(mean_sex_feminism) +
  geom_col(aes(x = p16, y = avg_diff_min, fill = avg_diff_min),
           alpha = 0.7) +
  scale_fill_viridis_c(option = "magma") +
  facet_wrap(~to_factor(sexo)) +
  theme_minimal()
```




