---
title: "Series temporales"
subtitle: "An√°lisis de series temporales en R"
title-slide-attributes:
  data-background-image: img/time-series.jpg
  data-background-size: cover
  data-background-opacity: "0.35"
author: "Grado en Estad√≠stica Aplicada ‚Ä¢ Javier √Ålvarez Li√©bana"
affiliation: Facultad de Estudios Estad√≠sticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier √Ålvarez Li√©bana</strong>](...) ‚Ä¢ Grado en Estad√≠stica Aplicada (UCM)"
    slide-number: c/t
execute:
  echo: true
---


::: r-fit-text
[¬°Bienvenidos a la nave del tiempo!]{.flow}
:::

[**Dejad vuestras regresiones a un lado**]{style="color:#444442;"}

---

## ¬°Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3¬™ planta). [**Tutor√≠as (curso 2024-2025)**]{.hl-green}: ...

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier √Ålvarez Li√©bana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matem√°ticas (UCM). [**Doctorado en estad√≠stica**]{.hl-yellow} (UGR).

-   Encargado de la [**visualizaci√≥n y an√°lisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Espa√±ola de Estad√≠stica e IO**]{.hl-yellow} y la [**Real Sociedad Matem√°tica Espa√±ola**]{.hl-yellow}.
:::
:::
:::


Actualmente, [**investigador y docente en la Facultad de Estad√≠stica de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Entender el [**concepto de serie temporal y sus diferencias con la regresi√≥n**]{.hl-yellow} ‚Üí lo que te equivocaste ayer influye en lo que te equivocar√°s hoy

- Entender conceptos te√≥ricos b√°sicos de [**procesos estoc√°sticos**]{.hl-yellow}

- Aprender a manejar [**paquetes estad√≠sticos de R**]{.hl-yellow} de series temporales ‚Üí la aplicabilidad de la teor√≠a ser√° tu valor en el futuro

- Introducirnos en la [**metodolog√≠a Box-Jenkins**]{.hl-yellow} ‚Üí los datos deben ser estacionarios



:::
:::
:::

---

## Evaluaci√≥n


-   [**Evaluaci√≥n continua**]{.hl-yellow}: [**3 entregas individuales a ordenador en clase (20%-25%-35%)**]{.hl-purple}, y una [**entrega individual te√≥rica a papel en clase (20%)**]{.hl-purple}. [**Asistencia**]{.hl-yellow} no obligatoria pero se [**valorar√° positivamente**]{.hl-purple} la participaci√≥n.

. . .

* [**Examen final**]{.hl-yellow}: la nota ponderar√° en funci√≥n de tu evaluaci√≥n continua.

  - [**M√°s de un 7**]{.hl-purple} -> podr√°s decidir **peso del final entre un 0% y un 100%** de la nota (es decir, [**no ser√° obligatorio el final**]{.hl-yellow}).
  - [**Entre 6 y 7**]{.hl-purple} -> podr√°s decidir **peso del final entre un 35% y un 100%**.
  - [**Entre 5 y 6**]{.hl-purple} -> podr√°s decidir **peso del final entre un 60% y un 100%**.
  - [**Entre 3.5 y 5**]{.hl-purple} -> podr√°s decidir **peso del final entre un 80% y un 100%**.
  - [**Por debajo del 3.5**]{.hl-purple} -> el **peso del final ser√° del 100%**
  
Si tienes que hacer el examen final, ser√° **obligatorio presentarse y sacar m√°s de un 3** para aprobar.

---

## Planificaci√≥n entregas

* [**Entrega I (20%)**]{.hl-yellow}: 8 de octubre (120 minutos).

* [**Entrega II (25%)**]{.hl-yellow}: ... (120 minutos).

* [**Entrega III (35%)**]{.hl-yellow}: ... (120 minutos).

* [**Entrega te√≥rica (20%)**]{.hl-yellow}: ... (120 minutos).

&nbsp;

* [**Examen final**]{.hl-yellow}: 14 de enero (10:00-13:30)

&nbsp;

Se podr√°n modificar las fechas por saturaci√≥n con otras asignaturas siempre y cuando el/la delegado/a lo solicite con **m√°s de 7 d√≠as de antelaci√≥n**.



---

## Planificaci√≥n {#planificacion}

::: column-screen-inset-right
::: {style="font-size:20px"}
|  CLASE | SEMANA | FECHAS | TOPIC | EJ. | WORKBOOK | ENTREGA | 
|:------:|:--------:|:--------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
| [1-2](#clase-1) | S1 | 9-12 sep | Repaso de R | [üíª](#tu-turno-1-1) [üíª](#tu-turno-1-2) [üíª](#tu-turno-1-3) [üíª](#tu-turno-1-4) [üíª](#tu-turno-1-5) | [üê£](#caso-practico-1-1) [üê£](#caso-practico-1-2) [üê£](#caso-practico-1-3) |  | 
| [3](#clase-2) | S1 | 12 sep | Repaso de estad√≠stica | [üíª](#tu-turno-2-1) [üíª](#tu-turno-2-2) |  [üê£](#caso-practico-2) |  |
:::
:::

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: diapositivas en `Quarto` disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia/time-series>**. En el men√∫ de las diapositivas (abajo a la izquierda) tienes una [**opci√≥n para descargarlas en pdf**]{.hl-yellow} en `Tools`
  
&nbsp;

* [**Material**]{.hl-yellow}: [**cuadernos de trabajo**](https://javieralvarezliebana.es/docencia/time-series/material/) y materiales extras y **res√∫menes de paquetes**

* [üóÉ **Datos**]{.hl-yellow}: datasets que usaremos a lo largo de la asignatura, disponibles en **<https://javieralvarezliebana.es/docencia/time-series/material>**

* [üìö **Recursos de apoyo**]{.hl-yellow}: en ingl√©s <https://r4ds.had.co.nz/> y en castellano <https://cdr-book.github.io/>, y <https://ivelasq.quarto.pub/intro-to-quarto/> para Quarto.

---

## Datasets

* [üóÉ **Datos**]{.hl-yellow}: datasets que usaremos a lo largo de la asignatura, disponibles en **<https://javieralvarezliebana.es/docencia/time-series/material>**

&nbsp;

* `airquality` del paquete `{datasets}` (ya instalado por defecto):
medidas diarias (153 observaciones) de la calidad del aire en Nueva York, de mayo a septiembre de 1973. Se midieron **6 variables**: ozono, radiaci√≥n solar, viento, temperatura, mes y d√≠a.


# Clases 1 y 2: [objetivos y repaso R]{.flow} {#clase-1}



[**Objetivos. Repaso de R**]{style="color:#444442;"}


* [üíª Ejercicios resueltos: repaso R](#tu-turno-1-1). [üíª Ejercicios resueltos: manejo de tibble](#tu-turno-1-2)
* [üê£ Workbook/caso pr√°ctico I](#caso-practico-1-1)
* [üíª Ejercicios resueltos: if-else y bucles](#tu-turno-1-3)
* [üê£ Workbook/caso pr√°ctico II](#caso-practico-1-2)
* [üíª Ejercicios resueltos: funciones](#tu-turno-1-4). [üíª Ejercicios resueltos: tidy data](#tu-turno-1-5)
* [üê£ Workbook/caso pr√°ctico III](#caso-practico-1-3)
* [üìÜ Planificaci√≥n](#planificacion)

---

## ¬øQu√© es una serie temporal?

Durante la carrera es probable que hayas tratado con multitud de datos pero hay uno muy especial que trataremos en esta asignatura de manera diferente: las [**series temporales**]{.hl-yellow}.

. . .

Vamos a cargar el fichero `retiro_temp.csv` donde tenemos los datos de temperaturas diarios (AEMET) desde 1980 hasta 2024 de la estaci√≥n instalada en El Retiro (Madrid).


```{r}
#| code-fold: true
library(readr) # de tidyverse
# en tidyverse, read_ en lugar de read.
# tendremos datos en formato tibble en lugar de data.frame
retiro <- read_csv(file = "./datos/retiro_temp.csv")
retiro
```


---

## ¬øQu√© es una serie temporal?


¬øQu√© analizar de estos datos?

. . .

:::: columns
::: {.column width="20%"}

Podemos por ejemplo visualizar un [**boxplot de las temperaturas medias de cada d√≠a**]{.hl-yellow} durante estos √∫ltimos 44 a√±os...

:::

::: {.column width="80%"}


```{r}
#| code-fold: true
library(tidyverse)
ggplot(retiro) +
  geom_boxplot(aes(y = tmed)) +
  scale_y_continuous(labels =
                       scales::label_number(suffix = "¬∫C")) +
  theme_minimal() +
  labs(title = "Temperatura desde 1980 hasta 2024",
       x = "Cuatrimestre", y = "Temperatura media diaria")
```


:::
::::

---

## ¬øQu√© es una serie temporal?

:::: columns
::: {.column width="20%"}

... la densidad de la temperatura durante todo ese tiempo...

:::

::: {.column width="80%"}


```{r}
#| code-fold: true
ggplot(retiro) +
  geom_density(aes(x = tmed)) +
  scale_x_continuous(labels =
                       scales::label_number(suffix = "¬∫C")) +
  theme_minimal() +
  labs(title = "Temperatura desde 1980 hasta 2024",
       x = "Temperatura media diaria")
```


:::
::::

---

## ¬øQu√© es una serie temporal?

:::: columns
::: {.column width="20%"}

... pero tambi√©n podr√≠amos querer [**relacionar la temperatura media con el mes**]{.hl-yellow} (por ejemplo con una regresi√≥n)...

:::

::: {.column width="80%"}


```{r}
#| code-fold: true
ggplot(retiro |> 
         mutate(mes = as_factor(lubridate::month(fecha))) |> 
         summarise(mean_temp = mean(tmed, na.rm = TRUE),
                   .by = "mes")) +
  geom_col(aes(x = mes, y = mean_temp)) +
  theme_minimal() +
  labs(title = "Temperatura media por mes",
       x = "Mes", y = "¬∫C (media)")
```


:::
::::

---

## ¬øQu√© es una serie temporal?

:::: columns
::: {.column width="20%"}

... o analizar c√≥mo la [**temperatura media va increment√°ndose en cada d√©cada**]{.hl-yellow}...

:::

::: {.column width="80%"}


```{r}
#| code-fold: true
ggplot(retiro |> 
         mutate(periodo =
                  if_else(fecha < as_date("1990-01-01"),
                          "1980-1990",
                          if_else(fecha < as_date("2000-01-01"),
                                  "1990-2000",
                                  if_else(fecha < as_date("2010-01-01"),
                                          "2000-2010",
                                          if_else(fecha < as_date("2020-01-01"),
                                          "2010-2020", "despu√©s de 2020")))))) +
  geom_boxplot(aes(x = periodo, y = tmed)) +
  theme_minimal() +
  labs(title = "Temperatura media seg√∫n periodo",
       x = "periodo", y = "¬∫C (media)")
```


:::
::::

---

## ¬øQu√© es una serie temporal?

En todos ejemplos anteriores hemos analizado una [**variable continua (temperatura)**]{.hl-yellow} en funci√≥n de una [**variable discreta o de grupo**]{.hl-purple} (periodo, d√©cada, etc). 

&nbsp;

. . .

¬øPero y si queremos relacionarla con una [**variable temporal "continua"**]{.hl-yellow} como es la propia fecha?

---

## ¬øQu√© es una serie temporal?


```{r}
#| code-fold: true
ggplot(retiro) +
  geom_line(aes(x = fecha, y = tmed), linewidth = 0.3, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## ¬øQu√© es una serie temporal?


F√≠jate bien...¬øqu√© elementos detectas?


```{r}
#| code-fold: true
ggplot(retiro) +
  geom_line(aes(x = fecha, y = tmed), linewidth = 0.3, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## ¬øQu√© es una serie temporal?

* [**Tendencia**]{.hl-yellow}: lo que ajustar√≠as con un modelo cl√°sico (por ejemplo, una regresi√≥n lineal) y representa el [**comportamiento global de la serie**]{.hl-purple}, algo as√≠ como un **nivel base respecto al que la serie oscila**.

(en nuestro caso: la temperatura global aumenta con el paso de los a√±os)


```{r}
#| code-fold: true
ggplot(retiro, aes(x = fecha, y = tmed)) +
  geom_line(linewidth = 0.3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## ¬øQu√© es una serie temporal?

* [**Estacionalidad**]{.hl-yellow}: al margen de esa tendencia general, si hacemos zoom, en muchas series podemos observar un [**patr√≥n que se repite cada x unidades temporales**]{.hl-purple}. En el caso de la temperatura, hay un patr√≥n anual: diciembre hace m√°s fr√≠o que en agosto.


```{r}
#| code-fold: true
ggplot(retiro |> 
         filter(between(fecha, as_date("2020-01-01"), as_date("2023-12-31"))),
                aes(x = fecha, y = tmed)) +
  geom_line(linewidth = 0.3, alpha = 0.7) +
  geom_smooth(method = "loess") +
  theme_minimal() +
  labs(title = "Temperatura media diaria de 2020 a 2023",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## ¬øQu√© es una serie temporal?

* [**At√≠picos**]{.hl-yellow}: como sucede siempre en estad√≠stica ser√° important√≠simo analizar y tratar los [**datos at√≠picos muy alejados de lo esperado**]{.hl-purple}. Por ejemplo, en nuestro caso, Filomena.


```{r}
#| code-fold: true
ggplot(retiro |> 
         filter(between(fecha, as_date("2020-01-01"), as_date("2023-12-31"))) |>
         mutate(filomena = between(fecha, as_date("2020-12-25"), as_date("2021-01-22"))),
                aes(x = fecha, y = tmed)) +
  geom_line(linewidth = 0.3, alpha = 0.7) +
  geom_point(aes(alpha = filomena), color = "#991545") +
  scale_alpha_manual(values = c(0, 1)) +
  guides(alpha = "none") +
  theme_minimal() +
  labs(title = "Temperatura media diaria de 2020 a 2023",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## ¬øQu√© es una serie temporal?

* [**Intervenciones**]{.hl-yellow}: incluso podr√≠a suceder que la serie tuviese un [**corte o salto en su comportamiento**]{.hl-purple}. Por ejemplo, imagina que de repente el aparato de medici√≥n empieza a medir +25 grados de la temperatura real.


```{r}
#| code-fold: true
ggplot(retiro |> 
         filter(between(fecha, as_date("2020-01-01"), as_date("2023-12-31"))) |>
         mutate(tmed = if_else(fecha <= "2021-12-31", tmed, tmed + 25))) +
  geom_line(aes(x = fecha, y = tmed), linewidth = 0.3, alpha = 0.7) +
  guides(alpha = "none") +
  theme_minimal() +
  labs(title = "Temperatura media diaria de 2020 a 2023",
       subtitle = "Error de +25¬∫C a partir de 2022",
       x = "t (fecha)", y = "¬∫C (media)")
```



---

## Ejemplos de series

En esta asignatura ser√° fundamental un concepto: [**estacionariedad**]{.hl-yellow}. Diremos que una [**serie es estacionaria si oscila de manera estable con una media y varianza constante**]{.hl-yellow}.

![](https://estrategiastrading.com/wp-content/plugins/phastpress/phast.php/c2VydmljZT1pbWFnZXMmc3JjPWh0dHBzJTNBJTJGJTJGZXN0cmF0ZWdpYXN0cmFkaW5nLmNvbSUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAxNiUyRjEyJTJGbWVkaWFfZXN0YWNpb25hcmlhLnBuZyZjYWNoZU1hcmtlcj0xNjMyOTAyOTc5LTc5NjQmdG9rZW49M2Y0ZTg4NWMyZjM0NmE2MA.q.png)

![](https://estrategiastrading.com/wp-content/plugins/phastpress/phast.php/c2VydmljZT1pbWFnZXMmc3JjPWh0dHBzJTNBJTJGJTJGZXN0cmF0ZWdpYXN0cmFkaW5nLmNvbSUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAxNiUyRjEyJTJGaG9tb3NjZWRhc3RpY2lkYWQucG5nJmNhY2hlTWFya2VyPTE2MzI5MDI5NzktOTA0MyZ0b2tlbj1mNjhiMDIwYjg2NGE1M2Rl.q.png)

---

## Distintos objetivos

- [**An√°lisis descriptivo**]{.hl-yellow}
  - Visualizaci√≥n: ¬øc√≥mo son los datos? ¬øExiste alg√∫n ausente o valor at√≠pico?
  - ¬øSe puede descomponer la serie en series m√°s sencillas?
  
. . .

- [**An√°lisis probabil√≠stico**]{.hl-yellow}:
  - ¬øExiste un modelo te√≥rico tal que lo que observamos sea simplemente una muestra dicho modelo probabil√≠stico?
  - Aunque los datos sean aleatorios, ¬øpodemos modelizar de manera te√≥rica alguna de sus caracter√≠stica?

. . .
 
- [**Predicci√≥n**]{.hl-yellow}
  - Conociendo su comportamiento pasado, ¬øcu√°nto valdr√° su valor ma√±ana?
  - ¬øCu√°nto me estoy equivocando? ¬øC√≥mo medir ese error?

---

## Bloques del curso

- [**Bloque I**]{.hl-yellow}: analisis exploratorio. [**Descomposici√≥n y suavizado**]{.hl-purple}

. . .

- [**Bloque II**]{.hl-yellow}: ¬øqu√© son los [**prcesos estoc√°sticos**]{.hl-purple}?

. . .

- [**Bloque III**]{.hl-yellow}: metodolog√≠a [**Box-Jenkins**]{.hl-purple}

 . . .
 
- [**Bloque IV**]{.hl-yellow}: problemas ([**intervenci√≥n, at√≠picos**]{.hl-purple}, ausentes, heterocedasticidad, ...


---

## Instalaci√≥n de R

El lenguaje `R` ser√° nuestra [**gram√°tica y ortograf√≠a**]{.hl-yellow} (nuestras reglas de juego)

::: incremental
-   [**Paso 1**]{.hl-yellow}: entra en <https://cran.r-project.org/> y selecciona tu sistema operativo.

-   [**Paso 2**]{.hl-yellow}: para Mac basta con que hacer click en el **archivo .pkg**, y abrirlo una vez descargado. Para sistemas Windows, debemos clickar en **install R for the first time** y despu√©s en **Download R for Windows**. Una vez descargado, abrirlo como cualquier archivo de instalaci√≥n.

-   [**Paso 3**]{.hl-yellow}: abrir el ejecutable de instalaci√≥n.
:::

. . .

::: callout-warning
Siempre que tengas que descargar algo de CRAN (ya sea el propio R o un paquete), [**aseg√∫rate de tener conexi√≥n a internet**]{.hl-orange}.
:::


---

## Instalaci√≥n de R Studio

`RStudio` ser√° el [**Word**]{.hl-yellow} que usaremos para escribir (lo que se conoce como un [**IDE: entorno integrado de desarrollo**]{.hl-yellow}).

::: incremental
-   [**Paso 1**]{.hl-yellow}: entra la [web oficial de RStudio](https://posit.co/download/rstudio-desktop/) (ahora llamado Posit) y selecciona la descarga gratuita.

-   [**Paso 2**]{.hl-yellow}: selecciona el ejecutable que te aparezca acorde a tu sistema operativo.

-   [**Paso 3**]{.hl-yellow}: tras descargar el ejecutable, hay que abrirlo como otro cualquier otro y dejar que termine la instalaci√≥n.
:::


---

## Scripts (documentos .R)


::: columns
::: {.column width="35%"}
![](img/abrir_script.jpg){width="350"}
:::

::: {.column width="65%"}
Un [**script**]{.hl-yellow} ser√° el documento en el que programamos, nuestro archivo `.doc` (aqu√≠ con extensi√≥n `.R`) donde escribiremos las √≥rdenes. Para **abrir nuestro primero script**, haz click en el men√∫ en `File < New File < R Script`.

::: callout-warning
## Cuidado

Es importante **no abusar de la consola**: todo lo que no escribas en un script, cuando cierres, [**lo habr√°s perdido**]{.hl-orange}.
:::

:::
:::



::: callout-warning
## Cuidado

`R` es [**case-sensitive**]{.hl-orange}: es sensible a may√∫sculas y min√∫sculas por lo que `x` y `X` representa variables distintas.
:::


---

## Ejecutando el primer script

Ahora tenemos una **cuarta ventana**: la ventana donde [**escribiremos nuestros c√≥digos**]{.hl-yellow}. ¬øC√≥mo ejecutarlo?

. . .

1.  **Escribimos** el c√≥digo a ejecutar.

. . .

2.  **Guardamos** el archivo .R haciendo click en `Save current document`.

. . .

3.  El c√≥digo no se ejecuta salvo que se lo indiquemos. Tenemos **tres opciones de ejecutar un script**:

-   [**Copiar y pegar**]{.hl-yellow} en consola.
-   [**Seleccionar l√≠neas**]{.hl-yellow} y `Ctrl+Enter`
-   [**Activar Source on save**]{.hl-yellow} a la derecha de guardar: no solo guarda sino que ejecuta el c√≥digo **completo**.

---


## S√© organizado: proyectos

De la misma manera que en el ordenador solemos trabajar de manera [**ordenada por carpetas**]{.hl-yellow}, en `RStudio` podemos hacer lo mismo para trabajar de [**manera eficaz creando proyectos**]{.hl-yellow}.

. . .

::: columns
::: {.column width="60%"}
Un [**proyecto ser√° una ¬´carpeta¬ª**]{.hl-yellow} dentro de `RStudio`, de manera que nuestro directorio ra√≠z autom√°ticamente ser√° la propia carpeta de proyecto (pudiendo pasar de un proyecto a otro con el menu superior derecho).

Podemos crear uno en una carpeta nueva o en una carpeta ya existente.
:::

::: {.column width="40%"}
![](img/rstudio_proyectos.png){width="370"}
:::
:::


---

## Buenas pr√°cticas


* [**Tip 1**]{.hl-green}: [**asignar, evaluar y comparar no es lo mismo**]{.hl-yellow}. Si te has fijado en `R` estamos usando `<-` para asignar valores a variables. Usaremos `=` para evaluar argumentos en funciones y `==` para saber si dos elementos son iguales.


```{r}
#| eval: false
x <- 1 # asignar
x = 1 # evaluar
x == 1 # comparar
```


. . .

* [**Tip 2**]{.hl-green}: programa como escribes. Al igual que cuando redactas en castellano, acost√∫mbrate a incorporar [**espacios y saltos de l√≠nea**]{.hl-yellow} paranoquedarteciego (es una buena pr√°ctica y no un requisito porque `R` no procesa los espacios)


```{r}
#| eval: false
x <- 1 # √≥ptimo
x<-1 # regu
x<- 1 # peor (dec√≠dete)
```


---

## Buenas pr√°cticas


* [**Tip 3**]{.hl-green}: no seas ca√≥tico, [**estandariza nombres**]{.hl-yellow}, acost√∫mbrate siempre a hacerlo igual. El √∫nico requisito es que **debe empezar siempre por una letra** (y sin tildes). La forma m√°s recomendable es la conocida como `snake_case`


```{r}
#| eval: false
variable_en_modo_snake_case
otraFormaMasDificilDeLeer
hay.gente.que.usa.esto
Incluso_Haygente.Caotica_que.NoMereceNuestraATENCION
```


. . .

* [**Tip 4**]{.hl-green}: facilita la lectura y escritura, [**pon m√°rgenes**]{.hl-yellow}. En `Tools < Global Options` puedes personalizar algunas opciones de `RStudio`. En `Code < Display` podemos indicarle en `Show margin`  (no interacciona con el c√≥digo).

![](img/show_margin.jpg){height="200"}


---

## Buenas pr√°cticas

* [**Tip 5**]{.hl-green}: el [**tabulador es tu mejor amigo**]{.hl-yellow}. En `RStudio` tenemos una herramienta maravillosa: si escribes parte del nombre de una variable o funci√≥n y tabulas, `RStudio` te autocompleta

![](img/tab-autocompletar.png)  


---

## Buenas pr√°cticas

* [**Tip 6**]{.hl-green}: ni un par√©ntesis soltero. Siempre que abras un par√©ntesis deber√°s cerrarlo. Para facilitar esta tarea entra en `Tools < Global Options < Code < Display` y activa la opci√≥n `Rainbow parentheses`

![](img/rainbow-parentheses.jpg)



---

## Buenas pr√°cticas

* [**Tip 7**]{.hl-green}: f√≠jate en el lateral izquierdo. No solo podr√°s ver la l√≠nea de c√≥digo por la que vas sino que, en caso de estar cometiendo un [**error de sintaxis**]{.hl-red}, el propio `RStudio` te avisar√°.

![](img/rstudio-error-sintaxis.png)

* [**Tip 8**]{.hl-green}: intenta [**trabajar siempre por proyectos**]{.hl-yellow} (para esta clase, crea un script `clase2.R` en el proyecto que creamos en la anterior clase)

&nbsp;

Ver m√°s tips en <https://r4ds.had.co.nz/workflow-basics.html#whats-in-a-name>

---

## Tipos de datos

¬øExisten [**variables m√°s all√° de los n√∫meros en la ciencia de datos**]{.hl-yellow}? Piensa por ejemplo en los datos que podr√≠as guardar de una persona:

::: {.fragment .fade-up}
-   La edad o el peso ser√° un [**n√∫mero**]{.hl-yellow}.


```{r}
edad <- 33
```

:::

::: {.fragment .fade-up}
-   Su nombre ser√° una cadena de [**texto (conocida como string o char)**]{.hl-yellow}.


```{r}
nombre <- "javi"
```

:::

::: {.fragment .fade-up}
-   A la pregunta ¬´¬øest√°s matriculado en la Facultad?¬ª la respuesta ser√° lo que llamamos una [**variable l√≥gica**]{.hl-yellow} (`TRUE` si est√° matriculado o `FALSE` en otro caso).


```{r}
matriculado <- TRUE
```

:::

::: {.fragment .fade-up}
-   Su fecha de nacimiento ser√° precisamente eso, una [**fecha**]{.hl-yellow}, un tipo de variable **crucial en esta asignatura**
:::

---


## Variables de fecha

Un tipo de datos muy especial: los [**datos de tipo fecha**]{.hl-yellow}.


```{r}
fecha_char <- "2021-04-21"
```


Parece una simple cadena de texto pero [**deber√≠a representar un instante en el tiempo**]{.hl-yellow}. ¬øQu√© deber√≠a suceder si [**sumamos un 1 a una fecha**]{.hl-purple}?

. . .


```{r}
#| error: true
fecha_char + 1
```



Las fechas [**NO pueden ser texto**]{.hl-red}: debemos convertir la cadena de texto a fecha.

. . .

&nbsp;

Para trabajar con fechas usaremos el paquete `{lubridate}`, que deberemos instalar antes de poder usarlo.


```{r}
#| eval: false
install.packages("lubridate")
```


---

## Variables de fecha

Una vez instalado, de todos los paquetes (libros) que tenemos, le indicaremos que nos cargue ese concretamente.


```{r}
library(lubridate) # instala si no lo has hecho
```


. . .

Para [**convertir a tipo fecha**]{.hl-yellow} usaremos la funci√≥n `as_date()` del paquete `{lubridate}` (por defecto en formato `yyyy-mm-dd`)

&nbsp;

:::: columns
::: {.column width="50%"}


```{r}
#| error: true
# ¬°no es una fecha, es un texto!
fecha_char + 1
class(fecha_char)
```


:::

::: {.column width="50%"}


```{r}
fecha <- as_date("2023-03-28")
fecha + 1
class(fecha)
```


:::

::::

---

## Variables de fecha

En `as_date()` el formato de fecha por defecto es `yyyy-mm-dd` as√≠ si la cadena de texto no se introduce de manera adecuada...


```{r}
as_date("28-03-2023")
```


. . .

Para [**cualquier otro formato debemos especificarlo**]{.hl-yellow} en el argumento opcional `format = ...` tal que `%d` representa d√≠as, `%m` meses, `%Y` en formato de 4 a√±os y `%y` en formato de 2 a√±os.


```{r}
as_date("28-03-2023", format = "%d-%m-%Y")
as_date("28-03-23", format = "%d-%m-%y")
as_date("03-28-2023", format = "%m-%d-%Y")
as_date("28/03/2023", format = "%d/%m/%Y")
```



---

## Variables de fecha

En dicho paquete tenemos funciones muy √∫tiles para [**manejar fechas**]{.hl-yellow}:

-   Con `today()` podemos obtener directamente la [**fecha actual**]{.hl-purple}.


```{r}
today()
```


. . .

-   Con `now()` podemos obtener la [**fecha y hora actual**]{.hl-purple}


```{r}
now()
```


. . .

-   Con `year()`, `month()` o `day()` podemos [**extraer el a√±o, mes y d√≠a**]{.hl-purple}


```{r}
fecha <- today()
year(fecha)
month(fecha)
```


---

## Res√∫menes de paquetes

![](img/lubridate.png)

::: callout-note
## Amplia contenido

Tienes un resumen en pdf de los paquetes m√°s importantes en la [**carpeta correspondiente en el campus**]{.hl-green}
:::

---

## Vectores: concatenar

Cuando trabajamos con datos normalmente tendremos [**columnas que representan variables**]{.hl-yellow}: llamaremos [**vectores**]{.hl-yellow} a una [**concatenaci√≥n**]{.hl-purple} de celdas (valores) del [**mismo tipo**]{.hl-purple} (lo que ser√≠a una columna de una tabla).

. . .

La forma m√°s sencilla es con el comando `c()` (c de **concatenar**), y basta con introducir sus **elementos entre par√©ntesis y separados por comas**


```{r}
edades <- c(32, 27, 60, 61)
edades
```


. . .

::: callout-tip
Un n√∫mero individual `x <- 1` (o bien `x <- c(1)`) es en realidad un **vector de longitud uno** --> todo lo que sepamos [**hacer con un n√∫mero podemos hacerlo con un vector de ellos**]{.hl-green}.
:::


---

## üíª Tu turno {#tu-turno-1-1}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset
### [**Ejercicio 1**]{.hl-yellow}

üìù Define el vector `x` como la concatenaci√≥n de los 5 primeros n√∫meros impares. Calcula la longitud del vector


```{r}
#| code-fold: true
#| eval: false
# Dos formas
x <- c(1, 3, 5, 7, 9)
x <- seq(1, 9, by = 2)

length(x)
```


### [**Ejercicio 2**]{.hl-yellow}

üìù Accede al tercer elemento de `x`. Accede al √∫ltimo elemento (sin importar la longitud, un c√≥digo que pueda ejecutarse siempre). Elimina el primer elemento.


```{r}
#| code-fold: true
#| eval: false
x[3]
x[length(x)]
x[-1]
```


### [**Ejercicio 3**]{.hl-yellow}

üìù Obt√©n los elementos de `x` mayores que 4. Calcula el vector `1/x` y gu√°rdalo en una variable.


```{r}
#| code-fold: true
#| eval: false
x[x > 4]
z <- 1/x
z
```


### [**Ejercicio 4**]{.hl-yellow}

üìù Crea un vector que represente los nombres de 5 personas, de los cuales uno es desconocido.


```{r}
#| code-fold: true
#| eval: false
nombres <- c("Javi", "Sandra", NA, "Laura", "Carlos")
nombres
```


### [**Ejercicio 5**]{.hl-yellow}

üìù Encuentra del vector `x` de ejercicios anteriores los elementos mayores (estrictos) que 1 Y ADEM√ÅS menores (estrictos) que 7. Encuentra una forma de averiguar si todos los elementos son o no positivos.


```{r}
#| code-fold: true
#| eval: false
x[x > 1 & x < 7]
all(x > 0)
```


### [**Ejercicio 6**]{.hl-yellow}

üìù Dado el vector `x <- c(1, -5, 8, NA, 10, -3, 9)`,  ¬øpor qu√© su media no devuelve un n√∫mero sino lo que se muestra en el c√≥digo inferior?


```{r}
x <- c(1, -5, 8, NA, 10, -3, 9)
mean(x)
```


### [**Ejercicio 7**]{.hl-yellow}

üìù Dado el vector `x <- c(1, -5, 8, NA, 10, -3, 9)`, extrae los elementos que ocupan los lugares 1, 2, 5, 6. 


```{r}
#| code-fold: true
#| eval: false
x <- c(1, -5, 8, NA, 10, -3, 9)
x[c(1, 2, 5, 6)]
x[-2]
```


### [**Ejercicio 8**]{.hl-yellow}

üìù Dado el vector `x` del ejercicio anterior, ¬øcuales tienen un dato ausente? Pista: las funciones `is.algo()` comprueban si el elemento es tipo `algo` (tabula)


```{r}
#| code-fold: true
#| eval: false
is.na(x)
```



### [**Ejercicio 9**]{.hl-yellow}

üìù Define el vector `x` como la concatenaci√≥n de los 4 primeros n√∫meros pares. Calcula el n√∫mero de elementos de `x` menores estrictamente que 5.


```{r}
#| code-fold: true
#| eval: false
x[x < 5] 
sum(x < 5)
```



### [**Ejercicio 10**]{.hl-yellow}

üìù Calcula el vector `1/x` y obt√©n la versi√≥n ordenada (de menor a mayor) de las dos formas posibles


```{r}
#| code-fold: true
#| eval: false
z <- 1/x
sort(z)
z[order(z)]
```



### [**Ejercicio 11**]{.hl-yellow}

üìù Encuentra del vector `x` los elementos mayores (estrictos) que 1 y menores (estrictos) que 6. Encuentra una forma de averiguar si todos los elementos son o no negativos.


```{r}
#| code-fold: true
#| eval: false
x[x > 1 & x < 7]
all(x > 0)
```


:::

---

## Primera base de datos

Cuando analizamos datos solemos tener [**varias variables**]{.hl-yellow} de cada individuo: necesitamos una ¬´tabla¬ª que las recopile. La opci√≥n m√°s inmediata son las [**matrices**]{.hl-yellow}: concatenaci√≥n de variables del [**mismo tipo e igual longitud**]{.hl-purple}.

Imagina que tenemos estaturas y pesos de 4 personas. ¬øC√≥mo [**crear un dataset con las dos variables**]{.hl-yellow}?

. . .


La opci√≥n m√°s habitual es usando `cbind()`: [**concatenamos (bind) vectores en forma de columnas (c)**]{.hl-yellow}


```{r}
#| code-line-numbers: "3"
estaturas <- c(150, 160, 170, 180)
pesos <- c(63, 70, 85, 95)
datos_matriz <- cbind(estaturas, pesos)
datos_matriz
```


---

## Primer intento: matrices

Tambi√©n podemos [**construir la matriz por filas**]{.hl-yellow} con la funci√≥n `rbind()` (concatenar - bind - por filas  - rows), aunque lo [**recomendable es tener cada variable en columna**]{.hl-green} e individuo en fila como luego veremos.


```{r}
rbind(estaturas, pesos) # Construimos la matriz por filas
```


. . .

-   Podemos [**comprobar las dimensiones**]{.hl-yellow} con `dim()`, `nrow()` y `ncol()`: las matrices son un tipo de **datos tabulados** (organizados en filas y columnas)


```{r}
dim(datos_matriz)
nrow(datos_matriz)
ncol(datos_matriz)
```


---

## Segundo intento: data.frame

Las matrices tienen el mismo problema que los vectores: si juntamos datos de distinto tipo, se [**perturba la integridad del dato**]{.hl-red} ya que los convierte (f√≠jate en el c√≥digo inferior: las edades y los `TRUE/FALSE` los ha convertido a texto)


```{r}
#| code-line-numbers: "4-5"
edades <- c(14, 24, NA)
soltero <- c(TRUE, NA, FALSE)
nombres <- c("javi", "laura", "luc√≠a")
matriz <- cbind(edades, soltero, nombres)
matriz
```


. . .

De hecho al no ser n√∫meros ya no podemos realizar operaciones aritm√©ticas


```{r}
#| error: true
matriz + 1
```


---

## Segundo intento: data.frame

Para poder trabajar con [**variables de distinto tipo**]{.hl-yellow} tenemos en `R` lo que se conoce como [**data.frame**]{.hl-yellow}: concatenaci√≥n de variables de igual longitud pero que pueden ser de [**tipo distinto**]{.hl-purple}.


```{r}
tabla <- data.frame(edades, soltero, nombres)
class(tabla)
tabla
```


---

## Segundo intento: data.frame

Dado que un `data.frame` es ya un intento de ¬´base de datos¬ª las variables no son meros vectores matem√°ticos: [**tienen un significado**]{.hl-yellow} y podemos (debemos) [**ponerles nombres**]{.hl-purple} que describan su significado


```{r}
library(lubridate)
tabla <-
  data.frame("edad" = edades, "estado" = soltero, "nombre" = nombres,
             "f_nacimiento" = as_date(c("1989-09-10", "1992-04-01", "1980-11-27")))
tabla
```



---

## Intento final: tibble

Las tablas en formato `data.frame` tienen algunas [**limitaciones**]{.hl-red}. La principal es que [**no permite la recursividad**]{.hl-red}: imagina que definimos una base de datos con estaturas y pesos, y queremos una tercera variable con el IMC


```{r}
#| error: true
data.frame("estatura" = c(1.7, 1.8, 1.6), "peso" = c(80, 75, 70),
           "IMC" = peso / (estatura^2))
```


. . .


En adelante usaremos el formato `tibble` ([**data.frame mejorado**]{.hl-yellow}) del paquete `{tibble}`


```{r}
library(tibble)
datos_tb <- 
  tibble("estatura" = c(1.7, 1.8, 1.6), "peso" = c(80, 75, 70), "IMC" = peso / (estatura^2))
class(datos_tb)
datos_tb
```


---

## Intento final: tibble


```{r}
datos_tb <-
  tibble("estatura" = c(1.7, 1.8, 1.6), "peso" = c(80, 75, 70), "IMC" = peso / (estatura^2))
datos_tb
```


Las tablas en formato `tibble` nos permitir√° una [**gesti√≥n m√°s √°gil, eficiente y coherente**]{.hl-yellow} de los datos, con 4 ventajas principales:

. . .

-   [**Metainformaci√≥n**]{.hl-yellow}: si te fijas en la cabecera, nos dice ya autom√°ticamente el n√∫mero de filas y columnas, y el tipo de cada variable

. . .

-   [**Recursividad**]{.hl-yellow}: permite definir las variables secuencialmente (como hemos visto)

---

## Intento final: tibble

-   [**Consistencia**]{.hl-yellow}: si accedes a una columna que no existe avisa con un warning


```{r}
#| warning: true
datos_tb$invent
```


. . .

-   [**Por filas**]{.hl-yellow}: crear por filas (copiar y pegar de una tabla) con `tribble()`


```{r}
tribble(~colA, ~colB,
        "a",   1,
        "b",   2)
```


. . .

::: callout-tip
El paquete `{datapasta}` nos permite [**copiar y pegar**]{.hl-green} tablas de p√°ginas web y documentos sencillos
:::


---


## Recapitulando

-   Cada [**celda puede ser de un tipo diverso**]{.hl-yellow}: n√∫meros, texto, fechas, valores l√≥gicos, etc

. . .

-   Un [**vector es una concatenaci√≥n de celdas**]{.hl-yellow} (las futuras columnas de nuestras tablas) --> En `R` por defecto las operaciones se hacen [**elemento a elemento**]{.hl-yellow}

. . .

-   Una [**matriz**]{.hl-yellow} nos permite concatenar [**variables del MISMO tipo y MISMA longitud**]{.hl-yellow} --> datos tabulados

. . .

-   Un [**data.frame**]{.hl-yellow} nos permite concatenar [**variables de DISTINTO tipo y MISMA longitud**]{.hl-yellow} --> usaremos [**tibble**]{.hl-yellow} como una opci√≥n mejorada de base de datos

---

## üíª Tu turno (tb/df) {#tu-turno-1-2}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Ejercicio 1**]{.hl-yellow}

üìù Carga del paquete `{datasets}` el conjunto de datos `airquality` (variables de la calidad del aire de Nueva York desde mayo hasta septiembre de 1973). ¬øEs el conjunto de datos airquality de tipo tibble? En caso negativo, convi√©rtelo a tibble (busca en la documentaci√≥n del paquete en <https://tibble.tidyverse.org/index.html>).


```{r}
#| code-fold: true
#| eval: false
library(tibble)
class(datasets::airquality)
airquality_tb <- as_tibble(datasets::airquality)
```


### [**Ejercicio 2**]{.hl-yellow}

üìù Una vez convertido a `tibble` obt√©n el nombre de las variables y las dimensiones del conjunto de datos. ¬øCu√°ntas variables hay? ¬øCu√°ntos d√≠as se han medido?


```{r}
#| code-fold: true
#| eval: false
names(airquality_tb)
ncol(airquality_tb)
nrow(airquality_tb)
```



### [**Ejercicio 3**]{.hl-yellow}

üìù Filtra solo los datos de la quinta observaci√≥n


```{r}
#| code-fold: true
#| eval: false
airquality_tb[Month == 8, ]
```


### [**Ejercicio 4**]{.hl-yellow}

üìù Filtra solo los datos del mes de agosto. ¬øC√≥mo indicarle que queremos solo las filas que cumplan una condici√≥n concreta? (pista: en realidad todo son vectores "formateados")


```{r}
#| code-fold: true
#| eval: false
airquality_tb[Month == 8, ]
```


### [**Ejercicio 5**]{.hl-yellow}

üìù Selecciona aquellos datos que no sean ni de julio ni de agosto.


```{r}
#| code-fold: true
#| eval: false
airquality_tb[Month != 7 & Month != 8, ]
airquality_tb[!(Month %in% c(7, 8)), ]
```


### [**Ejercicio 6**]{.hl-yellow}

üìù Modifica el siguiente c√≥digo para quedarte solo con las variable de ozono y temperatura (sin importar qu√© posici√≥n ocupen)


```{r}
#| eval: false
airquality_tb[, 3]
```


### [**Ejercicio 7**]{.hl-yellow}

üìù Selecciona los datos de temperatura y viento de agosto. 


```{r}
#| code-fold: true
#| eval: false
airquality_tb[Month == 8, c("Temp", "Wind")]
```


### [**Ejercicio 8**]{.hl-yellow}

üìù Traduce a castellano el nombre de las variables.



```{r}
#| code-fold: true
#| eval: false
names(airquality_tb) <- c("ozono", "rad_solar", "viento", "temp", "mes", "dia") 
```


:::




---

## üê£ Caso pr√°ctico I: tibble {#caso-practico-1-1}

Del paquete `{Biostatistics}` usaremos el conunto de datos `pinniped`, que guarda los **datos de peso de cuerpo y cerebro** (desagregado por sexo y mono/poligamia) de 33 especies de mam√≠feros marinos.



```{r}
Biostatistics::pinniped
```



Intenta responder a las preguntas planteadas en el [**workbook**](https://javieralvarezliebana.quarto.pub/repaso-r-ts/)



---

## Comunicar: rmd y Quarto

Una de las [**principales fortalezas**]{.hl-yellow} de `R` es la [**facilidad para generar informes, libros, webs, apuntes y hasta diapositivas**]{.hl-yellow} (este mismo material por ejemplo). Para ello [**instalaremos**]{.hl-purple} antes

::: columns
::: {.column width="40%"}
-   el paquete `{rmarkdown}` (para generar archivos `.rmd`)


```{r}
#| eval: false
install.packages("rmarkdown")
```


- instalar [**Quarto**](https://quarto.org/docs/get-started/) (si ya conoc√≠as `R`, el ¬´nuevo¬ª `.rmd` ahora como `.qmd`)
:::

::: {.column width="60%"}
![](img/quarto.png)
:::
:::

---

## Comunicar: rmd y Quarto

Hasta ahora solo hemos programado en scripts (archivos `.R`) dentro de proyectos, pero en muchas ocasiones [**no trabajaremos solos**]{.hl-yellow} y necesitaremos [**comunicar los resultados**]{.hl-yellow} en diferentes formatos:

- apuntes (para nosotros mismos)
- diapositivas
- web
- informes


Para todo ello usaremos [**Quarto**]{.hl-yellow} (ver m√°s en <https://ivelasq.quarto.pub/intro-to-quarto/>)


---

## Comunicar: rmd y Quarto

Los archivos de extensi√≥n `.qmd` (o `.rmd` antes) nos permitir√°n f√°cilmente combinar:

-   [**Markdown**]{.hl-yellow}: [**lenguaje tipado**]{.hl-purple} que nos permite crear contenido simple (tipo wordpress, con texto, **negritas**, _cursivas_, etc) con un dise√±o legible.

. . .

-   [**Matem√°ticas (latex)**]{.hl-yellow}: lenguaje para escribir notaci√≥n matem√°tica como $x^2$ o $\sqrt{y}$ o $\int_{a}^{b} f(x) dx$

. . .

-   [**C√≥digo y salidas**]{.hl-yellow}: podremos no solo mostrar el paso final sino el c√≥digo que has ido realizando (en `R`, `Python`, `C++`, `Julia`, ...), con [**cajitas de c√≥digo llamadas CHUNKS**]{.hl-purple}.

. . .

-   Im√°genes, [**gr√°ficas**]{.hl-yellow}, tablas, estilos (css, js), etc.

---

## Comunicar: rmd y Quarto

La principal ventaja de realizar este tipo de material en Quarto/Rmarkdown es que, al hacerlo desde `RStudio`, puedes generar un [**informe o una presentaci√≥n sin salirte del entorno de programaci√≥n**]{.hl-yellow} en el que est√°s trabajando

De esta forma podr√°s analizar los datos, resumirlos y a la vez comunicarlos con la misma herramienta.

. . .

Recientemente el equipo de `RStudio` desarroll√≥ [**Quarto**]{.hl-yellow}, una versi√≥n mejorada de Rmarkdown (archivos `.qmd`), con un formato un poco m√°s est√©tico y simple. Tienes toda la documentaci√≥n y ejemplos en [**https://quarto.org/**](https://quarto.org/)

---

## Usos de Quarto

::: panel-tabset
### Webs

![](./img/website.png){height="350"}

### Libros

![](./img/book.png){height="350"}

### Blogs

![](./img/blog.png){height="350"}

### Presentaciones

![](./img/presentation.png){height="350"}

### Revistas

![](https://user-images.githubusercontent.com/163582/42351114-e5deaa1c-8078-11e8-90de-2aff57bba255.png){height="350"}
:::

Im√°genes obtenidas de <https://ivelasq.quarto.pub/intro-to-quarto/#/working-with-the-rstudio-visual-editor>

---

## Nuestro primer informe

::: columns
::: {.column width="55%"}
![](img/quarto-create.png)
:::

::: {.column width="45%"}
Vamos a crear el [**primer fichero rmarkdown con Quarto**]{.hl-yellow} con extensi√≥n `.qmd`. Para ello solo necesitaremos hacer click en

`File << New File << Quarto Document`
:::
:::

---

## Nuestro primer informe

:::: columns
::: {.column width="45%"}
![](img/quarto-format.png)
:::

::: {.column width="55%"}
Tras hacerlo nos aparecer√°n varias [**opciones de formatos de salida**]{.hl-yellow}:

-   archivo `.pdf`
-   archivo `.html` ([**recomendable**]{.hl-yellow}): documento din√°mico, permite la interacci√≥n con el usuario, como una ¬´p√°gina web¬ª.
-   archivo `.doc` (nada recomendable)
:::
::::

. . .

De momento dejaremos marcado el [**formato HTML que viene por defecto**]{.hl-yellow}, y escribiremos el [**t√≠tulo**]{.hl-yellow} de nuestro documento. Tras ello tendremos nuestro [**archivo .qmd**]{.hl-yellow} (ya no es un script .R como los que hemos abierto hasta ahora).

---

## Nuestro primer informe

:::: columns
::: {.column width="60%"}
![](img/quarto-example.png)
:::

::: {.column width="40%"}

Deber√≠as tener algo similar a la captura de la imagen con [**dos modos de edici√≥n**]{.hl-yellow}: `Source` (con c√≥digo, la opci√≥n recomendada hasta que lo domines) y `Visual` (m√°s parecido a un blog)

:::
::::

Para [**ejecutar TODO el documento**]{.hl-yellow} debes clickar `Render on Save` y darle a guardar.

---

## Salida de Quarto

![](img/quarto-prueba-html.png)

Deber√≠as haber obtenido una [**salida en html similar a esta**]{.hl-yellow} (y se te ha generado en tu ordenador un [**archivo html**]{.hl-yellow})

---

## Editor: source vs visual

Como se indicaba, tienes dos formas de trabajar: con c√≥digo puro y algo parecido a un Notion (blog)

![](./img/rstudio-source-visual.png)

Imagen obtenida de <https://ivelasq.quarto.pub/intro-to-quarto/#/working-with-the-rstudio-visual-editor>

---

## Nuestro primer informe

:::: columns
::: {.column width="50%"}
![](img/quarto-example.png)

:::

::: {.column width="50%"}

Un fichero `.qmd` se [**divide b√°sicamente en tres partes**]{.hl-yellow}:

* [**Cabecera**]{.hl-yellow}: la parte que tienes al inicio entre `---`.

* [**Texto**]{.hl-yellow}: que podremos formatear y mejorar con negritas (escrito como **negritas**, con doble ast√©risco al inicio y final), cursivas (_cursivas_, con barra baja al inicio y final) o destacar nombres de funciones o variables de R. Puedes a√±adir ecuaciones como $x^2$ (he escrito `$x^2$`, entre d√≥lares).

* [**C√≥digo R**]{.hl-yellow}

:::
::::

---

## Cabecera de un qmd {auto-animate="true"}

La [**cabecera est√°n en formato YAML**]{.hl-yellow} y contiene los [**metadatos**]{.hl-yellow} del documento

:::: columns
::: {.column width="60%"}

* `title` y `subtitle`: el t√≠tulo/subt√≠tulo del documento
* `author`: autor del mismo
* `format`: formato de salida (podremos personalizar)
  * `theme`: si tienes alg√∫n archivo de estilos
  * `toc`: si quieres √≠ndice o no
  * `toc-location`: posici√≥n del √≠ndice
  * `toc-title`: t√≠tulo del √≠ndice
* `editor`: si est√°s en modo visual o source.

:::

::: {.column width="40%"}

``` yaml
---
title: "prueba"
format:
  html:
editor: visual
---
```

:::
::::

---

## Cabecera de un qmd {auto-animate="true"}

La [**cabecera est√°n en formato YAML**]{.hl-yellow} y contiene los [**metadatos**]{.hl-yellow} del documento

:::: columns
::: {.column width="60%"}

* `title` y `subtitle`: el t√≠tulo/subt√≠tulo del documento
* `author`: autor del mismo
* `format`: formato de salida (podremos personalizar)
  * `theme`: si tienes alg√∫n archivo de estilos
  * `toc`: si quieres √≠ndice o no
  * `toc-location`: posici√≥n del √≠ndice
  * `toc-title`: t√≠tulo del √≠ndice
* `editor`: si est√°s en modo visual o source.

:::

::: {.column width="40%"}

``` yaml
---
title: "prueba"
author: "javier √°lvarez li√©bana"
format:
  html:
editor: visual
---
```

:::
::::

---

## Cabecera de un qmd {auto-animate="true"}

La [**cabecera est√°n en formato YAML**]{.hl-yellow} y contiene los [**metadatos**]{.hl-yellow} del documento

:::: columns
::: {.column width="60%"}

* `title` y `subtitle`: el t√≠tulo/subt√≠tulo del documento
* `author`: autor del mismo
* `format`: formato de salida (podremos personalizar)
  * `theme`: si tienes alg√∫n archivo de estilos
  * `toc`: si quieres √≠ndice o no
  * `toc-location`: posici√≥n del √≠ndice
  * `toc-title`: t√≠tulo del √≠ndice
* `editor`: si est√°s en modo visual o source.

:::

::: {.column width="40%"}

``` yaml
---
title: "prueba"
author: "javier √°lvarez li√©bana"
format:
  html:
    style: style.css
    toc: true
editor: visual
---
```

:::
::::

---

## Cabecera de un qmd {auto-animate="true"}

La [**cabecera est√°n en formato YAML**]{.hl-yellow} y contiene los [**metadatos**]{.hl-yellow} del documento

:::: columns
::: {.column width="60%"}

* `title` y `subtitle`: el t√≠tulo/subt√≠tulo del documento
* `author`: autor del mismo
* `format`: formato de salida (podremos personalizar)
  * `theme`: si tienes alg√∫n archivo de estilos
  * `toc`: si quieres √≠ndice o no
  * `toc-location`: posici√≥n del √≠ndice
  * `toc-title`: t√≠tulo del √≠ndice
* `editor`: si est√°s en modo visual o source.

:::

::: {.column width="40%"}

``` yaml
---
title: "prueba"
author: "javier √°lvarez li√©bana"
format:
  html:
    style: style.css
    toc: true
    toc-location: left
editor: visual
---
```

:::
::::

---

## Cabecera de un qmd {auto-animate="true"}

La [**cabecera est√°n en formato YAML**]{.hl-yellow} y contiene los [**metadatos**]{.hl-yellow} del documento

:::: columns
::: {.column width="60%"}

* `title` y `subtitle`: el t√≠tulo/subt√≠tulo del documento
* `author`: autor del mismo
* `format`: formato de salida (podremos personalizar)
  * `theme`: si tienes alg√∫n archivo de estilos
  * `toc`: si quieres √≠ndice o no
  * `toc-location`: posici√≥n del √≠ndice
  * `toc-title`: t√≠tulo del √≠ndice
* `editor`: si est√°s en modo visual o source.

:::

::: {.column width="40%"}

``` yaml
---
title: "prueba"
author: "javier √°lvarez li√©bana"
format:
  html:
    style: style.css
    toc: true
    toc-location: left
    toc-title: √çndice
editor: visual
---
```

:::
::::


---

## Texto de un qmd

Respecto a la escritura solo hay una [**cosa importante**]{.hl-yellow}: salvo que indiquemos lo contrario, [**TODO lo que vamos a escribir es texto (normal)**]{.hl-yellow}. No c√≥digo R.

:::: columns
::: {.column width="35%"}
![](img/quarto-prueba-qmd2.png){width=350}
![](img/quarto-prueba-html2.png){width=320}
:::

::: {.column width="65%"}
Vamos a empezar escribiendo una secci√≥n al inicio (`# Intro` y detr√°s por ej. la frase

> Este material ha sido dise√±ado por el profesor Javier √Ålvarez Li√©bana, docente en la Universidad Complutense de Madrid

Adem√°s al `Running Code` le a√±adiremos una almohadilla `#`: las [**almohadillas FUERA DE CHUNKS**]{.hl-yellow} nos servir√°n para crear [**ep√≠grafes (secciones)**]{.hl-yellow} en el documento


:::
::::


---

## √çndice de un qmd

:::: columns
::: {.column width="40%"}
![](img/quarto-indice-qmd-2.png){width=370}
![](img/quarto-indice-html2.png){width=370}
:::

::: {.column width="60%"}
Para que el [**√≠ndice capture dichas secciones**]{.hl-yellow} modificaremos la cabecera del archivo como se observa en la imagen (puedes cambiar la localizaci√≥n del √≠ndice y el t√≠tulo si quieres para probar).

:::
::::

---

## Texto en un qmd

Vamos a [**personalizar un poco el texto**]{.hl-yellow} haciendo lo siguiente:


:::: columns
::: {.column width="50%"}
![](img/quarto-texto-mejorado-qmd.png){width=370}
![](img/quarto-texto-mejorado-html.png){width=370}
:::

::: {.column width="50%"}
* Vamos a a√±adir [**negrita al nombre**]{.hl-yellow} (poniendo ** al inicio y al final).

* Vamos a√±adir [**cursiva**]{.hl-yellow} a la palabra material (poniendo _ al inicio y al final).

* Vamos a√±adir un [**enlace**]{.hl-yellow} <https://www.ucm.es>, asoci√°ndolo al nombre de la Universidad. Para ello el t√≠tulo lo ponemos entre corchetes y justo detr√°s el enlace entre par√©ntesis `[¬´Universidad Complutense de Madrid¬ª](https://www.ucm.es)`

:::
::::

---

## C√≥digo en un qmd

Para [**a√±adir c√≥digo R**]{.hl-yellow} debemos crear nuestras [**cajas de c√≥digo llamadas chunks**]{.hl-yellow}: altos en el camino en nuestro texto markdown donde podremos incluir c√≥digo de casi cualquier lenguaje (y sus salidas).

&nbsp;

:::: columns
::: {.column width="50%"}
![](img/quarto-chunk-qmd.png){width=470}
:::

::: {.column width="50%"}

Para incluir uno deber√° de ir [**encabezado**]{.hl-yellow} de la siguiente forma tienes un atajo `Command + Option + I` (Mac) o `Ctrl + Shift + I` (Windows)
:::

::::

---

## C√≥digo en un qmd

Dentro de dicha cajita (que tiene ahora otro color en el documento) [**escribiremos c√≥digo R**]{.hl-yellow} como lo ven√≠amos haciendo hasta ahora en los scripts.

:::: columns
::: {.column width="50%"}
![](img/quarto-chunk-1-qmd.png){width=410}
![](img/quarto-chunk-1-html.png){width=410}
:::

::: {.column width="50%"}

 Vamos por ejemplo a definir dos variables y su suma de la siguiente manera, escribiendo dicho c√≥digo en nuestro `.qmd` (dentro de ese chunk)


```{r}
# C√≥digo R
x <- 1
y <- 2
x + y
```


:::

::::


---

## Etiquetando chunks


:::: columns
::: {.column width="50%"}
![](img/quarto-tag-chunks-qmd.png){width=400}
![](img/quarto-tag-chunks-html.png){width=400}
:::

::: {.column width="50%"}
Los chunks pueden tener un [**nombre o etiqueta**]{.hl-yellow}, de forma que podamos referenciarlos de nuevo para no repetir c√≥digo.
:::
::::


---

## Ejecutando chunks

:::: columns
::: {.column width="40%"}
![](img/quarto-inline-qmd.png){width=400}
![](img/quarto-inline-html.png){width=380}
:::

::: {.column width="60%"}
En cada chunk aparecen [**dos botones**]{.hl-yellow}:

* bot√≥n de [**play**]{.hl-yellow}: activa la [**ejecuci√≥n y salida de ese chunk particular**]{.hl-yellow} (lo puedes visualizar dentro de tu propio `RStudio`)

* bot√≥n de [**rebobinar**]{.hl-yellow}: activa la [**ejecuci√≥n y salida de todos los chunk hasta ese**]{.hl-yellow} (sin llegar a √©l)

&nbsp;

Adem√°s podemos [**incluir c√≥digo R dentro de la l√≠nea de texto**]{.hl-yellow} (en lugar de mostrar el texto x ejecuta el c√≥digo R mostrando la variable).
:::
::::



---

## Personalizaci√≥n de chunks

Los [**chunks podemos personalizarlos**]{.hl-yellow} con opciones al inicio del chunk precedido de `#|`:

* `#| echo: false`: [**ejecuta c√≥digo**]{.hl-green} y se [**muestra resultado**]{.hl-green} pero [**no visualiza c√≥digo**]{.hl-red} en la salida.

* `#| include: false`: [**ejecuta c√≥digo**]{.hl-green} pero [**no muestra resultado**]{.hl-red} y [**no visualiza c√≥digo**]{.hl-red} en la salida.

* `#| eval: false`: [**no ejecuta c√≥digo**]{.hl-red}, [**no muestra resultado**]{.hl-red} pero [**s√≠ visualiza c√≥digo**]{.hl-green} en la salida.

* `#| message: false`: [**ejecuta c√≥digo**]{.hl-green} pero [**no muestra mensajes de salida**]{.hl-red}.

* `#| warning: false`: [**ejecuta c√≥digo**]{.hl-green} pero [**no muestra mensajes de warning**]{.hl-red}.

* `#| error: true`: [**ejecuta c√≥digo**]{.hl-green} y [**permite que haya errores**]{.hl-green} mostrando el mensaje de error en la salida.


![](img/quarto-options-chunk.png){width=380}

Estas opciones podemos aplicarlas chunk a chunk o fijar los par√°metros de forma global con `knitr::opts_chunk$set()` al inicio del documento (dentro de un chunk).

---

## Personalizando chunks

Si queremos que aplique la **opci√≥n a todos los chunks por defecto** debemos incluirlo al final de la cabecera, como [**opciones de ejecuci√≥n**]{.hl-yellow}

``` yaml
---
title: "¬°Hola!"
format: html
editor: visual
execute:
  echo: false
---
```

---

## Organizando qmd

Adem√°s de texto y c√≥digo podemos introducir lo siguiente:

* [**Ecuaciones**]{.hl-yellow}: puedes a√±adir adem√°s ecuaciones como $x^2$ (he escrito `$x^2$`, la ecuaci√≥n entre d√≥lares).

* [**Listas**]{.hl-yellow}: puedes itemizar elementos poniendo `*`

`* Paso 1: ...`

`* Paso 2: ...`

* [**Cross-references**]{.hl-yellow}: puedes etiquetar partes del documento (la etiqueta se construye con `{#nombre-seccion}`) y llamarlas luego con `[Secci√≥n](@nombre-seccion)`

---

## Gr√°ficas/im√°genes en qmd

:::: columns
::: {.column width="50%"}
![](img/quarto-fig-qmd.png){width=340}
![](img/quarto-fig-html.png){width=390}
:::

::: {.column width="50%"}
Por √∫ltimo, tambi√©n podemos [**a√±adir pies de gr√°ficas o im√°genes**]{.hl-yellow} a√±adiendo `#| fig-cap: "..."`
:::
::::

. . .

:::: columns
::: {.column width="65%"}
F√≠jate que el [**caption est√° en el margen**]{.hl-yellow} (por ejemplo). Puedes cambiarlo introduciendo [**ajustes en la cabecera**]{.hl-yellow} (todo lo relativo a figuras empieza por `fig-`, y puedes ver las opciones tabulando). Tienes m√°s informaci√≥n en **<https://quarto.org/>**
:::

::: {.column width="35%"}
![](img/quarto-cabecera-desplegable.png){width=400}
:::
::::

---

## A√±adir estilos

:::: columns
::: {.column width="50%"}
![](img/quarto-estilos-qmd.png){width=400}
![](img/quarto-estilos-html.png){width=400}
:::

::: {.column width="50%"}
Por √∫ltimo puedes a√±adir un [**tema personalizado**]{.hl-yellow} incluyendo un [**archivo de estilos**]{.hl-yellow} (archivo en formato `.scss` o `.css`). Te he dejado uno en <https://github.com/dadosdelaplace/docencia-R-master-bio-2324/tree/main/material>.

::: callout-important
## Importante

El archivo de estilos debe estar en la misma carpeta que el archivo `.qmd`
:::
:::
::::

---

## A√±adir estilos

Tambi√©n puedes hacerlo de manera sencilla [**a√±adiendo a los textos un poco de HTML**]{.hl-yellow}. Por ejemplo, para personalizar el color de un texto va entre corchetes y justo tras el texto, entre llaves, las opciones de estilo

``` html
Esta palabra es [roja]{style="color:red;"} ...
```

``` html
... y esta [verde y en negrita]{style="color:green; font-weight: bold;"}
```

. . .

Esta palabra es [roja]{style="color:red;"} ...

... y esta [verde y en negrita]{style="color:green; font-weight: bold;"}

---

## Revealjs

Puedes a√±adir algunas ¬´animaciones¬ª usando lo que se conoce como Revealjs (javascript), especifc√°ndolo en la cabecera y usando [**bloques**]{.hl-yellow} de dicho lenguaje delimitados por `:::` al inicio y final, y la palabra de la ¬´herramienta¬ª a usar. Por ejemplo `{.incremental}` hace una transici√≥n de los elementos.

``` yaml
format:
  revealjs
```
&nbsp;

``` revealjs
::: {.incremental}
- Me
- llamo
- Javi
:::
```


::: {.incremental}
- Me
- llamo
- Javi
:::

---

## Bloques de llamada

Tambi√©n puedes usar los [**bloques de llamada**]{.hl-yellow} que por defecto son `note`, `tip`, `warning`, `caution` e `important` (aunque los puedes crear y personalizar). Para ello basta con usar `:::{.callout-tipo}` y el tipo que quieras

``` html
:::{.callout-tip}

Note that there are five types of callouts, including: 
`note`, `tip`, `warning`, `caution`, and `important`.

:::
```

. . .

:::{.callout-tip}

Recuerda que los 5 tipos son `note`, `tip`, `warning`, `caution` e `important`.

:::

. . .

:::{.callout-caution}

√ösalos con cabeza, a veces mucho recursos est√©tico puede marear.

:::

---

## C√≥digo ajeno a R

Adem√°s `{reticulate}` nos permite crear chunks de `python` dentro de un Quarto en `R` (ver <https://quarto.org/docs/computations/python.html> para crear jupyter notebooks directamente desde Quarto)


```{r}
#| echo: false
#| eval: false
library(reticulate)
```

```{r}
#| eval: false
# install.packages("reticulate")
library(reticulate)

install_python("3.9.12") # Instalar python en PC sino lo tienes

# Instalar paquetes de Python
reticulate::py_install("numpy")
reticulate::py_install("matplotlib")
```


:::: columns
::: {.column width="60%"}


```{python}
#| eval: false
import numpy as np
import matplotlib.pyplot as plt
r = np.arange(0, 2, 0.05)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
plt.show()
```


:::

::: {.column width="40%"}


```{python}
#| echo: false
#| eval: false
import numpy as np
import matplotlib.pyplot as plt
r = np.arange(0, 2, 0.05)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
plt.show()
```


:::
::::

---



## Ejemplo de entrega

Vamos a realizar un peque√±o simulacro antes de la entrega usando el dataset `starwars` del paquete `{dplyr}`

![](https://sm.ign.com/t/ign_latam/screenshot/default/baby-yoda-nombre_3x56.1280.jpg)

---

## Ejemplo de entrega


```{r}
library(dplyr)
starwars
```


En √©l tenemos **diferentes variables de los personajes de Star Wars**, con caracter√≠sticas de su pelo, piel, altura, nombre, etc.


---

## Ejemplo de entrega

> Crea un documento `.qmd` con nombre, t√≠tulo, formato e √≠ndice. Cada ejercicio posterior ser√° una subsecci√≥n del documento. Ejecuta los chunks que consideres y comenta las salidas para responder a cada pregunta

. . .

> Ejercicio 1. ¬øCu√°ntos personajes hay guardados en la base de datos? ¬øCu√°ntas caracter√≠sticas se han medido de cada uno?

. . .

> Ejercicio 2. Extrae en dos variables distintas `nombres` y `edades` las variables correspondientes de la tabla. ¬øDe qu√© tipo es la variable nombre? ¬øY la variable birth_year?

. . .

> Ejercicio 3. Obt√©n el vector de nombres de los personajes ordenados de mayores a j√≥venes.


---

## Ejemplo de entrega

> Ejercicio 4. Busca ayuda de la funci√≥n unique(). √ösala para saber que modalidades tiene la variable cualitativa correspondiente al color de ojos. ¬øCu√°ntos distintos hay?

. . .

> Ejercicio 5. ¬øExiste ALG√öN valor ausente en la variable de color ojos? 

. . .

> Ejercicio 6.  Calcula la media y desviaci√≥n t√≠pica de las variables de estatura y peso (cuidado con los ausentes). Define un nuevo tibble con esas dos variables e incorpora una tercera variable que se llame ‚ÄúIMC‚Äù que calcule el √≠ndice de masa corporal. Incorpora con `$ $` la f√≥rmula usada para el IMC.


---

## Estructuras de control

Una [**estructura de control**]{.hl-yellow} se compone de una serie de comandos orientados a  [**decidir el camino**]{.hl-yellow} que tu c√≥digo debe recorrer

* Si se cumple la condici√≥n A, ¬øqu√© sucede?

* ¬øY si sucede B?

* ¬øC√≥mo puedo repetir una misma expresi√≥n (dependiendo de una variable)?

. . .

Si has programado antes, quiz√°s te sea familiar las conocidas como [**estructuras condicionales**]{.hl-yellow} tales como `if (blabla) {...} else {...}`  o [**bucles**]{.hl-yellow} `for/while` (a evitar siempre que podamos).

---

## Estructura If

Una de las estructuras de control m√°s famosas son las conocidas como [**estructuras condicionales**]{.hl-yellow} `if`.

> SI (IF) un conjunto de condiciones se cumple (TRUE), entonces ejecuta lo que haya dentro de las llaves

Por ejemplo, la estructura `if (x == 1) { c√≥digo A }` lo que har√° ser√° [**ejecutar el c√≥digo A entre llaves**]{.hl-yellow} pero [**SOLO SI**]{.hl-purple} la [**condici√≥n entre par√©ntesis es cierta**]{.hl-purple} (solo si `x` es 1). En cualquier otro caso, no har√° nada.

. . .

Por ejemplo, definamos un vector de edades de 8 personas


```{r}
edad <- c(14, 17, 24, 56, 31, 20, 87, 73)
edad < 18
```


---

## Estructura If


Nuestra estructura condicional har√° lo siguiente: [**si existe alg√∫n menor de edad, imprimir√°**]{.hl-yellow} por pantalla un mensaje.


```{r}
if (any(edad < 18)) { 
  
  print("Existe alguna persona menor de edad")
  
}
```


---

## Estructura If


```{r}
#| eval: false
if (any(edad < 18)) { 
  
  print("Existe alguna persona menor de edad")
  
}
```



En caso de que las [**condiciones no sean ciertas**]{.hl-yellow} dentro de `if()` (`FALSE`), no sucede nada



```{r}
if (all(edad >= 18)) { 
  
  print("Todos son mayores de edad")
  
}
```


**No obtenemos ning√∫n mensaje** porque la condici√≥n `all(edad >= 18)` no es `TRUE`, as√≠ que no ejecuta nada.

---

## Estructura If-else

La estructura `if (condicion) { c√≥digo A }` puede combinarse con un `else { c√≥digo B }`: cuando la [**condici√≥n no est√° verificada**]{.hl-yellow}, se [**ejecutar√° el c√≥digo alternativo B**]{.hl-yellow} dentro de `else { }`, permiti√©ndonos decidir que sucede cuando se cumple y cuando no.

. . .

Por ejemplo, `if (x == 1) { c√≥digo A } else { c√≥digo B }` ejecutar√° A si `x` es igual a 1 y B en cualquier otro caso.


```{r}
if (all(edad >= 18)) { 
  
  print("Todos son mayores de edad")
  
} else {
  
  print("Existe alguna persona menor de edad")
}
```


---

## Estructura If-else

Esta estructura `if - else` puede ser [**anidada**]{.hl-yellow}: imagina que queremos ejecutar un c√≥digo si todos son menores; si no sucede, pero todos son mayores de 16, hacer otra cosa; en cualquier otra cosa, otra acci√≥n.


```{r}
if (all(edad >= 18)) { 
  
  print("Todos son mayores de edad")
  
} else if (all(edad >= 16)) {
  
  print("Hay alg√∫n menor de edad pero todos con 16 a√±os o m√°s")
  
} else { print("Hay alguna persona con menos de 16 a√±os") }
```


::: callout-note
## Truco

Puedes **colapsar las estructuras** haciendo click en la flecha a la izquierda que aparece en tu script.

:::


---

## If-else vectorizado



Esta estructura condicional se puede [**vectorizar**]{.hl-yellow} (en una sola l√≠nea) con `if_else()` (del paquete `{dplyr}`), cuyos argumentos son

* la condici√≥n a evaluar
* lo que sucede cuando se cumple y cuando no
* un argumento opcional para cuando la condici√≥n a evaluar es `NA`

Vamos a etiquetar sin son mayores/menores y un "desconocido" cuando no conocemos


```{r}
library(dplyr)
edad <- c(NA, edad)
if_else(edad >= 18, "mayor", "menor", missing = "desconocido")
```


En `R` base existe `ifelse()`: no deja especificar que hacer con los ausentes pero permite especificar distintos tipos de datos en `TRUE` y en `FALSE`.


---

## Bucles

Aunque en la mayor√≠a de ocasiones se pueden reemplazar por otras estructuras m√°s eficientes y legibles, es importante conocer una de las expresiones de control m√°s famosas: los [**bucles**]{.hl-yellow}.

* `for { }`: permite [**repetir el mismo c√≥digo**]{.hl-yellow} en un [**n√∫mero prefijado y conocido**]{.hl-purple} de veces.

* `while { }`: permite [**repetir el mismo c√≥digo**]{.hl-yellow} pero en un [**n√∫mero indeterminado de veces**]{.hl-purple} (hasta que una **condici√≥n** deje de cumplirse).

---

## Bucles for {auto-animate="true"}

Un bucle [**for**]{.hl-yellow} es una estructura que permite [**repetir**]{.hl-yellow} un conjunto de √≥rdenes un n√∫mero [**finito, prefijado y conocido de veces**]{.hl-purple} dado un conjunto de √≠ndices.


Vamos a definir un vector `x <- c(0, -7, 1, 4)` y otra variable vac√≠a `y`. Tras ello definiremos un bucle for con `for () { }`: dentro de los par√©ntesis indicaremos un √≠ndice y unos valores a recorrer, dentro de las llaves el c√≥digo a ejecutar en cada iteraci√≥n (en este caso, rellenar `y` como `x + 1`)


```{r}
x <- c(0, -7, 1, 4)
y <- c()
```


---

## Bucles for {auto-animate="true"}

Un bucle [**for**]{.hl-yellow} es una estructura que permite [**repetir**]{.hl-yellow} un conjunto de √≥rdenes un n√∫mero [**finito, prefijado y conocido de veces**]{.hl-purple} dado un conjunto de √≠ndices.


Vamos a definir un vector `x <- c(0, -7, 1, 4)` y otra variable vac√≠a `y`. Tras ello definiremos un bucle for con `for () { }`: dentro de los par√©ntesis indicaremos un √≠ndice y unos valores a recorrer, dentro de las llaves el c√≥digo a ejecutar en cada iteraci√≥n (en este caso, rellenar `y` como `x + 1`)


```{r}
x <- c(0, -7, 1, 4)
y <- c()

for (i in 1:4) {
  
}
```


---

## Bucles for {auto-animate="true"}

Un bucle [**for**]{.hl-yellow} es una estructura que permite [**repetir**]{.hl-yellow} un conjunto de √≥rdenes un n√∫mero [**finito, prefijado y conocido de veces**]{.hl-purple} dado un conjunto de √≠ndices.


Vamos a definir un vector `x <- c(0, -7, 1, 4)` y otra variable vac√≠a `y`. Tras ello definiremos un bucle for con `for () { }`: dentro de los par√©ntesis indicaremos un √≠ndice y unos valores a recorrer, dentro de las llaves el c√≥digo a ejecutar en cada iteraci√≥n (en este caso, rellenar `y` como `x + 1`)


```{r}
x <- c(0, -7, 1, 4)
y <- c()

for (i in 1:4) {
  y[i] <- x[i] + 1
}
```


---

## Bucles for

F√≠jate que debido a que `R` funciona de manera [**vectorial por defecto**]{.hl-yellow}, el bucle es lo mismo que hacer `x + 1` directamente.


```{r}
x <- c(0, -7, 1, 4)
y <- c()

for (i in 1:4) {
  y[i] <- x[i] + 1
}
y

y2 <- x + 1
y2
```


---

## Bucles for

Otra opci√≥n habitual es indicar los √≠ndices de manera ¬´autom√°tica¬ª: desde el primero `1` hasta el √∫ltimo (que corresponde con la longitud de x `length(x)`)


```{r}
x <- c(0, -7, 1, 4)
y <- c()

for (i in 1:length(x)) {
  y[i] <- x[i] + 1
}
y

```



---

## Bucles for

As√≠ la [**estructura general de un bucle for**]{.hl-yellow} ser√° siempre la siguiente


```{r}
#| eval: false
for (√≠ndice in conjunto) { 
  c√≥digo (dependiente de i)
}
```


[**SIEMPRE**]{.hl-green} sabemos cu√°ntas iteraciones tenemos (tantas como elementos haya en el conjunto a indexar)

---

## Evitando bucles


Como ya hemos aprendido con el paquete`{microbenchmark}` podemos chequear como [**los bucles suelen ser muy ineficientes**]{.hl-yellow} (de ah√≠ que debamos evitarlos en la mayor√≠a de ocasiones


```{r}
library(microbenchmark)
x <- 1:1000
microbenchmark(y <- x^2, 
               for (i in 1:100) { y[i] <- x[i]^2 },
               times = 500)
```


---
 
## Bucles for


Podemos ver otro ejemplo de bucle  [**combinando n√∫meros y textos**]{.hl-yellow}: definimos un vector de edades y de nombres, e imprimimos el nombre y edad i-√©sima.


```{r}
nombres <- c("Javi", "Sandra", "Carlos", "Marcos", "Marta")
edades <- c(33, 27, 18, 43, 29)
library(glue)
for (i in 1:5) { 
  
  print(glue("{nombres[i]} tiene {edades[i]} a√±os")) 
  
}
```



---

## Bucles for

Aunque normalmente se suelen indexar con vectors num√©ricos, los bucles pueden ser [**indexados sobre cualquier estructura vectorial**]{.hl-yellow}, da igual de que tipo sea el conjunto


```{r}
library(stringr)
week_days <- c("monday", "tuesday", "wednesday", "thursday",
               "friday", "saturday", "sunday")

for (days in week_days) {
  
  print(str_to_upper(days))
}
```


---

## Bucles y condicionales

Vamos a **combinar las estructuras condicionales y los bucles**: usando el conjunto `swiss` del paquete `{datasets}`, vamos a asignar `NA`  si los valores de fertilidad son mayores de 80.


```{r}
for (i in 1:nrow(swiss)) {
  
  if (swiss$Fertility[i] > 80) { 
    
    swiss$Fertility[i] <- NA
    
  }
}
```


. . .

Esto es exactamente igual a un `if_else()` vectorizado


```{r}
data("swiss")
swiss$Fertility <- if_else(swiss$Fertility > 80, NA, swiss$Fertility)
```



---

## Bucles while

Otra forma de crear un bucle es con la estructura `while { }`, que nos ejecutar√° un bucle [**un n√∫mero desconocido de veces**]{.hl-yellow}, hasta que una condici√≥n [**deje de cumplirse**]{.hl-yellow} (de hecho puede que nunca termine). Por ejemplo, vamos a inializar una variable `ciclos <- 1`, que incrementaremos en cada paso, y no saldremos del bucle hasta que `ciclos > 4`.


```{r}
ciclos <- 1
while(ciclos <= 4) {
  
  print(glue("No todav√≠a, vamos por el ciclo {ciclos}")) 
  ciclos <- ciclos + 1
  
}
```


---

## Bucles while

Un bucle `while` ser√° siempre como sigue


```{r}
#| eval: false
while(condici√≥n) {
  
  c√≥digo a hacer mientras la condici√≥n sea TRUE
  # normalmente aqu√≠ se actualiza alguna variable
  
}
```


---
  
## Bucles while

¬øQu√© sucede cuando la [**condici√≥n nunca es FALSE**]{.hl-yellow}? Pru√©balo tu mismo


```{r}
#| eval: false
while (1 > 0) {
  
  print("Presiona ESC para salir del bucle")
  
}
```


&nbsp;

::: callout-warning
## Cuidado

Un bucle `while { }` puede ser bastante ¬´peligroso¬ª sino controlamos bien c√≥mo pararlo.

:::

---

## Bucles while

Contamos con dos palabras reservadas para [**abortar un bucle o forzar su avance**]{.hl-yellow}:

* `break`: permite [**abortar un bucle**]{.hl-yellow} incluso si no se ha llegado a su final


```{r}
for(i in 1:10) {
  if (i == 3) {
    
    break # si i = 3, abortamos bucle
    
  }
  print(i)
}
```


---

## Bucles while

Contamos con dos palabras reservadas para [**abortar un bucle o forzar su avance**]{.hl-yellow}:

* `next`: [**fuerza un bucle a avanzar a la siguiente iteraci√≥n**]{.hl-yellow} 


```{r}
for(i in 1:5) {
  if (i == 3) {
    
    next # si i = 3, la obvia y continua al siguiente
    
  }
  print(i)
}
```




---

## üíª Tu turno {#tu-turno-1-3}


[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Ejercicio 1**]{.hl-yellow}

üìù ¬øCu√°l es la salida del siguiente c√≥digo?


```{r}
#| eval: false
if_else(sqrt(9) < 2, sqrt(9), 0)
```

```{r}
#| eval: false
#| code-fold: true

La salida es 0 ya que sqrt(9) es igual 3, y dado que no es menor que 2, devuelve el segundo argumento que es 0
```


### [**Ejercicio 2**]{.hl-yellow}

üìù ¬øCu√°l es la salida del siguiente c√≥digo?


```{r}
#| eval: false
x <- c(1, NA, -1, 9)
if_else(sqrt(x) < 2, 0, 1)
```

```{r}
#| eval: false
#| code-fold: true

La salida es el vector c(0, NA, NA, 1) ya que sqrt(1) s√≠ es menor que 2, sqrt(9) no lo es, y tanto en el caso de sqrt(NA) (ra√≠z de ausente) como sqrt(-1) (devuelve NaN, not a number), su ra√≠z cuadrada no puede verificarse si es menor que 2 o no, as√≠ que la salida es NA.
```



### [**Ejercicio 3**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para que, cuando no se pueda verificar si la ra√≠z cuadrada de un n√∫mero es menor que 2, devuelva -1


```{r}
#| eval: false
x <- c(1, NA, -1, 9)
if_else(sqrt(x) < 2, 0, 1)
```

```{r}
#| eval: false
#| code-fold: true
x <- c(1, NA, -1, 9)
if_else(sqrt(x) < 2, 0, 1, missing = -1)
```



### [**Ejercicio 4**]{.hl-yellow}

üìù ¬øCu√°l es son los valores de `x` e `y` del c√≥digo inferior para `z <- 1`, `z <- -1` y `z <- -5`?


```{r}
#| eval: false
z <- -1
if (z > 0) {
  
  x <- z^3
  y <- -sqrt(z)
  
} else if (abs(z) < 2) {
  
  x <- z^4
  y <- sqrt(-z)
  
} else {
  
  x <- z/2
  y <- abs(z)
  
}
```

```{r}
#| eval: false
#| code-fold: true
En primero caso x = 1 e y = -1. En el segundo caso x = 1 e y = 1. En el tercer caso -1 y 2
```



### [**Ejercicio 5**]{.hl-yellow}

üìù ¬øQu√© suceder√° si ejecutamos el c√≥digo inferior?


```{r}
#| eval: false
z <- "a"
if (z > 0) {
  
  x <- z^3
  y <- -sqrt(z)
  
} else if (abs(z) < 2) {
  
  x <- z^4
  y <- sqrt(-z)
  
} else {
  
  x <- z/2
  y <- abs(z)
  
}
```

```{r}
#| code-fold: true
#| eval: false

# dar√° error ya que no es un argumento num√©rico
Error in z^3 : non-numeric argument to binary operator
```



### [**Ejercicio 6**]{.hl-yellow}

üìù Del paquete `{lubridate}`, la funci√≥n `hour()` nos devuelve la hora de una fecha dada, y la funci√≥n `now()` nos devuelve fecha y hora del momento actual. Con ambas funciones haz que se imprima por pantalla (`cat()`) "buenas noches" solo a partir de las 21 horas.


```{r}
#| code-fold: true
#| eval: false

# Cargamos librer√≠a
library(lubridate)

# Fecha-hora actual
fecha_actual <- now()

# Estructura if
if (hour(fecha_actual) > 21) {
  
  cat("Buenas noches") # print/cat dos formas de imprimir por pantalla
}
```


### [**Ejercicio 7**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para que se imprima un mensaje por pantalla si y solo si todos los datos de `airquality` son con mes distinto a enero


```{r}
#| eval: false
library(datasets)
months <- airquality$Month

if (months == 2) {
  print("No hay datos de enero")
}
```

```{r}
#| code-fold: true
#| eval: false
library(datasets)
months <- airquality$Month

if (all(months != 1)) {
  print("No hay datos de enero")
}
```



### [**Ejercicio 8**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para guardar en una variable llamada `temp_alta` un `TRUE` si alguno de los registros tiene una temperatura superior a 90 grados Farenheit y `FALSE` en cualquier otro caso
 

```{r}
#| eval: false
temp <- airquality$Temp

if (temp == 100) {
  print("Algunos de los registros tienen temperaturas superiores a 90 grados Farenheit")
}
```

```{r}
#| eval: false
#| code-fold: true
# Option 1
temp <- airquality$Temp
temp_alta <- FALSE
if (any(temp > 90)) {
   temp_alta <- TRUE
}

# Option 2
temp_alta <- any(airquality$Temp > 90)
```


### [**Ejercicio 9**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para dise√±ar un bucle `for` de 5 iteraciones que solo recorra los primeros 5 impares (y en cada paso del bucle los imprima)


```{r}
#| eval: false
for (i in 1:5) {
  
  print(i)
}
```

```{r}
#| eval: false
#| code-fold: true
for (i in c(1, 3, 5, 7, 9)) {
  
  print(i)
}
```


### [**Ejercicio 10**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para dise√±ar un bucle `while` que empiece con un contador `count <- 1` y pare cuando llegue a 6


```{r}
#| eval: false
count <- 1
while (count == 2) {
  
  print(count)
}
```

```{r}
#| eval: false
#| code-fold: true
count <- 1
while (count < 6) {
  
  print(count)
  count <- count + 1
  
}
```



:::

---

## üê£ Caso pr√°ctico II {#caso-practico-1-2}


Intenta responder a las preguntas planteadas en el [**workbook**](https://javieralvarezliebana.quarto.pub/repaso-r-ts/) donde tendr√°s que dise√±ar algunos estudios de simulaci√≥n haciendo uso de **bucles y estructuras condicionales**


---

## Creando funciones {auto-animate="true"}

No solo podemos usar **funciones predeterminadas** que vienen ya cargadas en paquetes, adem√°s podemos [**crear nuestras propias funciones**]{.hl-yellow} para **automatizar tareas**. ¬øC√≥mo [**crear nuestra propia funci√≥n**]{.hl-purple}? Veamos su **esquema b√°sico**:

* [**Nombre**]{.hl-yellow}: por ejemplo `name_fun` (sin espacios ni caracteres extra√±os). Al nombre le [**asignamos la palabra reservada**]{.hl-yellow} `function()`.

* Definir [**argumentos de entrada**]{.hl-yellow} (dentro de `function()`).

* [**Cuerpo**]{.hl-yellow} de la funci√≥n dentro de `{ }`.

* Finalizamos la funci√≥n con los [**argumentos de salida**]{.hl-yellow} con `return()`.




```{r}
#| eval: false
name_fun <- function() {
  
}
```


---

## Creando funciones {auto-animate="true"}

No solo podemos usar **funciones predeterminadas** que vienen ya cargadas en paquetes, adem√°s podemos [**crear nuestras propias funciones**]{.hl-yellow} para **automatizar tareas**. ¬øC√≥mo [**crear nuestra propia funci√≥n**]{.hl-purple}? Veamos su **esquema b√°sico**:

* [**Nombre**]{.hl-yellow}: por ejemplo `name_fun` (sin espacios ni caracteres extra√±os). Al nombre le [**asignamos la palabra reservada**]{.hl-yellow} `function()`.

* Definir [**argumentos de entrada**]{.hl-yellow} (dentro de `function()`).

* [**Cuerpo**]{.hl-yellow} de la funci√≥n dentro de `{ }`.

* Finalizamos la funci√≥n con los [**argumentos de salida**]{.hl-yellow} con `return()`.


```{r}
#| eval: false
name_fun <- function(arg1, arg2, ...) {
  
}
```


---

## Creando funciones {auto-animate="true"}

No solo podemos usar **funciones predeterminadas** que vienen ya cargadas en paquetes, adem√°s podemos [**crear nuestras propias funciones**]{.hl-yellow} para **automatizar tareas**. ¬øC√≥mo [**crear nuestra propia funci√≥n**]{.hl-purple}? Veamos su **esquema b√°sico**:

* [**Nombre**]{.hl-yellow}: por ejemplo `name_fun` (sin espacios ni caracteres extra√±os). Al nombre le [**asignamos la palabra reservada**]{.hl-yellow} `function()`.

* Definir [**argumentos de entrada**]{.hl-yellow} (dentro de `function()`).

* [**Cuerpo**]{.hl-yellow} de la funci√≥n dentro de `{ }`.

* Finalizamos la funci√≥n con los [**argumentos de salida**]{.hl-yellow} con `return()`.


```{r}
#| eval: false
name_fun <- function(arg1, arg2, ...) {
  
  c√≥digo a ejecutar
  
}
```


---


## Creando funciones {auto-animate="true"}

No solo podemos usar **funciones predeterminadas** que vienen ya cargadas en paquetes, adem√°s podemos [**crear nuestras propias funciones**]{.hl-yellow} para **automatizar tareas**. ¬øC√≥mo [**crear nuestra propia funci√≥n**]{.hl-purple}? Veamos su **esquema b√°sico**:

* [**Nombre**]{.hl-yellow}: por ejemplo `name_fun` (sin espacios ni caracteres extra√±os). Al nombre le [**asignamos la palabra reservada**]{.hl-yellow} `function()`.

* Definir [**argumentos de entrada**]{.hl-yellow} (dentro de `function()`).

* [**Cuerpo**]{.hl-yellow} de la funci√≥n dentro de `{ }`.

* Finalizamos la funci√≥n con los [**argumentos de salida**]{.hl-yellow} con `return()`.


```{r}
#| eval: false
name_fun <- function(arg1, arg2, ...) {
  
  c√≥digo a ejecutar
  
  return(var_salida)
  
}
```



---

## Creando funciones

* `arg1, arg2, ...`: ser√°n los [**argumentos de entrada**]{.hl-yellow}, los argumentos que toma la funci√≥n para ejecutar el c√≥digo que tiene dentro

* `c√≥digo`: l√≠neas de c√≥digo que queramos que [**ejecute la funci√≥n**]{.hl-yellow}. 

* `return(var_salida)`: se introducir√°n los [**argumentos de salida**]{.hl-yellow}.



```{r}
#| eval: false
name_fun <- function(arg1, arg2, ...) {
  
  # C√≥digo que queramos ejecutar
  c√≥digo
  
  # Salida
  return(var_salida)
  
}
```


::: callout-important
## Importante

Todas las variables que definamos dentro de la funci√≥n son [**variables LOCALES: solo existir√°n dentro de la funci√≥n**]{.hl-yellow} salvo que especifiquemos lo contrario.

:::

---

## Creando funciones {auto-animate="true"}

Veamos un ejemplo muy simple de funci√≥n para [**calcular el √°rea de un rect√°ngulo**]{.hl-yellow}.

Dado que el √°rea de un rect√°ngulo se calcula como el **producto de sus lados**, necesitaremos precisamente eso, sus lados: esos ser√°n los [**argumentos de entrada**]{.hl-yellow} y el [**valor a devolver**]{.hl-purple} ser√° justo su **√°rea** ($lado_1 * lado_2$).


```{r}
# Definici√≥n del nombre de funci√≥n y argumentos de entrada
calcular_area <- function(lado_1, lado_2) {
  
}
```


---


## Creando funciones {auto-animate="true"}

Veamos un ejemplo muy simple de funci√≥n para [**calcular el √°rea de un rect√°ngulo**]{.hl-yellow}.

Dado que el √°rea de un rect√°ngulo se calcula como el **producto de sus lados**, necesitaremos precisamente eso, sus lados: esos ser√°n los [**argumentos de entrada**]{.hl-yellow} y el [**valor a devolver**]{.hl-purple} ser√° justo su **√°rea** ($lado_1 * lado_2$).


```{r}
# Definici√≥n del nombre de funci√≥n y argumentos de entrada
calcular_area <- function(lado_1, lado_2) {
  
  area <- lado_1 * lado_2
  
}
```


---


## Creando funciones {auto-animate="true"}

Veamos un ejemplo muy simple de funci√≥n para [**calcular el √°rea de un rect√°ngulo**]{.hl-yellow}.

Dado que el √°rea de un rect√°ngulo se calcula como el **producto de sus lados**, necesitaremos precisamente eso, sus lados: esos ser√°n los [**argumentos de entrada**]{.hl-yellow} y el [**valor a devolver**]{.hl-purple} ser√° justo su **√°rea** ($lado_1 * lado_2$).


```{r}
# Definici√≥n del nombre de funci√≥n y argumentos de entrada
calcular_area <- function(lado_1, lado_2) {
  
  area <- lado_1 * lado_2
  return(area)
  
}
```


---

## Uso de funciones

Tambi√©n podemos hacer una definici√≥n directa de las variables **sin almacenar por el camino**.


```{r}
# Definici√≥n del nombre de funci√≥n y argumentos de entrada
calcular_area <- function(lado_1, lado_2) {
  
  return(lado_1 * lado_2)
  
}
```


. . .

[**¬øC√≥mo aplicar la funci√≥n?**]{.hl-yellow}


```{r}
calcular_area(5, 3) # √°rea de un rect√°ngulo 5 x 3 
calcular_area(1, 5) # √°rea de un rect√°ngulo 1 x 5
```


---

## Uso de funciones

::: callout-tip

Aunque no sea necesario, es [**recomendable hacer expl√≠cita la llamada de los argumentos**]{.hl-green}, especificando en el c√≥digo qu√© valor es para cada argumento para que no dependa de su orden, haciendo el c√≥digo m√°s legible
:::



```{r}
calcular_area(lado_1 = 5, lado_2 = 3) # √°rea de un rect√°ngulo 5 x 3 
calcular_area(lado_2 = 3, lado_1 = 5) # √°rea de un rect√°ngulo 5 x 3 
```


---

## Argumentos por defecto

Imagina ahora que nos damos cuenta que el 90% de las veces usamos dicha funci√≥n para [**calcular por defecto el √°rea de un cuadrado**]{.hl-yellow} (es decir, solo necesitamos un lado). Para ello, podemos definir [**argumentos por defecto**]{.hl-yellow} en la funci√≥n: tomar√°n dicho valor salvo que le asignemos otro.

¬øPor qu√© no asignar `lado_2 = lado_1` **por defecto**, para ahorrar l√≠neas de c√≥digo y tiempo?

. . .


```{r}
calcular_area <- function(lado_1, lado_2 = lado_1) {
  
  # Cuerpo de la funci√≥n
  area <- lado_1 * lado_2
  
  # Resultado que devolvemos
  return(area)
  
}
```


---

## Argumentos por defecto



```{r}
calcular_area <- function(lado_1, lado_2 = lado_1) {
  
  # Cuerpo de la funci√≥n
  area <- lado_1 * lado_2
  
  # Resultado que devolvemos
  return(area)
  
}
```



Ahora [**por defecto**]{.hl-yellow} el segundo lado ser√° igual al primero (si se lo a√±adimos usar√° ambos).



```{r}
calcular_area(lado_1 = 5) # cuadrado
calcular_area(lado_1 = 5, lado_2 = 7) # rect√°ngulo
```


---
 

## Salida m√∫ltiple

Compliquemos un poco la funci√≥n y a√±adamos en la salida los valores de cada lado, etiquetados como `lado_1` y `lado_2`, [**empaquetando la salida en una vector**]{.hl-yellow}.


```{r}
#| code-line-numbers: "7-8"
# Definici√≥n del nombre de funci√≥n y argumentos de entrada
calcular_area <- function(lado_1, lado_2 = lado_1) {
  
  # Cuerpo de la funci√≥n
  area <- lado_1 * lado_2
  
  # Resultado
  return(c("area" = area, "lado_1" = lado_1, "lado_2" = lado_2))
  
}
```


---

## Salida m√∫ltiple

Podemos complicar un poco m√°s la salida a√±adiendo una cuarta variable que nos diga, en funci√≥n de los argumentos, [**si rect√°ngulo o cuadrado**]{.hl-yellow}, teniendo que a√±adir en la salida una variable que de tipo caracter (o l√≥gica).


```{r}
#| code-line-numbers: "7-9"
# Definici√≥n del nombre de funci√≥n y argumentos de entrada
calcular_area <- function(lado_1, lado_2 = lado_1) {
  
  # Cuerpo de la funci√≥n
  area <- lado_1 * lado_2
  
  # Resultado
  return(c("area" = area, "lado_1" = lado_1, "lado_2" = lado_2,
           "tipo" = if_else(lado_1 == lado_2, "cuadrado", "rect√°ngulo")))
  
}
calcular_area(5, 3)
```


. . .

[**Problema**]{.hl-red}: al intentar juntar n√∫meros y texto, lo convierte todo a n√∫meros. Podr√≠amos guardarlo todo en un `tibble()` como hemos aprendido o en un objeto conocido en `R` como [**listas**]{.hl-yellow}


---

## Orden de los argumentos

Antes nos daba igual el orden de los argumentos pero ahora el [**orden de los argumentos de entrada importa**]{.hl-yellow}, ya que en la salida incluimos `lado_1` y `lado_2`. 

. . .

::: callout-note
## Recomendaci√≥n

Como se comentaba, altamente recomendable hacer la llamada a la funci√≥n [**indicando expl√≠citamente los argumentos**]{.hl-yellow} para mejorar **legibilidad e interpretabilidad**.


```{r}
# Equivalente a calcular_area(5, 3)
calcular_area(lado_1 = 5, lado_2 = 3)
```


:::


---

## Variables locales vs globales

Un aspecto importante sobre el que reflexionar con las funciones: ¬øqu√© sucede si [**nombramos a una variable dentro**]{.hl-yellow} de una funci√≥n a la que se nos ha **olvidado asignar** un valor dentro de la misma?

. . .

Debemos ser cautos al usar funciones en `R`, ya que debido a la [**¬´regla lexicogr√°fica¬ª**]{.hl-yellow}, si una variable no se define dentro de la funci√≥n, `R` [**buscar√° dicha variable en el entorno**]{.hl-purple} de variables.


```{r}
x <- 1
funcion_ejemplo <- function() {
    
  print(x) # No devuelve nada, solo realiza la acci√≥n 
}
funcion_ejemplo()
```


---

## Variables locales vs globales

Si una variable  [**ya est√° definida fuera de la funci√≥n (entorno global)**]{.hl-yellow}, y adem√°s es usada dentro de cambiando su valor, el valor [**solo cambia dentro**]{.hl-yellow} pero [**no en el entorno global**]{.hl-red}.


```{r}
x <- 1
funcion_ejemplo <- function() {
    
  x <- 2
  print(x) # lo que vale dentro
}
```

```{r}
# lo que vale dentro
funcion_ejemplo() #<<
# lo que vale fuera
print(x) #<<
```


---

## Variables locales vs globales


Si queremos que adem√°s de cambiar localmente lo haga [**globalmente**]{.hl-yellow} deberemos usar la [**doble asignaci√≥n**]{.hl-yellow} (`<<-`).


```{r}
x <- 1
y <- 2
funcion_ejemplo <- function() {
  
  # no cambia globalmente, solo localmente
  x <- 3 
  # cambia globalmente
  y <<- 0 #<<
  
  print(x)
  print(y)
}

funcion_ejemplo() # lo que vale dentro
x # lo que vale fuera
y # lo que vale fuera
```


---


## üíª Tu turno {#tu-turno-1-4}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Ej 1**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para definir una funci√≥n llamada `funcion_suma`, de forma que dados dos elementos, devuelve su suma.


```{r}
#| eval: false
nombre <- function(x, y) {
  suma <- # c√≥digo a ejecutar
  return()
}
# Aplicamos la funci√≥n
suma(3, 7)
```

```{r}
#| code-fold: true
#| eval: false
funcion_suma <- function(x, y) {
  suma <- x + y
  return(suma)
}
funcion_suma(3, 7)
```


### [**Ej 2**]{.hl-yellow}

üìù Modifica el c√≥digo inferior para definir una funci√≥n llamada `funcion_producto`, de forma que dados dos elementos, devuelve su producto, pero que por defecto calcule el cuadrado


```{r}
#| eval: false
nombre <- function(x, y) {
  producto <- # c√≥digo de la multiplicaci√≥n
  return()
}
producto(3)
producto(3, -7)
```

```{r}
#| code-fold: true
#| eval: false
funcion_producto <- function(x, y = x) {
  producto <- x * y
  return(producto)
}
funcion_producto(3)
funcion_producto(3, -7)
```


### [**Ej 3**]{.hl-yellow}

üìù Define una funci√≥n llamada `igualdad_nombres` que, dados dos nombres, nos diga si son iguales o no. Hazlo considerando importantes las may√∫sculas, y sin que importen las may√∫sculas. Usa el paquete `{stringr}`.


```{r}
#| code-fold: true
#| eval: false
# Distinguiendo may√∫sculas
igualdad_nombres <- function(persona_1, persona_2) {
  return(persona_1 == persona_2)
}
igualdad_nombres("Javi", "javi")
igualdad_nombres("Javi", "Luc√≠a")

# Sin importar may√∫sculas
igualdad_nombres <- function(persona_1, persona_2) {
  return(toupper(persona_1) == toupper(persona_2))
}
igualdad_nombres("Javi", "javi")
igualdad_nombres("Javi", "Luc√≠a")
```


### [**Ej 4**]{.hl-yellow}

üìù Crea una funci√≥n llamada `calculo_IMC` que, dados dos argumentos (peso y estatura en metros) y un nombre, devuelva una lista con el IMC ($peso/(estatura_m^2)$) y el nombre.


```{r}
#| code-fold: true
#| eval: false
calculo_IMC <- function(nombre, peso, estatura) {
  
  return(list("nombre" = nombre, "IMC" = peso/(estatura^2)))
}
```


### [**Ej 5**]{.hl-yellow}

üìù Repite el ejercicio anterior pero con otro argumento opcional que se llame unidades (por defecto, `unidades = "metros"`). Desarrolla la funci√≥n de forma que haga lo correcto si `unidades = "metros"` y si `unidades = "cent√≠metros"`.


```{r}
#| code-fold: true
#| eval: false
calculo_IMC <- function(nombre, peso, estatura, unidades = "metros") {
  
  return(list("nombre" = nombre,
              "IMC" = peso/(if_else(unidades == "metros", estatura, estatura/100)^2)))
}
```


 
### [**Ej 6**]{.hl-yellow}

üìù Crea un tibble ficticio de 7 personas, con tres variables (inventa nombre, y simula peso, estatura en cent√≠metros), y aplica la funci√≥n definida de forma que obtengamos una cuarta columna con su IMC.


```{r}
#| code-fold: true
#| eval: false
datos <-
  tibble("nombres" = c("javi", "sandra", "laura",
                       "ana", "carlos", "leo", NA),
         "peso" = rnorm(n = 7, mean = 70, sd = 1),
         "estatura" = rnorm(n = 7, mean = 168, sd = 5))

datos |> 
  mutate(IMC = calculo_IMC(nombres, peso, estatura, unidades = "cent√≠metros")$IMC)
```



### [**Ej 7**]{.hl-yellow}

üìù Crea una funci√≥n llamada `atajo` que tenga dos argumentos num√©ricos `x` e `y`. Si ambos son iguales, debes devolver `"iguales"` y hacer que la funci√≥n acaba autom√°ticamente (piensa cu√°ndo una funci√≥n sale). OJO: `x` e `y` podr√≠an ser vectores. Si son distintos (de igual de longitud) calcula la proporci√≥n de elementos  diferentes. Si son distintos (por ser distinta longitud), devuelve los elementos que no sean comunes.


```{r}
#| code-fold: true
#| eval: false
atajo <- function(x, y) {
  
  if (all(x == y) & length(x) == length(y)) { return("iguales") }
  else {
   
    if (length(x) == length(y)) {
      
      n_diff <- sum(x != y) / length(x)
      return(n_diff)
      
    } else {
      
      diff_elem <- unique(c(setdiff(x, y), setdiff(y, x)))
      return(diff_elem)
    }
    
  }
}
```



:::


---


## R base vs Tidyverse

Hasta ahora todo lo que hemos repasado en `R` lo hemos realizado en el paradigma de programaci√≥n conocido como [**R base**]{.hl-yellow}. Y es que cuando `R` naci√≥ como lenguaje, muchos de los que programaban en √©l imitaron formas y metodolog√≠as heredadas de otros lenguajes, basado en el uso de

-   Bucles [**for**]{.hl-yellow}

-   Bucles [**while**]{.hl-yellow}

-   Estructuras [**if-else**]{.hl-yellow}


Y aunque conocer dichas estructuras puede sernos en algunos casos interesantes, en la [**mayor√≠a de ocasiones han quedado caducas y vamos a poder evitarlas**]{.hl-red} (en especial los bucles) ya que `R` est√° especialmente [**dise√±ado para trabajar de manera funcional**]{.hl-yellow} (en lugar de elemento a elemento).

---

## ¬øQu√© es tidyverse?

::: columns
::: {.column width="50%"}
![](img/tidyverrse_universe.jpg)
:::

::: {.column width="50%"}
![](img/flow_tidyverse.jpg)
:::
:::

En ese contexto de programaci√≥n funcional, hace una d√©cada nac√≠a `{tidyverse}`, un [**¬´universo¬ª de paquetes**]{.hl-yellow} para garantizar un flujo de trabajo eficiente, coherente y lexicogr√°ficamente sencillo de entender, basado en la idea de que [**nuestros datos est√°n limpios y ordenados (tidy)**]{.hl-purple}

---

## ¬øQu√© es tidyverse?

::: columns
::: {.column width="45%"}
![](img/tidyverrse_universe.jpg)

-   `{lubridate}` manejo de fechas
-   `{rvest}`: web scraping
-   `{tidymodels}`: modelizaci√≥n/predicci√≥n

:::

::: {.column width="55%"}
-   `{tibble}`: optimizando data.frame
-   `{tidyr}`: limpieza de datos
-   `{readr}`: carga datos rectangulares (.csv), `{readxl}` para importar archivos .xls y .xlsx
-   `{dplyr}`: gram√°tica para depurar
-   `{stringr}`: manejo de textos
-   `{purrr}`: manejo de listas
-   `{forcats}`: manejo de cualitativas
-   `{ggplot2}`: visualizaci√≥n de datos



:::
:::



---

## ¬øQu√© es tidyverse?

::: columns
::: {.column width="45%"}
![](img/tidyverrse_universe.jpg)

-   `{lubridate}` manejo de fechas
-   `{rvest}`: web scraping
-   `{tidymodels}`: modelizaci√≥n/predicci√≥n


:::

::: {.column width="55%"}
-   `{tibble}`: [**optimizando data.frame**]{.hl-yellow}
-   `{tidyr}`: [**limpieza de datos**]{.hl-yellow}
-   `{readr}`: carga datos rectangulares (.csv), `{readxl}` para importar archivos .xls y .xlsx
-   `{dplyr}`: gram√°tica para depurar
-   `{stringr}`: manejo de textos
-   `{purrr}`: manejo de listas
-   `{forcats}`: manejo de cualitativas
-   `{ggplot2}`: visualizaci√≥n de datos

:::
:::


---

## Filosof√≠a base: tidy data

> Tidy datasets are all alike, but every messy dataset is messy in its own way (Hadley Wickham, Chief Scientist en RStudio)

::: {style="font-size:120px; text-align: center; color:#F8DF58;"}
<b>TIDY</b><b>[VERSE</b>]{style="color:#CAB0EE;"}
:::

El [**universo**]{.hl-purple} de paquetes `{tidyverse}` se basa en la idea introducida por **Hadley Wickham** (el Dios al que rezamos) de [**estandarizar**]{.hl-yellow} el formato de los datos para

::: incremental
-   [**sistematizar**]{.hl-green} la depuraci√≥n
-   hacer m√°s [**sencillo**]{.hl-green} su manipulaci√≥n.
-   c√≥digo [**legible**]{.hl-green}
:::

---

## Reglas del tidy data

Lo primero por tanto ser√° entender qu√© son los [**conjuntos tidydata**]{.hl-yellow} ya que todo `{tidyverse}` se basa en que los datos est√°n estandarizados.

::: columns
::: {.column width="50%"}
::: {.fragment .fade-in}
1.  Cada [**variable**]{.hl-yellow} en una [**√∫nica columna**]{.hl-purple}
:::

::: {.fragment .fade-in}
2.  Cada [**individuo**]{.hl-yellow} en una [**fila diferente**]{.hl-purple}
:::

::: {.fragment .fade-in}
3.  Cada [**celda**]{.hl-yellow} con un [**√∫nico valor**]{.hl-purple}
:::

::: {.fragment .fade-in}
4.  Cada [**dataset**]{.hl-yellow} en un [**tibble**]{.hl-purple}
:::

::: {.fragment .fade-in}
5.  Si queremos cruzar [**m√∫ltiples tablas**]{.hl-yellow} debemos tener una [**columna com√∫n**]{.hl-purple}
:::
:::

::: {.column width="50%"}
![](img/tidy_def.jpg){width="160%"}
:::
:::

---

## Tuber√≠a (pipe)

En `{tidyverse}` ser√° clave el [**operador pipe (tuber√≠a)**]{.hl-yellow} definido como `|>` ([**ctrl+shift+M**]{.hl-purple}): ser√° una [**tuber√≠a que recorre los datos**]{.hl-yellow} y los transforma.

. . .

::: columns
::: {.column width="50%"}
En R base, si queremos aplicar tres funciones `first()`, `second()` y `third()` en orden, ser√≠a


```{r}
#| eval: false
third(second(first(datos)))
```

:::

::: {.column width="50%"}
En `{tidyverse}` podremos [**leer de izquierda a derecha**]{.hl-yellow} y separar los datos de las acciones


```{r}
#| eval: false
datos |> first() |> second() |> third()
```

:::
:::


. . .

::: callout-caution
## Apunte importante

Desde la versi√≥n 4.1.0 de `R` disponemos de `|>`, un pipe **nativo** disponible [**fuera de tidyverse**]{.hl-purple}, sustituyendo al [**antiguo pipe**]{.hl-red} `%>%` que depend√≠a del paquete `{magrittr}` (bastante problem√°tico).
:::

---

## Tuber√≠a (pipe)

La principal ventaja es que el [**c√≥digo sea muy legible (casi literal)**]{.hl-yellow} pudiendo hacer grandes operaciones con los datos con apenas c√≥digo.


::: columns
::: {.column width="50%"}

```{r}
#| eval: false
datos |>
  limpio(...) |>
  filtro(...) |>
  selecciono(...) |>
  ordeno(...) |>
  modifico(...) |>
  renombro(...) |>
  agrupo(...) |>
  cuento(...) |>
  resumo(...) |>
  pinto(...)
```

:::

::: {.column width="50%"}
<center><img src="img/logo_pipe.png" width="360px"/></center>
:::
:::

---

## Datos SUCIOS: messy data

¬øPero qu√© aspecto tienen los [**datos no tidy**]{.hl-yellow}? Vamos a cargar la tabla `table4a` del paquete `{tidyr}` (ya lo tenemos cargado del entorno tidyverse).



```{r}
library(tidyr)
table4a
```



[**¬øQu√© puede estar fallando?**]{.hl-red}

---

## Pivotar: pivot_longer()

::: columns
::: {.column width="40%"}

```{r}
table4a
```

:::

::: {.column width="60%"}
‚ùé Cada [**fila representa dos observaciones**]{.hl-red} (1999 y 2000) ‚Üí las columnas `1999` y `2000` en realidad deber√≠an ser en s√≠ [**valores de una variable**]{.hl-yellow} y no nombres de columnas.
:::
:::

. . .

Incluiremos una [**nueva columna**]{.hl-yellow} que nos guarde el a√±o y otra que guarde el valor de la variable de inter√©s en cada uno de esos a√±os. Y lo haremos con la funci√≥n `pivot_longer()`: [**pivotaremos la tabla**]{.hl-yellow} a formato long:


```{r}
table4a |> 
  pivot_longer(cols = c("1999", "2000"), names_to = "year", values_to = "cases")
```


---

## Pivotar: pivot_longer()

::: columns
::: {.column width="50%"}

```{r}
table4a |> 
  pivot_longer(cols = c("1999", "2000"),
               names_to = "year",
               values_to = "cases")
```

:::

::: {.column width="50%"}
![](img/table4a.jpg)
:::
:::



-   `cols`: [**nombre de las variables a pivotar**]{.hl-yellow}
-   `names_to`: nombre de la nueva variable a la quemandamos la [**cabecera**]{.hl-yellow} de la tabla (los nombres).
-   `values_to`: nombre de la nueva variable a la que vamos a mandar los [**datos**]{.hl-yellow}.

---

## Datos SUCIOS: messy data

Veamos otro ejemplo con la tabla `table2`



```{r}
table2
```



[**¬øQu√© puede estar fallando?**]{.hl-red}

---

## Pivotar: pivot_wider()

::: columns
::: {.column width="60%"}

```{r}
#| echo: false
table2
```

:::

::: {.column width="40%"}
‚ùé Cada [**observaci√≥n est√° dividido en dos filas**]{.hl-red} ‚Üí los [**registros con el mismo a√±o deber√≠an ser el mismo**]{.hl-yellow}
:::
:::

. . .

Lo que haremos ser√° lo opuesto: con `pivot_wider()` [**ensancharemos la tabla**]{.hl-yellow}


```{r}
table2 |>  pivot_wider(names_from = type, values_from = count)
```


---

## Datos SUCIOS: messy data

Veamos otro ejemplo con la tabla `table3`



```{r}
table3
```



[**¬øQu√© puede estar fallando?**]{.hl-red}

---

## Separar: separate()

::: columns
::: {.column width="60%"}

```{r}
table3
```

:::

::: {.column width="40%"}
‚ùé Cada [**celda contiene varios valores**]{.hl-red}
:::
:::

. . .

Lo que haremos ser√° hacer uso de la funci√≥n `separate()` para mandar [**separar cada valor**]{.hl-yellow} a una columna diferente.


```{r}
table3 |> separate(rate, into = c("cases", "pop"))
```


---

## Separar: separate()


```{r}
table3 |> separate(rate, into = c("cases", "pop"))
```


F√≠jate que los datos, aunque los ha separado, [**los ha mantenido como texto**]{.hl-red} cuando en realidad deber√≠an ser variables num√©ricas. Para ello podemos a√±adir el argumento opcional `convert = TRUE`

. . .


```{r}
table3 |> separate(rate, into = c("cases", "pop"), convert = TRUE)
```


---

## Datos SUCIOS: messy data

Veamos el √∫ltimo ejemplo con la tabla `table5`



```{r}
table5
```



[**¬øQu√© puede estar fallando?**]{.hl-red}

---

## Unir unite()

::: columns
::: {.column width="50%"}

```{r}
table5
```

:::

::: {.column width="50%"}
‚ùé Tenemos [**mismos valores divididos en dos columnas**]{.hl-red}
:::
:::

. . .

Usaremos `unite()` para [**unir los valores**]{.hl-yellow} de siglo y a√±o en una misma columna


```{r}
table5 |> unite(col = year_completo, century, year, sep = "")
```


---

## Ejemplo: relig_income

Vamos a realizar un ejemplo juntos con la tabla `relig_income` del paquete `{tidyr}`. Como se indica en la ayuda `? relig_income`, la tabla representa la cantidad de personas que hay en cada tramo de ingresos anuales (20k = 20 000$) y en cada religi√≥n.


```{r}
relig_income
```


---

## Ejemplo: relig_income


```{r}
relig_income
```


[**¬øEs tidydata?**]{.hl-yellow} 

---

## Ejemplo: relig_income


```{r}
relig_income
```


No lo es ya que en realidad [**solo deber√≠amos tener una variable de ingresos**]{.hl-red} y la tenemos dividida en 11: todas ellas es la misma variable solo que adopta un valor diferente.  [**¬øC√≥mo convertirla a tidy data?**]{.hl-green}

---

## Ejemplo: relig_income


La idea es [**pivotar todas las columnas de ingresos**]{.hl-yellow} para que acaben en una sola columna llamada `income`, y los valores (el n√∫mero de personas) en otra llamada `people` (por ejemplo). La tabla la haremos m√°s larga y menos ancha as√≠ que...

. . .


```{r}
relig_tidy <-
  relig_income |>
  pivot_longer(cols = "<$10k":"Don't know/refused", names_to = "income",
               values_to = "people")
relig_tidy 
```


---

## Ejemplo: relig_income

Vamos a hilar m√°s fino: ahora mismo en la variable `income` en realidad tenemos dos valores, el l√≠mite inferior y el superior de la renta. Vamos a [**separar dicha variable e ingresos**]{.hl-yellow} en dos, llamadas `income_inf` y `income_sup`


```{r}
relig_tidy 
```


---

## Ejemplo: relig_income

Vamos a hilar m√°s fino: ahora mismo en la variable `income` en realidad tenemos dos valores, el l√≠mite inferior y el superior de la renta. Vamos a [**separar dicha variable e ingresos**]{.hl-yellow} en dos, llamadas `income_inf` y `income_sup`



```{r}
relig_tidy |>
  # Separamos por -
  separate(income, into = c("income_inf", "income_sup"), sep = "-")
```


. . .

¬øEst√° ya bien? F√≠jate bien...

---

## Ejemplo: relig_income


```{r}
relig_tidy |>
  # Separamos por -
  separate(income, into = c("income_inf", "income_sup"), sep = "-")
```


Si te fijas la primera columna el `"$10k"` deber√≠a ser una cota superior, no inferior. ¬øC√≥mo indicarle que separe bien ese caso?

---

## Ejemplo: relig_income

Le indicaremos que separe si encuentra `"-"` o `"<"` (usamos `|` para separar ambas opciones)


```{r}
relig_tidy |>
  # Separamos por -
  separate(income, into = c("income_inf", "income_sup"), sep = "-|<")
```


---

## Ejemplo: relig_income


```{r}
relig_tidy <-
  relig_tidy |>
  # Separamos por -
  separate(income, into = c("income_inf", "income_sup"), sep = "-|<")
relig_tidy
```


Piensa ahora como podemos [**convertir los l√≠mites de ingresos a num√©ricas (eliminando s√≠mbolos, letras, etc)**]{.hl-yellow}

---

## Ejemplo: relig_income

Para ello usaremos el paquete `{stringr}`, en concreto la funci√≥n `str_remove_all()`, a la que le podemos pasar los caracteres que queremos eliminar (f√≠jate que `$` al ser un caracter reservado en `R` hay que indic√°rselo con `\\$`)


```{r}
relig_tidy$income_inf <-
  str_remove_all(relig_tidy$income_inf, "\\$|>|k")
relig_tidy$income_sup <-
  str_remove_all(relig_tidy$income_sup, "\\$|>|k")

relig_tidy
```


---

## Ejemplo: relig_income

F√≠jate que tenemos `"Don't now/refused"`. ¬øQu√© deber√≠amos tener?

. . .

Deber√≠a ser un [**dato ausente**]{.hl-yellow} as√≠ que usaremos `if_else()`: si contiene dicha frase, `NA`, en caso contrario su valor (consejo: `str_detect()` para detectar patrones en textos, y evitar tener que escribir toda la palabra sin errores)


```{r}
relig_tidy$income_inf <-
  if_else(str_detect(relig_tidy$income_inf, "refused"), NA, relig_tidy$income_inf)
relig_tidy$income_sup <-
  if_else(str_detect(relig_tidy$income_sup, "refused"), NA, relig_tidy$income_sup)
relig_tidy
```


---

## Ejemplo: relig_income


```{r}
relig_tidy
```


En la primera l√≠nea, ese `""` tambi√©n deber√≠a ser `NA``


```{r}
relig_tidy$income_inf <-
  if_else(relig_tidy$income_inf == "", NA, relig_tidy$income_inf)
relig_tidy$income_suop <-
  if_else(relig_tidy$income_sup == "", NA, relig_tidy$income_sup)
```


---

## Ejemplo: relig_income


```{r}
relig_tidy
```


Adem√°s si te fijas los n√∫meros son en realidad caracteres, as√≠ que vamos a [**convertirlos a n√∫meros**]{.hl-yellow}



---

## Ejemplo: relig_income


Adem√°s si te fijas los n√∫meros son en realidad caracteres, as√≠ que vamos a [**convertirlos a n√∫meros**]{.hl-yellow}


```{r}
relig_tidy$income_inf <- as.numeric(relig_tidy$income_inf)
relig_tidy$income_sup <- as.numeric(relig_tidy$income_sup)
relig_tidy
```


---

## Ejemplo: relig_income

¬øSe te ocurre alguna forma de **¬´cuantificar num√©ricamente¬ª** los valores ausentes que tenemos en este caso?

. . .

Si te fijas en realidad cuando hay ausente en el l√≠mite inferior en realidad podr√≠amos poner un 0 (nadie puede ganar menos de eso) y cuando lo tenemos en el l√≠mite superior ser√≠a `Inf`


```{r}
relig_tidy$income_inf <-
  if_else(is.na(relig_tidy$income_inf), 0, relig_tidy$income_inf)
relig_tidy$income_sup <-
  if_else(is.na(relig_tidy$income_sup), Inf, relig_tidy$income_sup)
relig_tidy
```


---

## Ejemplo: relig_income

Aunque nos haya llevado un rato este es el **c√≥digo completo resumido**


```{r}
#| eval: false
relig_tidy <-
  relig_income |>
  pivot_longer(cols = "<$10k":"Don't know/refused", names_to = "income",
               values_to = "people") |>
  separate(income, into = c("income_inf", "income_sup"), sep = "-|<")

relig_tidy$income_inf <- str_remove_all(relig_tidy$income_inf, "\\$|>|k")
relig_tidy$income_sup <- str_remove_all(relig_tidy$income_sup, "\\$|>|k")

relig_tidy$income_inf <-
  if_else(str_detect(relig_tidy$income_inf, "refused") |
            relig_tidy$income_inf == "", 0, as.numeric(relig_tidy$income_inf))
relig_tidy$income_sup <-
  if_else(str_detect(relig_tidy$income_sup, "refused") |
            relig_tidy$income_sup == "", Inf, as.numeric(relig_tidy$income_sup))
```


---

## Ejemplo: relig_income

¬øPor qu√© era [**importante tenerlo en tidydata**]{.hl-yellow}? Lo veremos m√°s adelante al visualizar los datos pero esto ya nos permite realizar filtros muy r√°pidos con muy poco c√≥digo.

Por ejemplo: ¬øcu√°ntas personas agn√≥sticas con ingresos superiores (o iguales) a 30 tenemos?

. . .


```{r}
# una l√≠nea de c√≥digo
sum(relig_tidy$people[relig_tidy$religion == "Agnostic" & relig_tidy$income_inf >= 30])
```


---

## üíª Tu turno {#tu-turno-1-5}

[**Intenta realizar los siguientes ejercicios sin mirar las soluciones**]{style="color:#444442;"}

::: panel-tabset

### [**Ejercicio 1**]{.hl-yellow}

üìù Usa el dataset original `relig_income` y trata de responder a la √∫ltima pregunta:  ¬øcu√°ntas personas agn√≥sticas con ingresos superiores (o iguales) a 30 tenemos? Compara el c√≥digo a realizar cuando tenemos tidydata a cuando no. ¬øCu√°l es m√°s legible si no supieses `R`? ¬øCu√°l tiene mayor probabilidad de error?


```{r}
#| code-fold: true
#| eval: false

sum(relig_income[relig_income$religion == "Agnostic",
             c("$30-40k", "$40-50k", "$50-75k", "$75-100k", "$100-150k", ">150k")])
```



### [**Ejercicio 2**]{.hl-yellow}

üìù Usando `relig_tidy` determina qui√©n tiene m√°s ingresos medios, ¬øcat√≥licos (`Catholic`) o agn√≥sticos (`Agnostic`)? Crea antes una variable `avg_income` (ingresos medios por intervalo): si hay 5 personas entre 20 y 30, y 3 personas entre 30 y 50, la media ser√≠a $(25*5 + 40*3)/8$ (si es `Inf` por arriba, `NA`)


```{r}
#| code-fold: true
#| eval: false
relig_tidy$avg_income <- 
  if_else(is.infinite(relig_tidy$income_sup), NA, (relig_tidy$income_sup + relig_tidy$income_inf)/2)

# Agnosticos vs catolicos
sum((relig_tidy$avg_income[relig_tidy$religion == "Agnostic"] * relig_tidy$people[relig_tidy$religion == "Agnostic"]), na.rm = TRUE) /
  sum(relig_tidy$people[relig_tidy$religion == "Agnostic"], na.rm = TRUE)

sum((relig_tidy$avg_income[relig_tidy$religion == "Catholic"] * relig_tidy$people[relig_tidy$religion == "Catholic"]), na.rm = TRUE) /
  sum(relig_tidy$people[relig_tidy$religion == "Catholic"], na.rm = TRUE)
```



### [**Ejercicio 3**]{.hl-yellow}

üìù Si debemos elegir budismo (`Buddhist`) e hinduismo (`Hindu`), ¬øcu√°l de las dos es la religi√≥n mayoritaria entre los que ganan m√°s de 50 000$ anuales?


```{r}
#| code-fold: true
#| eval: false

greatest_income <-
  relig_tidy[relig_tidy$income_inf >= 50 & relig_tidy$religion %in% c("Buddhist", "Hindu"), ]

sum(greatest_income$people[greatest_income$religion == "Buddhist"], na.rm = TRUE)
sum(greatest_income$people[greatest_income$religion == "Hindu"], na.rm = TRUE)
```



### [**Ejercicio 4**]{.hl-yellow}

üìù Echa un vistazo a la tabla `table4b` del paquete `{tidyr}`. ¬øEs tidydata? En caso negativo, ¬øqu√© falla? ¬øC√≥mo convertirla a tidy data en caso de que no lo sea ya?


```{r}
#| code-fold: true
#| eval: false
table4b |>
  pivot_longer(cols = "1999":"2000", names_to = "year",
               values_to = "cases")
```



### [**Ejercicio 5**]{.hl-yellow}

üìù Echa un vistazo a la tabla `billboard` del paquete `{tidyr}`. ¬øEs tidydata? En caso negativo, ¬øqu√© falla? ¬øC√≥mo convertirla a tidy data en caso de que no lo sea ya?


```{r}
#| code-fold: true
#| eval: false
billboard |>
  pivot_longer(cols = "wk1":"wk76",
               names_to = "week",
               names_prefix = "wk",
               values_to = "position",
               values_drop_na = TRUE)
```


:::

---

## üê£ Caso pr√°ctico III {#caso-practico-1-3}

En el paquete `{tidyr}` contamos con el dataset `who2` (dataset de la Organizaci√≥n Mundial de la Salud). Intenta responder a las preguntas planteadas en el [**workbook**](https://javieralvarezliebana.quarto.pub/repaso-r-ts/).


```{r}
who2
```


# Clase 3: [introducci√≥n a series]{.flow} {#clase-3}

[**Introducci√≥n al an√°lisis descriptivo de series temporales**]{style="color:#444442;"}


* [üìÜ Planificaci√≥n](#planificacion)

---

## ¬øQu√© es una serie temporal?

Como ya hemos visto, una [**serie temporal**]{.hl-yellow} se puede definir de manera informal como una [**muestra de una variable**]{.hl-yellow} (usualmente continua) recogida de manera [**secuencial en el tiempo**]{.hl-purple}


```{r}
#| code-fold: true
ggplot(retiro) +
  geom_line(aes(x = fecha, y = tmed), linewidth = 0.3, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## Un poco de historia

Las [**primeras series temporales**]{.hl-yellow} aparecieron en el siglo XIX, cuando el matem√°tico **Laplace** (matem√°tico, profesor de Napole√≥n y luego ministro) se dedicaba a estudiar la [**relaci√≥n entre las fases de La Luna y la presi√≥n**]{.hl-yellow} atmosf√©rica (por el nivel del mar).

. . .

Aunque su primera aproximaci√≥n fue un poco chapuza (intent√≥ ajustar una funci√≥n seno sin tener en cuenta el tiempo), [**Arthur Schuster decidi√≥ aplicar los trabajos de Fourier para que dichas sinusoidales dependiesen del tiempo**]{.hl-yellow} (Fourier hab√≠a demostrado que toda funci√≥n peri√≥dica pod√≠a descomponerse como suma de senos y cosenos).

---

## Un poco de historia

Treinta a√±os m√°s tarde, [**Yule y Slutsky aplicaron las ideas de la regresi√≥n**]{.hl-yellow} desarrollados por Galton y Pearson al estudio de procesos cuya variable regresora es ella misma en otro instante temporal (**procesos autoregresivos**), aunque no fue hasta la llegada de **Kolmogorov** cuando se formaliz√≥ su definici√≥n matem√°tica en el contexto de los [**procesos estoc√°sticos**]{.hl-yellow}.

. . .

Tras acabar la Segunda Guerra Mundial quedaron desclasificados algunos trabajos de Wiener, Kolmogorov, Bartlett y Tukey sobre la predicci√≥n de series temporales, as√≠ como su estudio  en funci√≥n del an√°lisis de las [**correlaciones**]{.hl-yellow}. Tras los trabajos de alisado de Holt y Winter en los a√±os 60, en 1970 [**Box y Jenkins publican ¬´La Biblia¬ª de las series temporales**]{.hl-yellow}, un manual donde se presenta una metodolog√≠a para la identificaci√≥n, estimaci√≥n y predicci√≥n de series temporales (los conocidos como **procesos SARIMA**)


---

## M√©todos de an√°lisis


* [**M√©todos descriptivos o cl√°sicos**]{.hl-yellow}: desarrollados entre 1940 y 1970, est√°n enfocadas principalmente en la [**estimaci√≥n de los valores de la serie**]{.hl-yellow}, siendo bastante [**malos en la predicci√≥n futura**]{.hl-red}.

  * [**M√©todos de decomposici√≥n**]{.hl-purple} (tendencia-estacionalidad-error):
    * Suavizado cl√°sico
    * STL (seasonal-trend decomposition procedure based on loess)
  * [**M√©todos de alisado**]{.hl-purple} (dependencia del pasado disminuye con el tiempo):
    * Alisado exponencial simple
    * Alisado doble de Holt y triple de Holt-Winters
. . .

* [**M√©todos probabil√≠sticos**]{.hl-yellow}: enmarcados dentro del an√°lisis de los procesos estoc√°sticos, se considera que la serie temporal observada es solo una muestra de un proceso estoc√°stico. Necesitaremos **hip√≥tesis**.

---

## M√©todos descriptivos

Los [**m√©todos descriptivos o cl√°sicos**]{.hl-yellow} se basan en entender, de manera casi emp√≠rica, el comportamiento de la serie, pareci√©ndose m√°s una interpolaci√≥n que a una metodolog√≠a rigurosa de estimaci√≥n y predicci√≥n.

&nbsp;

Vamos a empezar denotando a la serie como $X = \left\lbrace X_t \right\rbrace_{t}$, de la cual observamos una **muestra** $\left(x_0, \ldots, x_{T} \right)$.

. . .

¬øCu√°l se te ocurre que ser√≠a el [**caso m√°s sencillo de serie temporal**]{.hl-yellow}? 

---

## Descomposici√≥n cl√°sica: sin tendencia

El caso m√°s sencillo es considerar que la serie es [**completamente aleatoria**]{.hl-yellow}, es decir $X_t = \varepsilon_t$.

. . .

Al t√©rmino $\left\lbrace \varepsilon_t \right\rbrace_{t \in T}$ le llamaremos [**error o innovaci√≥n**]{.hl-purple} y, dado que se supone que no captura ning√∫n patr√≥n y que la serie debe [**ser finita**]{.hl-yellow}, tendr√≠amos

$$X_t = \varepsilon_t, \quad {\rm E} \left[X_t \right] = {\rm E} \left[\varepsilon_t \right] = 0, \quad  {\rm Var} \left[\varepsilon_t \right] = \sigma_{\varepsilon}^{2} = cte < \infty$$
donde normalmente el error sigue una distribuci√≥n normal de varianza finita.


. . .

Adem√°s dicho t√©rmino de error cumplir√° que el [**pasado no proporciona ning√∫n tipo de informaci√≥n sobre el futuro**]{.hl-yellow}, es decir, 

$$\varepsilon_{t+1} | \left(\varepsilon_t, \varepsilon_{t-1}, \ldots, \varepsilon_0 \right)  \sim \varepsilon_{t+1} \sim N \left(0, \sigma_{\varepsilon}^{2} \right)$$

---

## Sin tendencia

üíª ¬øC√≥mo podr√≠amos simular dicha serie temporal? 


. . .

1. **Paso 1**: construye un tibble de 5 columnas, donde la primera columna contenga los valores $t=1, 2, \ldots, 1000$; y donde la segunda columna contenga valores simulados seg√∫n una normal $N(0, \sigma = 0.5)$, la tercera con $\sigma = 1$, la cuarta con $\sigma = 2$ y la quinta con $\sigma = 4$.


```{r}
#| code-fold: true
n <- 1000
datos <-
  tibble("t" = 1:n,
         "sd_05" = rnorm(n, mean = 0, sd = 0.5), "sd_1" = rnorm(n, mean = 0, sd = 1),
         "sd_2" = rnorm(n, mean = 0, sd = 2), "sd_4" = rnorm(n, mean = 0, sd = 4))
```


---

## Sin tendencia

2. **Paso 2**: haz un gr√°fico (¬øcu√°l har√≠as?) solo considerando $t$ y la primera serie `sd_05`


. . .


```{r}
#| code-fold: true
ggplot(datos) +
  geom_line(aes(x = t, y = sd_05)) +
  theme_minimal() +
  labs(x = "t", y = "X_t",
       title = "Serie temporal X_t = eps_t con sd = 0.5")
```


---

## Sin tendencia


3. **Paso 3**: ¬øc√≥mo deber√≠amos de transformar los datos para poder pintar todas las series a la vez?  Hazte un borrador de c√≥mo ser√≠a el c√≥digo de ggplot para dibujarlo.

. . .

La idea es que si tenemos $p$ series, en lugar de tener $p$ variables distintas, tengamos una serie ¬´debajo de¬ª otra. Por ejemplo, vamos a definir la primera y pongamos debajo al segunda otra.


```{r}
n <- 1000
datos <- tibble("t" = 1:n, "X_t" = rnorm(n, mean = 0, sd = 0.5), "sd" = "sd_0.5")
datos_tidy <- 
  rbind(datos, tibble("t" = 1:n, "X_t" = rnorm(n, mean = 0, sd = 1), "sd" = "sd_1"))
datos_tidy
```


---

## Sin tendencia

El resto las iremos concatenando de la misma manera, a√±adiendo filas al `datos_tidy` que ya tenemos.


```{r}
datos_tidy <- 
  rbind(datos_tidy, tibble("t" = 1:n, "X_t" = rnorm(n, mean = 0, sd = 2), "sd" = "sd_2"))

datos_tidy <- 
  rbind(datos_tidy, tibble("t" = 1:n, "X_t" = rnorm(n, mean = 0, sd = 4), "sd" = "sd_4"))
```


. . .

Lo anterior se pueda hacer m√°s "conciso" con `{tidyverse}` haciendo uso de `pivot_longer()`.


```{r}
#| eval: false
datos_tidy <-
  datos |>
  pivot_longer(cols = "sd_05":"sd_4", names_to = "sd", values_to = "X_t")
```


---


## Sin tendencia



```{r}
#| code-fold: true
ggplot(datos_tidy) +
  geom_line(aes(x = t, y = X_t, color = sd),
            alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~sd) +
  theme_minimal() +
  labs(x = "t", y = "X_t", color = "Desv. t√≠pica",
       title = "Serie temporal X_t = eps_t con distintas varianzas")
```




---

## Sin tendencia

4. **Paso 4**. Para automatizarlo, dise√±a una **funci√≥n** tal que le introduzcas como argumento un tama√±o muestral $n$, un $t$ y un vector de desviaciones t√≠picas, y devuelva en formato tidy data los valores de las series temporales (tantas series como longitud tenga el vector de desviaciones)


```{r}
#| code-fold: true
time_series_error <- function(n, t = 1:n, sd_vec = 1) {
  
  datos_tidy <- tibble()
  
  for (i in 1:length(sd_vec)) {
    
    datos_tidy <- 
      datos_tidy |>
      rbind(datos_tidy,
            tibble("t" = t, "sd" = glue::glue("sd_{sd_vec[i]}"),
                   "X_t" = rnorm(n, mean = 0, sd = sd_vec[i])))
  }
  return(datos_tidy)
}
```


---

## Sin tendencia

Esta serie temporal $X_t = \varepsilon_t$ es la m√°s sencilla que podemos imaginar y [**no podemos predecirla**]{.hl-red} ya que no hay ning√∫n tipo de patr√≥n determin√≠stico que podamos capturar.


```{r}
#| code-fold: true
datos <- time_series_error(n = 1000, sd = c(0.5, 1, 2, 4))
ggplot(datos) +
  geom_line(aes(x = t, y = X_t, color = sd), alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~sd) +
  theme_minimal() +
  labs(x = "t", y = "X_t", color = "Desv. t√≠pica",
       title = "Serie temporal X_t = eps_t con distintas varianzas")
```


# Clase 4: [simulaci√≥n error + tendencia]{.flow} {#clase-4}

[**Introducci√≥n al an√°lisis descriptivo de series temporales**]{style="color:#444442;"}


* [üíª Ejercicios de simulaci√≥n](#tu-turno-4-1)
* [üìÜ Planificaci√≥n](#planificacion)


---

## Con tendencia

Normalmente una serie temporal suele ser m√°s complejo y lleva al menos incorporada una [**componente de tendencia o nivel $\mu_t$**]{.hl-yellow} tal que

$$X_t = f \left(\mu_t, \varepsilon_t\right) =^{*} \mu_t + \varepsilon_t, \quad {\rm E} \left[X_t \right] = \mu_t$$
$*$ De momento estamos considerando una [**descomposici√≥n aditiva**]{.hl-yellow}

&nbsp;

F√≠jate que ahora ${\rm E} \left[X_t \right] = \mu_t$ ya que la esperanza de la parte aleatoria (ruido) ser√° asumida siempre nula: $\mu_t$ es el nivel de la serie respecto a la que oscila en el infinito.

---


## Con tendencia


$$X_t = f \left(\mu_t, \varepsilon_t\right) =^{*} \mu_t + \varepsilon_t, \quad {\rm E} \left[X_t \right] = \mu_t$$

Dicha tendencia $\mu_t$ puede ser a su vez modelada en funci√≥n de $t$ y de un vector de par√°metros $\beta$ tal que $\mu_t := f \left(t, \beta \right)$. Esa funci√≥n $f \left( \cdot \right)$ puede ser cualquier funci√≥n que se te ocurre pero algunas de las tendencias m√°s habituales son:

- [**Constante**]{.hl-purple}: $\mu_t = \beta = \beta_0 = cte$ 

- [**Lineal**]{.hl-purple}: $\mu_t = \beta_0 + \beta_1 t$ 

- [**Polin√≥mica (no lineal)**]{.hl-purple}: $\mu_t = \beta_0 + \beta_1 t + \ldots + \beta_r t^{r}$ 

- [**No polin√≥mica**]{.hl-purple}: $\mu_t = \sin \left(\pi t \right)$ 


---

## Con tendencia

- [**Constante**]{.hl-purple}: $\mu_t = \beta = \beta_0 = cte$ 


$$X_t = f \left(\mu_t, \varepsilon_t\right) = \beta_0  + \varepsilon_t , \quad {\rm E} \left[X_t \right] = \beta_0, \quad \widehat{X}_{t + k} = \widehat{\beta}_0$$


```{r}
#| echo: false
trend <- 3
datos <-
  time_series_error(n = 1000, sd = 1) |> 
  mutate(X_t = X_t + trend)
ggplot(datos) +
  geom_line(aes(x = t, y = X_t), alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t",
       title = "Serie temporal X_t = mu_t + eps_t con mu_t = 3")
```


---

## Con tendencia

- [**Lineal**]{.hl-purple}: $\mu_t = \beta_0 + \beta_1 t$ 


$$X_t = f \left(\mu_t, \varepsilon_t\right) = \beta_0  + \beta_1 t + \varepsilon_t , \quad {\rm E} \left[X_t \right] = \beta_0  + \beta_1 t \to \pm \infty, \quad \widehat{X}_{t + k} = \widehat{\beta}_0 + \widehat{\beta}_1 \left(t + k \right)$$


```{r}
#| echo: false
trend <- c(-3, 0.01)
datos <-
  time_series_error(n = 1000, sd = 1) |> 
  mutate(X_t = X_t + trend[1] + trend[2]*t)
ggplot(datos) +
  geom_line(aes(x = t, y = X_t), alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t",
       title = "Serie temporal X_t = mu_t + eps_t con mu_t = -3 + 0.01*t")
```


---

## Con tendencia

- [**Polin√≥mica (no lineal)**]{.hl-purple}: $\mu_t = \beta_0 + \beta_1 t + \ldots + \beta_r t^{r}$ 

$$X_t = f \left(\mu_t, \varepsilon_t\right) = \beta_0 + \beta_1 t + \ldots + \beta_r t^{r} + \varepsilon_t , \quad \widehat{X}_{t + k} = \widehat{\beta}_0 + \widehat{\beta}_1 \left(t + k \right) + \ldots + \widehat{\beta}_r \left(t + k \right)^r$$


```{r}
#| echo: false
trend <- c(-1, 0.01, 0.000001, -0.00000001)
datos <-
  time_series_error(n = 1000, sd = 1) |> 
  mutate(X_t = X_t + trend[1] + trend[2]*t + trend[3]*t^2 + trend[4]*t^3)
ggplot(datos) +
  geom_line(aes(x = t, y = X_t), alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t",
       title = "Serie temporal X_t = mu_t + eps_t con mu_t = -1 + 0.01*t + 0.000001*t^2 -0.00000001*t^3")
```


---

## Con tendencia

Si nuestra tendencia est√° definida de forma **param√©trica**, para la predicci√≥n de los valores en un tiempo futuro $t + k$ simplemente necesitamos realizar la [**estimaci√≥n del vector de par√°metros $\widehat{\beta}$**]{.hl-yellow}. Para ello recurriremos al [**m√©todo de los minimos cuadrados**]{.hl-yellow}. Por ejemplo, en el caso de tendencia polin√≥mica

$$\widehat{\beta} = \arg \min_{\beta \in \mathbb{R}^{r+1}} \sum_{t = 0}^{T} \left(x_t - \widehat{x}_t \right)^2 = \arg \min_{\beta \in \mathbb{R}^{r+1}} \sum_{t = 0}^{T} \left(x_t - \left(\beta_0 + \beta_1 t \ldots \beta_r t^r \right) \right)^2$$

. . .

Como suele ser habitual, para encontrar el m√≠nimo basta con **derivar respecto a los par√°metros e igualar a 0**. Por ejemplo...

- [**Constante**]{.hl-purple}: $\frac{\partial \sum_{t=0}^{T} \left(x_t - \beta_0 \right)^2}{\partial \beta_0} = T \beta_0 - \sum_{t=0}^{T} x_t = 0$ -> $\widehat{\beta}_0 = \overline{x}_{t=0, ..., T}$


---

## üíª Tu turno {#tu-turno-4-1}

### [**Ejercicio 1**]{.hl-yellow}

üíª Si a√∫n no lo has hecho, haz una funci√≥n llamada `time_series_error` que [**simule una serie temporal solo con error**]{.hl-yellow}. Los argumentos deben ser: tama√±o muestral `n`, un vector temporal `t` y la desv t√≠pica `sd` (debes permitir que pueda ser un vector para simular varias a la vez, ya colocadas en tidydata)



```{r}
#| code-fold: true
#| eval: false
time_series_error <- function(n, t = 1:n, sd_vec = 1) {
  
  datos_tidy <- tibble()
  for (i in 1:length(sd_vec)) {
    datos_tidy <- 
      datos_tidy |>
      rbind(datos_tidy,
            tibble("t" = t, "sd" = glue::glue("sd_{sd_vec[i]}"),
                   "X_t" = rnorm(n, mean = 0, sd = sd_vec[i])))
  }
  return(datos_tidy)
}
time_series_error(n = 100, sd = c(0.5, 2))
```


---

## üíª Tu turno

### [**Ejercicio 2**]{.hl-yellow}


üíª Usando la funci√≥n anterior, define  `time_series_trend_error()` que simule una serie temporal con **tendencia cte** y error, con solo 4 argumentos: `n`, `t`, desviaci√≥n y la constante. Usa dicha funci√≥n y dibuja.


```{r}
#| code-fold: true
time_series_trend_error <- function(n = 1000, t = 1:n, beta_0 = 0, sd = 1) {
  # modo R base
  datos <- time_series_error(n = n, t = t, sd = sd)
  datos$X_t <- datos$X_t + beta_0
  
  # modo tidyverse
  # datos <- time_series_error(n = n, t = t, sd = sd) |> mutate(X_t = X_t + trend)
  return(datos)
}

datos <- time_series_trend_error(n = 1000, beta_0 = 3, sd = 0.5)
ggplot(datos, aes(x = t, y = X_t)) +
  geom_line(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t", title = "Serie temporal X_t = mu_t + eps_t con mu_t = 3")
```


---

## üíª Tu turno

### [**Ejercicio 3**]{.hl-yellow}

üíª Generaliza la funci√≥n anterior para simular una [**serie temporal con error y tendencia lineal**]{.hl-yellow} (donde antes defin√≠amos solo una constante ahora ser√° un vector de coeficientes). F√≠jate que la **l√≠nea de ajuste de ggplot es literal la estimaci√≥n** que har√≠amos si solo consideramos error + tendencia.



```{r}
#| code-fold: true
time_series_trend_error <- function(n = 1000, t = 1:n, beta = c(1, -0.01), sd = 1) {
  
  datos <- time_series_error(n = n, t = t, sd = sd)
  datos$X_t <- datos$X_t + (beta[1] + beta[2]*datos$t)
  return(datos)
}

datos <- time_series_trend_error(n = 1000, beta = c(1, -0.01), sd = 0.5)
ggplot(datos, aes(x = t, y = X_t)) +
  geom_line(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t", title = "Serie temporal X_t = mu_t + eps_t con mu_t = 1 - 0.01*t")
```





# Clase 5: [simulaci√≥n error + tendencia]{.flow} {#clase-5}

[**Introducci√≥n al an√°lisis descriptivo de series temporales**]{style="color:#444442;"}

* [üíª Ejercicios de simulaci√≥n](#tu-turno-5-1)
* [üìÜ Planificaci√≥n](#planificacion)

---

## üíª Tu turno {#tu-turno-5-1}


### [**Ejercicio 4**]{.hl-yellow}


üíª Generaliza la funci√≥n anterior de manera que simule una [**serie temporal con error y tendencia polin√≥mica**]{.hl-yellow} (que acepte un vector de par√°metros general). 


```{r}
#| code-fold: true
time_series_trend_error <-
  function(n = 1000, t = 1:n, beta = c(1, -0.01, 0.001, -0.0001), sd = 1) {
  
  datos <- time_series_error(n = n, t = t, sd = sd)
  for (i in 1:length(beta)) {
    datos$X_t <- datos$X_t + beta[i]*(datos$t^(i - 1))
  } 
  return(datos)
}

datos <- time_series_trend_error(n = 1000, beta = c(1, 0.01, 0.000001, -0.00000001), sd = 0.5)
ggplot(datos, aes(x = t, y = X_t)) +
  geom_line(alpha = 0.7) +
  geom_smooth(formula = y ~ poly(x, 3), se = FALSE) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t", title = "Serie temporal X_t = mu_t + eps_t con mu_t c√∫bica")
```



---


## üíª Tu turno

### [**Ejercicio 5**]{.hl-yellow}

üíª Dise√±a una funci√≥n `estim_ts_trend_error()` que, dada una serie (un tibble de dos columnas `t` y `X_t`), nos devuelva la misma tabla pero con una **tercera columna con su estimaci√≥n** asumiendo una tendencia polin√≥mica (necesitamos dos argumentos: la tabla y el grado del polinomio; haz uso dentro de `poly()`, chequea en la ayuda de la funci√≥n `? poly()`)


```{r}
#| code-fold: true
estim_ts_trend_error <- function(datos, degree = 1) {
  if (degree == 0) {
    modelo <- datos |> lm(formula = X_t ~ 1)
  } else {
    modelo <- datos |> lm(formula = X_t ~ poly(t, degree, raw = TRUE))
  }
  datos$X_hat <- predict(modelo, tibble("t" = datos$t))
  return(datos)
}
datos <- time_series_trend_error(n = 1000, beta = c(1, 0.01, 0.000001, -0.00000001), sd = 0.5)
# ajustamos un modelo polin√≥mico de tendencia
modelo <- datos |> estim_ts_trend_error(degree = 3)
```

```{r}
#| echo: false
ggplot(modelo) +
  geom_line(aes(x = t, y = X_t), alpha = 0.7) +
  geom_line(aes(x = t, y = X_hat), alpha = 0.7, color = "red") +
  theme_minimal() +
  labs(x = "t", y = "X_t", title = "Serie temporal X_t = mu_t + eps_t con mu_t c√∫bica")
```




---

## Caso real: AEMET

Como ya te puedes estar imaginando, esta forma de estimar una serie temporal con un polinomio puede ser [**bastante imprecisa**]{.hl-red}, m√°xime si aparece en nuestra serie una [**componente estacional**]{.hl-yellow} (un patr√≥n peri√≥dico).

Vamos a retomar por ejemplo nuestros **datos de temperatura del AEMET**


```{r}
#| code-fold: true
ggplot(retiro) +
  geom_line(aes(x = fecha, y = tmed), linewidth = 0.3, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```


---

## Caso real: AEMET

üíª Aplica la funci√≥n de estimaci√≥n definida anteriormente a los datos reales del AEMET para incluir 3 nuevas columnas con los 3 m√©todos de estimaci√≥n (tendencia constante, lineal y polin√≥mica de grado 3)

. . .


```{r}
retiro_estim <-
  tibble("fecha" = retiro$fecha, "t" = 1:length(fecha), "X_t" = retiro$tmed) |>
  # aplicamos funci√≥n y renombramos variable de salida de la estimaci√≥n
  estim_ts_trend_error(degree = 0) |> rename(X_t_hat_0 = X_hat) |>
  estim_ts_trend_error(degree = 1) |> rename(X_t_hat_1 = X_hat) |> 
  estim_ts_trend_error(degree = 3) |> rename(X_t_hat_3 = X_hat) 
```



---

## Caso real: AEMET


:::: columns
::: {.column width="25%"}

Como observas las [**predicciones no son precisas cuando hay una componente estacional**]{.hl-red} ya que el ajuste realizado solo se fija en una tendencia con unos **coef ctes**.

&nbsp;


¬øSe te ocurre alguna **idea para mejorar**?

:::

::: {.column width="75%"}


```{r}
#| code-fold: true
ggplot(retiro_estim |>
         pivot_longer(-c(fecha, t), names_to = "type", values_to = "pred")) +
  geom_line(aes(x = fecha, y = pred, color = type),
            linewidth = 0.4, alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```


:::
::::



# Clase 6: [medias m√≥viles]{.flow} {#clase-6}

[**Suavizado por medias m√≥viles**]{style="color:#444442;"}

* [üíª Ejercicios](#tu-turno-6-1)
* [üìÜ Planificaci√≥n](#planificacion)


---

## Recapitulando

Hasta ahora nos hemos centrado sobre todo en tres cosas

* Repasar lo que supi√©ramos de `R`

* Entender c√≥mo simular y estimar series sencillas

* Visualizar dichas series

. . .

Pero a partir de ahora pulsaremos un poco el acelerador (as√≠ qui√©n tenga muchos problemas en la parte de programaci√≥n, deber√° **empezar a usar del mail y las tutor√≠as**)

---

## Recapitulando

¬øQu√© deber√≠amos saber hasta ahora?

* Deber√≠amos tener una funci√≥n `ts_error()` parecida a esta para [**simular un ruido**]{.hl-yellow} de una varianza dada.


```{r}
ts_error <- function(n, t = 1:n, sd_vec = 1) {
  
  datos_tidy <- tibble()
  for (i in 1:length(sd_vec)) {
    datos_tidy <- 
      rbind(datos_tidy,
            tibble("t" = t, "sd" = glue::glue("sd_{sd_vec[i]}"),
                   "X_t" = rnorm(n, mean = 0, sd = sd_vec[i])))
  }
  return(datos_tidy)
}
ts_error(n = 100, sd = c(0.5, 2))
```


---

## Recapitulando

¬øQu√© deber√≠amos saber hasta ahora?

* Deber√≠amos tener una funci√≥n `ts_trend_error` parecida a esta para [**simular una serie formada por tendencia polin√≥mica m√°s ruido**]{.hl-yellow}, con una varianza dada y un vector de coeficientes dado.


```{r}
ts_trend_error <-
  function(n = 1000, t = 1:n, beta = c(1, -0.01, 0.001, -0.0001), sd = 1) {
  
  datos <- ts_error(n = n, t = t, sd = sd)
  for (i in 1:length(beta)) {
    datos$X_t <- datos$X_t + beta[i]*(datos$t^(i - 1))
  } 
  return(datos)
}
datos <- ts_trend_error(n = 1000, beta = c(1, 0.01, 0.000001, -0.00000001), sd = 0.5)
```


---

## Recapitulando

¬øQu√© deber√≠amos saber hasta ahora?

* Deber√≠as ser capaz de entender c√≥mo [**organizar los datos**]{.hl-yellow} de manera que podamos visualizar de manera sencilla.

:::: columns
::: {.column width="47%"}


```{r}
#| eval: false
ggplot(datos) +
  geom_line(aes(x = t, y = X_t),
            alpha = 0.6) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t",
       title = "Serie temporal X_t = mu_t + eps_t (mu_t c√∫bica)")
```


:::

::: {.column width="53%"}


```{r}
#| echo: false
ggplot(datos) +
  geom_line(aes(x = t, y = X_t), alpha = 0.6) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(x = "t", y = "X_t",
       title = "Serie temporal X_t = mu_t + eps_t (mu_t c√∫bica)")
```


:::

::::

---

## Recapitulando


* Y por √∫ltimo deber√≠as tener una `estim_ts_trend_error()` similar a esta para que, dada una [**serie cualquiera**]{.hl-yellow}, con dos columnas para $t$ y $X_t$ (con cualquier nombre) haga la estimaci√≥n. [**F√≠jate de los argumentos `tag_estim` y `nuevos_valores` que hacen para facilitarnos la vida a futuro**]{.hl-purple}


```{r}
estim_ts_trend_error <-
  function(datos, degree = 1, tag_estim = paste0("estim_poly_", degree),
           col_t = "t", col_X_t = "X_t", nuevos_valores = NULL) {
    
    datos <- # versi√≥n tidyverse
      datos |> select(all_of(col_t), all_of(col_X_t)) |>
      rename(t = all_of(col_t), X_t = all_of(col_X_t))
    
    # R base normal
    # datos <- datos[, c(col_t, col_X_t)]
    # names(datos)[names(datos) == col_t] <- "t"
    # names(datos)[names(datos) == col_X_t] <- "X_t"

    if (degree == 0) {
      modelo <- datos |> lm(formula = X_t ~ 1)
    } else {
      modelo <- datos |> lm(formula = X_t ~ poly(t, degree, raw = TRUE))
    }
    datos[, tag_estim] <- predict(modelo, tibble("t" = datos$t))
    
    if (!is.null(nuevos_valores)) {
      nuevos_datos <- tibble("t" = nuevos_valores, "X_t" = NA)
      nuevos_datos[, tag_estim] <- predict(modelo, tibble("t" = nuevos_datos$t))
      datos <- rbind(datos, nuevos_datos)
    }
    return(datos)
}

```


---

## Caso real: AEMET

Con todo esto podemos aplicar nuestras funciones para [**estimar los datos reales del AEMET**]{.hl-yellow}, estimando bajo 4 hip√≥tesis: solo ruido, ruido + tendencia cte, ruido + tendencia lineal y ruido + tendencia polin√≥mica de grado 3.

. . .


```{r}
retiro_estim <-
  estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 0,
                       col_t = "fecha", col_X_t = "tmed")
retiro_estim$estim_poly_1 <- 
  estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 1,
                       col_t = "fecha", col_X_t = "tmed")$estim_poly_1
retiro_estim$estim_poly_3 <- 
  estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 3,
                       col_t = "fecha", col_X_t = "tmed")$estim_poly_3
retiro_estim$estim_noise <- 0 # f√≠jate que el ruido la estimaci√≥n es...0
retiro_estim_tidy <-
  retiro_estim |> 
  pivot_longer(cols = -t, names_to = "serie", values_to = "X_t")
```


---

## Caso real: AEMET

F√≠jate que de cada `estim_ts_trend_error()` solo nos interesa la propia estimaci√≥n ya que `t` y `X_t` es igual siempre, as√≠ que podr√≠amos hacer un `left_join()` de las diferentes tablas.


```{r}
#| code-line-numbers: 4-9
retiro_estim <-
  estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 0,
                       col_t = "fecha", col_X_t = "tmed") |> 
  left_join(estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 1,
                                 col_t = "fecha", col_X_t = "tmed"),
            by = c("t", "X_t")) |> 
  left_join(estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 3,
                                 col_t = "fecha", col_X_t = "tmed"),
            by = c("t", "X_t"))
retiro_estim$estim_noise <- 0 # f√≠jate que el ruido la estimaci√≥n es...0
retiro_estim_tidy <-
  retiro_estim |> 
  pivot_longer(cols = -t, names_to = "serie", values_to = "X_t")
```


---

## Caso real: AEMET

Y tambi√©n deber√≠amos saber ya visualizar todo


```{r}
#| code-fold: true
ggplot(retiro_estim_tidy) +
  geom_line(aes(x = t, y = X_t, color = serie,
                linewidth = serie, alpha = serie, linetype = serie)) +
  ggthemes::scale_color_colorblind() +
  scale_alpha_manual(values = c(0.85, 0.85, 0.85, 0.85, 1)) +
  scale_linewidth_manual(values = c(1.1, 1.1, 1.1, 1.1, 0.1)) +
  scale_linetype_manual(values = c("dotted", "dotted", "dotted", "dotted", "solid")) +
  theme_minimal() +
  labs(x = "t", y = "Temperatura (¬∫C)", title = "Estimaci√≥n serie AEMET")
```


---

## Caso real: AEMET

La idea de los m√©todos de estimaci√≥n es que podamos usarlos no solo para estimar sino tambi√©n para [**predecir en instantes temporales futuros**]{.hl-yellow}, haciendo uso de esos **¬´nuevos valores¬ª** que podemos usar en `estim_ts_trend_error()`.

. . .

Por ejemplo, en los datos tenemos solo hasta el 31 de agosto de 2024: [**¬øcu√°l es la predicci√≥n de los distintos m√©todos para todo el mes de septiembre, octubre y noviembre?**]{.hl-purple}

---

## Caso real: AEMET


Por ejemplo, en los datos tenemos solo hasta el 31 de agosto de 2024: [**¬øcu√°l es la predicci√≥n de los distintos m√©todos para lo que queda de 2024 y 2025?**]{.hl-purple}


```{r}
#| code-line-numbers: "1,6,9,13"
nuevos_valores <- seq(as_date("2024-09-01"), as_date("2025-12-31"), by = 1)

retiro_predict <-
  estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 0,
                       col_t = "fecha", col_X_t = "tmed",
                       nuevos_valores = nuevos_valores) |> 
  left_join(estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 1,
                                 col_t = "fecha", col_X_t = "tmed",
                                 nuevos_valores = nuevos_valores),
            by = c("t", "X_t")) |> 
  left_join(estim_ts_trend_error(retiro[, c("fecha", "tmed")], degree = 3,
                                 col_t = "fecha", col_X_t = "tmed",
                                 nuevos_valores = nuevos_valores),
            by = c("t", "X_t"))
retiro_predict$estim_noise <- 0 # f√≠jate que el ruido la predicci√≥n es...0

retiro_predict_tidy <-
  retiro_predict |> 
  pivot_longer(cols = -t, names_to = "serie", values_to = "X_t")
```


---


## Caso real: AEMET



```{r}
#| code-fold: true
# filtro un poco para que se vea mejor
ggplot(retiro_predict_tidy |> 
         filter(t > as_date("2015-01-01"))) +
  geom_line(aes(x = t, y = X_t, color = serie,
                linewidth = serie, alpha = serie, linetype = serie)) +
  geom_vline(xintercept = max(retiro$fecha), linetype = "twodash", color = "#a61d0f", alpha = 0.5, linewidth = 0.9) +
  ggthemes::scale_color_colorblind() +
  scale_alpha_manual(values = c(0.85, 0.85, 0.85, 0.85, 1)) +
  scale_linewidth_manual(values = c(1, 1, 1, 1, 0.2)) +
  scale_linetype_manual(values = c("dotted", "dotted", "dotted", "dotted", "solid")) +
  theme_minimal() +
  labs(x = "t", y = "Temperatura (¬∫C)", title = "Predicci√≥n serie AEMET")
```


---

## Tendencia ¬´din√°mica¬ª

El problema del ajuste anterior es que, am√©n de la parte puramente estoc√°stica y la tendencia (m√°s o menos compleja que pueda tener), [**existe una parte ESTACIONAL**]{.hl-yellow}

. . .

Diremos que una serie tiene una [**componente estacional**]{.hl-yellow} siempre que presente un [**patr√≥n que se repite en periodos (aprox.) fijos en el tiempo**]{.hl-yellow} tal que 

$$X_t = f \left(\mu_t, S_t, \varepsilon_t\right) =^{adit} \mu_t + S_t + \varepsilon_t, \quad X_t =^{mult} \mu_t * S_t * \varepsilon_t$$

. . .

Trataremos de **manera general con los modelos aditivos** ya que, en caso de ser multiplicativo, $\log \left(X_t\right) =\log \left( \mu_t \right) + \log \left(S_t \right) + \log \left( \varepsilon_t \right)$

---

## Medias m√≥viles

Existen diferentes estrategias para tener en cuenta la estacionalidad, muchas de ellas basadas en la idea de considerar que la [**tendencia no es algo est√°tico**]{.hl-yellow}

. . .

:::: columns
::: {.column width="35%"}

La m√°s famosa (y sencilla) probablemente sea la idea de [**suavizado de medias m√≥viles**]{.hl-yellow}: en lugar de suavizar la serie considerando una media global, vamos [**mirar la serie por una peque√±a ventana**]{.hl-yellow} donde para cada $t$ solo observemos un peque√±o trozo de la serie.

:::

::: {.column width="65%"}

![](img/window-ma.webp)
:::
::::

---

## Medias m√≥viles


Imagina que tenemos la siguiente serie


```{r}
datos <- tibble("t" = 1:15,
                "x" = c(0.8, 1.3, 1.6, 1.5, 2.2, 2.3, 2.2, 2.4,
                        2, 1.5, 1.2, 1.3, 1.2, 1, 0.7))
```

```{r}
#| echo: false
ggplot(datos) +
  geom_line(aes(x = t, y = x)) +
  theme_minimal()
```



---

## Medias m√≥viles



```{r}
datos <- tibble("t" = 1:15,
                "x" = c(0.8, 1.3, 1.6, 1.5, 2.2, 2.3, 2.2, 2.4,
                        2, 1.5, 1.2, 1.3, 1.2, 1, 0.7))
```


La idea de las [**medias m√≥viles**]{.hl-yellow} es la siguiente:

. . .

1. Decide la [**anchura de tu ventana**]{.hl-yellow} (cuantos datos permites que entren), por ejemplo $5$.

. . .

2. Decide qu√© [**peso vas a asignar**]{.hl-yellow} a cada uno de los puntos (por ejemplo, $1/5$ en nuestro caso)

. . .

3. [**Avanza en la serie con tu ventana**]{.hl-yellow} en cada valor de $t$ y centra la ventana en cada punto (si lo tenemos)

---

## Medias m√≥viles

$y_1 = \color{red}{x_1}$

. . .

$y_2 = \frac{x_1 + \color{red}{x_2} + x_3}{3}$

. . .

$y_3 = \frac{x_1 + x_2 + \color{red}{x_3} + x_4 + x_5}{5}$

. . .

$y_4 = \frac{x_2 + x_3 + \color{red}{x_4} + x_5 + x_6}{5}$

. . .

$y_5 = \frac{x_3 + x_4 + \color{red}{x_5} + x_6 + x_7}{5}$

. . .

$y_6 = \frac{x_4 + x_5 + \color{red}{x_6} + x_7 + x_8}{5}$

. . .

$y_7 = \frac{x_5 + x_6 + \color{red}{x_7} + x_8 + x_9}{5}$

. . .

En general llamaremos [**media m√≥vil**]{.hl-yellow} a la tranformaci√≥n **lineal** 

$$y_t = \sum_{r = -q}^{s} a_r x_{t+r}, \quad t = q + 1, \ldots, n-s, \quad \sum_{r = -q}^{s} a_r = 1$$

---

## Medias m√≥viles

$$y_t = \sum_{r = -q}^{s} a_r x_{t+r}, \quad t = q + 1, \ldots, n-s, \quad \sum_{r = -q}^{s} a_r = 1$$

La transformaci√≥n es una [**media ponderada**]{.hl-yellow} de $q + s + 1$ valores donde, seg√∫n avanzamos, se **elimina el dato m√°s antiguo y entra el m√°s nuevo**

. . .

* Si los [**pesos son todos iguales**]{.hl-purple} $a_j = \frac{1}{N}$, con $N = q+s+1$, se conoce como **media m√≥vil de orden N**.

. . .

* Si $q = s$ tal que $a_{-j} = a_j$, para todo $j=1,\ldots,q$ tal que $k = 2*q + 1$, se conoce como [**k MA (moving average sim√©trica)**]{.hl-yellow}

. . .

* El problema de determinar los primeros/√∫ltimos valores se conoce como [**problema de los efectos terminales**]{.hl-yellow}

---

## Medias m√≥viles

Para calcular una [**media m√≥vil de orden $k=2*q+1$**]{.hl-yellow} en `R` podemos hacerlo con `filter()` del paquete `{stats}` teniendo la variable ya **ordenada** ([**¬°cuidado!**]{.hl-red}: si tienes cargado `{tidyverse}` debes especificar que `filter()` es)


```{r}
k <- 3
datos$x_linear <- predict(datos |> lm(formula = x ~ t), datos)

datos <- datos[order(datos$t), ]
datos$x_smooth_3ma <- stats::filter(datos$x, filter = rep(1/k, k))
datos
```


---

## Medias m√≥viles


```{r}
#| code-fold: true
ggplot(datos |>
         pivot_longer(cols = -t, names_to = "var", values_to = "values")) +
  geom_line(aes(x = t, y = values, color = var, linetype = var),
            linewidth = 0.9, alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  theme_minimal()
```


F√≠jate c√≥mo ahora la curva queda [**suavizada pero de manera din√°mica**]{.hl-yellow}

---

## Medias m√≥viles


```{r}
datos$x_smooth_5ma <- stats::filter(datos$x, filter = rep(1/5, 5))
datos$x_smooth_7ma <- stats::filter(datos$x, filter = rep(1/7, 7))
datos$x_smooth_9ma <- stats::filter(datos$x, filter = rep(1/9, 9))
```


:::: columns
::: {.column width="25%"}

F√≠jate que [**cuando aumenta $k$, la serie es m√°s suavizada**]{.hl-green} (m√°s agresiva con las fluctuaciones) pero [**hay m√°s datos ausentes**]{.hl-red} (problema de efectos terminales)

:::

::: {.column width="75%"}


```{r}
#| code-fold: true
ggplot(datos |>
         pivot_longer(cols = -t, names_to = "var", values_to = "values")) +
  geom_line(aes(x = t, y = values, color = var),
            linewidth = 0.9, alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  theme_minimal()
```


:::
::::


---

## üíª Tu turno {#tu-turno-6-1}

### [**Ejercicio 1**]{.hl-yellow}

üíª Realiza una estimaci√≥n de la serie aplicando el suavizado de medias m√≥viles con par√°metro $k=7, 14, 28, 365$ a los datos AEMET e incluye las 4 nuevas columnas con los 4 nuevos m√©todos de estimaci√≥n al conjunto `retiro_estim` que ten√≠amos de clases anteriores. Importante: la serie que le pases a `stats::filter()` no puede tener ausentes.



```{r}
#| code-fold: true
#| eval: false
# versi√≥n tidyverse (en R base tabla$variable_nueva <- valor)
retiro_estim <-
  retiro_estim |>  drop_na(X_t) |> arrange(t) |> 
  mutate(x_smooth_7ma = stats::filter(X_t, filter = rep(1/7, 7)),
         x_smooth_14ma = stats::filter(X_t, filter = rep(1/14, 14)),
         x_smooth_28ma = stats::filter(X_t, filter = rep(1/28, 28)),
         x_smooth_365ma = stats::filter(X_t, filter = rep(1/365, 365)))

ggplot(retiro_estim |>
         pivot_longer(-c(fecha, t), names_to = "type", values_to = "pred") |>
         filter(between(fecha, as_date("2016-01-01"), as_date("2022-01-01")))) +
  geom_line(aes(x = fecha, y = pred, color = type),
            linewidth = 0.7, alpha = 0.85) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(title = "Temperatura media como SERIE TEMPORAL",
       x = "t (fecha)", y = "¬∫C (media)")
```





# Clase 7: [estacionalidad]{.flow} {#clase-7}

[**Introduciendo y estimando la componente estacional**]{style="color:#444442;"}

* [üíª Ejercicios](#tu-turno-7-1)
* [üìÜ Planificaci√≥n](#planificacion)

---

## Componente estacional

El problema todos los ajuste anteriores es que [**existe una parte ESTACIONAL**]{.hl-yellow} que estamos ignorandome deliberadamente. Diremos que una serie tiene una [**componente estacional $S_t$**]{.hl-yellow} siempre que presente un [**patr√≥n que se repite en periodos (aprox.) fijos en el tiempo**]{.hl-yellow} tal que 

$$X_t = f \left(\mu_t, S_t, \varepsilon_t\right) =^{adit} \mu_t + S_t + \varepsilon_t, \quad X_t =^{mult} \mu_t * S_t * \varepsilon_t$$

. . .

Trataremos de **manera general con los modelos aditivos** ya que, en caso de ser multiplicativo, $\log \left(X_t\right) =\log \left( \mu_t \right) + \log \left(S_t \right) + \log \left( \varepsilon_t \right)$

. . .

F√≠jate que ahora ${\rm E} \left[X_t \right] = \mu_t + S_t$ y por tanto nuestra [**estimaci√≥n muestral**]{.hl-yellow} vendr√° definida como

$$\widehat{X}_t = \widehat{\mu}_t + \widehat{S}_t$$

. . .

[**¬øQu√© significa que $S_t$ estacional (o peri√≥dico)?**]{.hl-purple}

---

## Componente estacional

En general, diremos que una funci√≥n [**$S_t := S(t)$ es estacional (o peri√≥dica) de periodo $s$**]{.hl-yellow} siempre que $S_t = S_{t + s} = S_{t + 2s}$: si es estacional de **periodo $s$** significa que, cada $s$ valores, la funci√≥n se repite.

. . .

F√≠jate que si $S_t = S_{t + s}$ tambi√©n entonces sucede que $S_t = S_{t - s}$.

. . .

::: callout-important
## Importante

Es importante entender que determinar el **periodo** no es solo determinar si es ¬´anual¬ª o ¬´mensual¬ª: el valor $s$ es el [**n√∫mero de valores de la componente $S_t$ que pasan hasta que se repita**]{.hl-yellow}. Por ejemplo, una serie puede tener una periodicidad anual pero si los datos son recogidos de manera trimestral, $s = 4$; la misma periodicidad para unos datos recogidos de manera diaria ser√° $s = 365$.

En el caso de los datos del AEMET, $s = 365$, no solo porque la temperatura tengan una periodicidad anual obviamente, sino porque los **datos son diarios** (la misma variable, recogida de manera mensual, tendr√≠a $s = 12$).

:::

---

## Componente estacional

¬øC√≥mo [**estimar dicha componente estacional**]{.hl-yellow}? Los [**m√©todos de descomposici√≥n cl√°sica**]{.hl-yellow} tienen la siguiente estructura:

1. [**Estimaci√≥n de la tendencia**]{.hl-purple}. Dada la serie original $X_t$ se realiza una estimaci√≥n de su nivel o tendencia $\mu_t$. Como hemos visto tenemos distintas alternativas:

- **medias m√≥viles** (ser√° clave **determinar la ventana $k$**)
- tendencia polin√≥mica
- regresi√≥n local (conocida como [**regresi√≥n LOESS o LOWESS**]{.hl-yellow}: ajusta a los datos una **regresi√≥n polin√≥mica pero de manera LOCAL**, en cada punto solo se utiliza un % de los datos). Ver <https://www.statology.org/lowess-smoothing-r/>

. . .

Tras estimar la tendencia se construye la [**serie centrada**]{.hl-yellow}

$$Y_t = X_t - \mu_t = S_t + \varepsilon_t, \quad \hat{Y}_t = X_t - \hat{\mu}_t$$

---

## Componente estacional

$$Y_t = X_t - \mu_t = S_t + \varepsilon_t, \quad \hat{Y}_t = X_t - \hat{\mu}_t$$

F√≠jate que $\hat{Y}_t$ es ya una serie sin tendencia, y cuyos [**valores ya no representan la serie original sino la anomal√≠a**]{.hl-yellow} que tendr√≠a cada $t$ respecto al nivel global (en el caso de las temperaturas, por ejemplo la anomal√≠a promedio de temperatura entre los distintos d√≠as del a√±o y el nivel general de la serie).

. . .

2. [**Estimar los coeficientes estacionales**]{.hl-purple}. El objetivo ser√° obtener un conjunto de $s$ coeficientes $\left(S_1, S_2, \ldots, S_{s-2}, S_{s-1}, S_s \right)$ que cumplir√°n por definici√≥n dos condiciones:

* Se repiten cada $s$ valores (por solo necesitamos estimar un tramo).

* Su suma es cero (ya que representan las anomal√≠as respecto al nivel general, por lo que si hay valores por encima tienen que existir por debajo)

---

## Componente estacional


2. [**Estimar los coeficientes estacionales**]{.hl-purple}. El objetivo ser√° obtener un conjunto de $s$ coeficientes $\left(S_1, S_2, \ldots, S_{s-2}, S_{s-1}, S_s \right)$.

Para su estimaci√≥n lo que haremos ser√° calcular, con la serie centrada, la [**diferencia entre la media de cada periodo estacional**]{.hl-yellow} y la **media general**

$$\widehat{S}_j = \overline{Y}_j - \overline{Y}, \quad \overline{Y}_j = \frac{1}{n} \sum_{i=1}^{n} Y_{s*i + j}, \quad \overline{Y} = \frac{1}{T} \sum_{i=0}^{n-1} \sum_{j=1}^{s} Y_{s*i + j}, \quad j = 1, \ldots, s$$

::: callout-tip

## Datos AEMET

En el **caso del AEMET**: $\widehat{Y}_j$ representa la media (de la serie centrada) de todos los d√≠as $j=1, \ldots, 365$, es decir, la media de los 1-enero, sin importar el a√±o; de los 2-enero, ..., de los 31-diciembre (la anomal√≠a de temperatura que en promedio hace el 1-enero respecto a la tendencia general, y as√≠ para cada d√≠a del a√±o). En este caso $n = 45$ ya que tenemos datos de 45 a√±os (salvo el final de 2024).

:::

---

## Componente estacional

3. [**Estimar las innovaciones**]{.hl-purple}. Una vez estimada la tendencia y estacionalidad

$$\widehat{\varepsilon}_t = X_t - \widehat{X}_t, \quad \widehat{X}_t = \widehat{\mu}_t + \widehat{S}_t$$

. . .

F√≠jate que la componente estacional estimada cumple tambi√©n que $\widehat{S}_t = \widehat{S}_{t + s}$, es decir, **solo necesitamos $s$ coeficientes estimados** (f√≠jate en el sub√≠ndice del $\hat{S}$)

$$\widehat{\varepsilon}_{s*i + j} = X_{s*i + j} - \widehat{X}_{s*i + j}, \quad \widehat{X}_{s*i + j}= \widehat{\mu}_{s*i + j} + \widehat{S}_j, \quad j = 1,\ldots,s$$

. . .

* $\widehat{Y}_t$ representa la [**estimaci√≥n de la serie centrada**]{.hl-green}: lo que falta [**por modelizar es componente estacional**]{.hl-yellow} 

* $X_t - \widehat{S}_t$ representa la [**estimaci√≥n de la serie DESESTACIONALIZADA**]{.hl-green}: lo que falta [**por modelizar es tendencia sin tener en cuenta el efecto estacional**]{.hl-yellow} (ejemplo: tasa de paro sin el efecto que tienen periodos como verano o navidad)


---

## Algunas observaciones


::: callout-warning

El m√©todo descrito se conoce [**m√©todo cl√°sico de descomposici√≥n**]{.hl-yellow} (o STL si se usa una regresi√≥n local para estimar la tendencia en lugar de medias m√≥viles) pero la componente estacional podr√≠a ser tambi√©n estimada por [**cualquier funci√≥n peri√≥dica o arm√≥nica de periodo $s$**]{.hl-yellow}, por ejemplo, $sin(\frac{2 \pi t}{s})$, se repite cada $s$ valores).
:::

&nbsp;

::: callout-warning

Todo lo anterior tambi√©n est√° dise√±ado bajo la [**hip√≥tesis de que solo tenemos una periodicidad $s$**]{.hl-yellow}: nuestra serie podr√≠a tener distintas periodicidades superpuestas (Fourier: toda funci√≥n peri√≥dica puede representarse como suma de funciones seno de distinta frecuencia y amplitud). De momento supondremos siempre un **√∫nico ciclo**.
:::


---

## üíª Tu turno {#tu-turno-7-1}

### [**Ejercicio 1**]{.hl-yellow}

üíª Renombra la funci√≥n de estimaci√≥n de la tendencia polin√≥mica que ya tenemos hecha como `estim_ts_trend_poly()`




```{r}
estim_ts_trend_poly <-
  function(datos, degree = 1, tag_estim = paste0("estim_poly_", degree),
           col_t = "t", col_X_t = "X_t", nuevos_valores = NULL) {
    
    # R base normal
    datos <- datos[, c(col_t, col_X_t)]
    names(datos)[names(datos) == col_t] <- "t"
    names(datos)[names(datos) == col_X_t] <- "X_t"

    if (degree == 0) {
      modelo <- datos |> lm(formula = X_t ~ 1)
    } else {
      modelo <- datos |> lm(formula = X_t ~ poly(t, degree, raw = TRUE))
    }
    datos[, tag_estim] <- predict(modelo, tibble("t" = datos$t))
    
    if (!is.null(nuevos_valores)) {
      nuevos_datos <- tibble("t" = nuevos_valores, "X_t" = NA)
      nuevos_datos[, tag_estim] <- predict(modelo, tibble("t" = nuevos_datos$t))
      datos <- rbind(datos, nuevos_datos)
    }
    return(datos)
}
```


---

## üíª Tu turno 

### [**Ejercicio 2**]{.hl-yellow}

üíª Dise√±a una funci√≥n general `estim_ts_trend()` que use `estim_ts_trend_poly()` que ya tenemos hecha. Dicha funci√≥n debe tener los mismos par√°metros que ten√≠amos en esa funci√≥n con dos argumentos extras: `tipo_trend_estim` (que haga la polin√≥mica si `"poly"` y medias m√≥viles si `"MA"`) y `k` (la anchura de la ventana; la haremos siempre sim√©trica y con pesos uniformes de momento).


```{r}
#| code-fold: true
estim_ts_trend <- 
  function(datos, degree = 1, k = NA, tipo_trend_estim = "poly",
           tag_estim =
             paste0("estim_", tipo_trend_estim, if_else(is.na(k), degree, k)),
           col_t = "t", col_X_t = "X_t", nuevos_valores = NULL) {
    
    if (tipo_trend_estim == "poly") {
      
      estim <- estim_ts_trend_poly(datos, degree, tag_estim,
                                   col_t, col_X_t, nuevos_valores)
    } else if (tipo_trend_estim == "MA") {
      
      estim <-
        datos |> 
        select(all_of(col_t), all_of(col_X_t)) |> 
        rename(t = col_t, X_t = col_X_t) |> 
        # hay que ordenar!
        arrange(t) |> 
        # con !! + := le indicamos que el nombre sale de una variable
        # (es una t√©cnica llamada lazyeval)
        mutate(!!tag_estim := 
                 stats::filter(X_t, filter = rep(1/k, k), sides = 2))

    }
    return(estim)
  }
temp_retiro <- read_csv(file = "./datos/retiro_temp.csv")
ejemplo1 <-
  temp_retiro |> estim_ts_trend(degree = 1, tipo_trend_estim = "poly", col_t = "fecha", col_X_t = "tmed")
ejemplo2 <-
  temp_retiro |> estim_ts_trend(k = 7, tipo_trend_estim = "MA", col_t = "fecha", col_X_t = "tmed")
```


---

## üíª Tu turno 

### [**Ejercicio 3**]{.hl-yellow}

üíª Dise√±a una funci√≥n `ts_detrend()` que use `estim_ts_trend()` que ya tenemos hecha (por lo que tendr√° que tener, m√≠nimo, los mismos par√°metros que ten√≠amos en esa funci√≥n), de manera que dado un conjunto de datos, devuelva una nueva columna con la tendencia estimada (seg√∫n el m√©todo que decidamos) y una nueva columna con la serie centrada


```{r}
#| code-fold: true
ts_detrend <- 
  function(datos, degree = 1, k = NA, tipo_trend_estim = "poly",
           tag_estim = paste0("estim_", tipo_trend_estim, if_else(is.na(k), degree, k)),
           col_t = "t", col_X_t = "X_t", nuevos_valores = NULL) {
    
    estim <-
      # estimamos tendencia
      estim_ts_trend(datos, degree, k, tipo_trend_estim, tag_estim,
                     col_t, col_X_t, nuevos_valores) |> 
      # la llamamos siempre estim_trend
      rename(estim_trend = tag_estim) |> 
      # el tipo de estimaci√≥n lo guardamos en otra variable aparte
      mutate("tipo_trend_estim" = tag_estim,
             # calculamos serie centrada
             # si es NA, ponemos la media global en la estimaci√≥n de la tendencia
             "estim_trend" =
               if_else(is.na(estim_trend), mean(X_t, na.rm = TRUE), estim_trend),
             "detrend" = X_t - estim_trend)
    return(estim)
  }
ejemplo1 <- temp_retiro |> ts_detrend(degree = 1, col_t = "fecha", col_X_t = "tmed")
ejemplo2 <- temp_retiro |> ts_detrend(k = 365, tipo_trend_estim = "MA", col_t = "fecha", col_X_t = "tmed")
```




# Clase 8: [practicar con funciones]{.flow} {#clase-8}

[**Dise√±o de funciones para nuestra descomposici√≥n**]{style="color:#444442;"}

* [üíª Ejercicios](#tu-turno-8-1)
* [üìÜ Planificaci√≥n](#planificacion)


---

## üíª Tu turno {#tu-turno-8-1}

### [**Ejercicio 1**]{.hl-yellow}

üíª Dise√±a una funci√≥n `ts_deseason()` que, dada una serie cualquiera y un periodo $s$, nos devuelva una nueva columna con los coeficientes estacionales estimados (con el m√©todo visto anteriormente) y una nueva columna con la serie desestacionalizada. Vas a necesitar darle
como argumento no solo el nombre de las columnas donde est√© $t$ y $Y_t$ sino como se llamar√° la variable de grupo que debes crear dentro usando $s$.


```{r}
ts_deseason <- function(datos, s, col_group, col_t = "t", col_Y_t = "Y_t") {
  
  # primero renombramos como siempre y ordenamos
  estim <-
    datos |>
    rename(t = col_t, Y_t = col_Y_t) |> 
    arrange(t)
  
  # Contruimos variable de grupo
  estim <- 
    estim |>
    rowid_to_column(var = "id") |>
    mutate(!!col_group := ((id - 1) %% s) + 1) |> 
    select(-id)

  # Estimamos la estacionacionalidad
  estim_season <-
    estim |>
    summarise("season_coef" = mean(Y_t, na.rm = TRUE), .by = col_group) |>
    mutate("estim_season" = season_coef - mean(season_coef, na.rm = TRUE))
  
  # Lo √∫nimos a los datos
  estim <- 
    estim |> 
    left_join(estim_season, by = col_group) |> 
    mutate("tipo_estim_season" = "clasica",
           "deseason" = Y_t - estim_season) 
  
  return(estim)
}
```



---

## üíª Tu turno 

### [**Ejercicio 2**]{.hl-yellow}

üíª Tomando los datos de retiro: a) aplica la funci√≥n `ts_detrend()` de manera que la tendencia sea estimada con $MA(k = 365)$; b) a esa tabla apl√≠cale `ts_deseason()` para estimar $S_t$, donde la variable de grupo se llama `"mes_dia"`; c) tras acabar renombra `Y_t` como `detrend`


---

## üíª Tu turno 

### [**Ejercicio 2**]{.hl-yellow}



```{r}
#| code-fold: true
estim_retiro <-
  temp_retiro |>
  ts_detrend(k = 365, tipo_trend_estim = "MA", col_t = "fecha", col_X_t = "tmed") |> 
  mutate("mes_dia" = paste0(day(t), "-", month(t))) |>
  ts_deseason(s = 365, col_group = "mes_dia", col_Y_t = "detrend") |> 
  rename(detrend = Y_t)
```


---

## üíª Tu turno 

### [**Ejercicio 3**]{.hl-yellow}

üíª Con `estim_retiro` del ejercicio anterior, calcula una √∫ltima columna que sea la estimaci√≥n total de la serie `estim_X_t` (estimaci√≥n de la tendencia + estimaci√≥n de la estacionalidad). Tras ello pivota como consideres para poder dibujar en la misma gr√°fica la serie real y la estimaci√≥n

---

## üíª Tu turno 

### [**Ejercicio 3**]{.hl-yellow}


```{r}
estim_retiro |> 
  mutate("estim_X_t" = estim_trend + estim_season) |> 
  # solo queremos dos de las curvas
  select(t, X_t, estim_X_t) |> 
  pivot_longer(cols = -t, names_to = "serie", values_to = "X_t") |>
  ggplot() +
  geom_line(aes(x = t, y = X_t, color = serie, alpha = serie)) +
  ggthemes::scale_color_colorblind() +
  scale_alpha_manual(values = c(0.9, 0.35)) +
  theme_minimal() + 
  labs(x = "fecha", y = "Temperatura (¬∫C)",
       title = "Estimaci√≥n decomposici√≥n cl√°sica",
       subtitle = "Tendencia estimada con MA(k = 365)")
```


---

## üíª Tu turno 

### [**Ejercicio 4**]{.hl-yellow}

üíª Con `estim_retiro` del ejercicio 2, calcula una √∫ltima columna que sea la estimaci√≥n total de la serie `estim_X_t` (estimaci√≥n de la tendencia + estimaci√≥n de la estacionalidad) pero ahora selecciona y pivota como consideres para poder hacer luego una visualizaci√≥n de 6 gr√°ficas (por separado pero en el mismo plot): i) la serie real, ii) la estimaci√≥n de la tendencia, iii) la estimaci√≥n de la estacionalidad, iv) la serie sin tendencia, v) la serie sin tendencia ni estacionalidad, vi) la estimaci√≥n

---

## üíª Tu turno 

### [**Ejercicio 4**]{.hl-yellow}



```{r}
estim_retiro |> 
  mutate("estim_X_t" = estim_trend + estim_season) |> 
  # solo queremos dos de las curvas
  select(t, X_t, estim_trend, detrend, estim_season, deseason, estim_X_t) |> 
  pivot_longer(cols = -t, names_to = "serie", values_to = "X_t") |>
  ggplot() +
  geom_line(aes(x = t, y = X_t, color = serie)) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~serie, scales = "free_y") +
  theme_minimal() + 
  labs(x = "fecha", y = "Temperatura (¬∫C)",
       title = "Estimaci√≥n decomposici√≥n cl√°sica",
       subtitle = "Tendencia estimada con MA(k = 365)")
```


---

## üíª Tu turno 

### [**Ejercicio 6**]{.hl-yellow}

üíª Repite los ejercicios anteriores pero haciendo una peque√±a modificaci√≥n en la funci√≥n que estima la estacionalidad, permitiendo que pueda estimarla de manera cl√°sica o de manera sinuosoidal $\widehat{S}_t = \sin(2 \pi t/s)$.


# Clase 9: [m√©tricas de error]{.flow} {#clase-9a}

[**Calibrando nuestra estimaci√≥n: train/validaci√≥n y errores**]{style="color:#444442;"}

* [üíª Ejercicios](#tu-turno-8-1)
* [üìÜ Planificaci√≥n](#planificacion)





# Clase 10: [m√©todos de alisado]{.flow} {#clase-10}


---

## Limitaciones

Hasta ahora todo lo que hemos hecho ha sido suponer que el comportamiento de nuestra  [**serie temporal se pod√≠a explicar en t√©rminos de subpatrones o conductas**]{.hl-yellow}, que en su forma aditiva, pueden tener la siguiente estructura

$$X_t = T_t + S_t + \varepsilon_t, \quad \widehat{X}_t = \widehat{T}_t + \widehat{S}_t$$

* $\widehat{T}_t$ la hemos estimado mediante un polinomio o medias m√≥viles

* $\widehat{S}_t$ la hemos estimado asumiendo que **solo hay un periodo**

. . .

Siempre hemos [**ponderado las observaciones por igual**]{.hl-red} pero...¬øno tendr√≠amos que dar [**m√°s importancia a los datos m√°s recientes**]{.hl-green}?

---

## Clasificaci√≥n de Pegel

![](img/pegel.jpg)

Imagen extra√≠da de Gonz√°lez Velasco, M., & Puerto Garc√≠a, I. M. del. (2009). Series temporales. Universidad de Extremadura.

**Estacionalidad multiplicativa = serie heteroced√°stica (varianza no constante)**

---

## Alisado exponencial

Para superar esas limitaciones, en los a√±os 50 y 60 se propusieron una serie de m√©todos que, m√°s all√° de asumir una estructura en los datos, su objetivo era [**describir la serie en t√©rminos de sus propios cambios**]{.hl-yellow}.

&nbsp;

El objetivo de los distintos m√©todos de alisado/suavizado ser√° [**intuir la inercia de la serie, eliminando sus posibles fluctuaciones aleatorias**]{.hl-yellow}, asumiendo que si la tendencia de la serie es ascendente, probablemente la serie siga subiendo, teniendo en consideraci√≥n la pendiente con la que crece o decrece.

. . .

El nombre de [**alisado/suavizado EXPONENCIAL**]{.hl-yellow} se debe a que vamos a ponderar el pasado de la serie con un conjunto de pesos que, normalmente, decrecer√°n de manera exponencial (ejemplo: si la observaci√≥n inmediatamente anterior tiene un peso de $0.5$, la siguiente tendr√° un peso de $0.5^2 = 0.25$)

---


## Alisado simple

Imagina que tenemos el valor de la serie en $X_t$ y su estimaci√≥n $\widehat{X}_t$... ¬øc√≥mo poder [**predecir $\widehat{X}_{t+1}$ solo usando ambos valores**]{.hl-yellow}?

. . .

La idea que tuvo **Holt (1956)** fue la de hacer una [**media ponderada del valor real $X_t$ y su estimaci√≥n $\widehat{X}_t$**]{.hl-yellow} 

$$\widehat{X}_{t+1} = \theta \widehat{X}_t + \left( 1 - \theta \right)X_t = X_t + \theta \left(\widehat{X}_t - X_t \right) = X_t + \theta \widehat{\varepsilon}_t , \quad 0 < \theta < 1$$

. . .

* Si [**$\theta \to 1$**]{.hl-purple}, entonces $\widehat{X}_{t+1} \to \widehat{X}_t$ el [**modelo de alisado produce predicciones casi constantes**]{.hl-yellow} sin muchas variaciones

. . .

* Si [**$\theta \to 0$**]{.hl-purple}, entonces $\widehat{X}_{t+1} \to X_t$ el [**modelo de alisado produce predicciones muy variables dependientes del √∫ltimo valor observado**]{.hl-yellow} (con mucha fluctuaci√≥n causada por la propia aleatoriedad de la serie)

---

## Alisado simple

Si hacemos lo mismo para la estimaci√≥n en $X_t$ tenemos

$$\begin{eqnarray}\widehat{X}_{t+1} &=& \theta \left( \theta \widehat{X}_{t-1} + \left( 1 - \theta \right)X_{t-1} \right) + \left( 1 - \theta \right)X_t \nonumber \\ &=& \theta^2 \widehat{X}_{t-1} + \theta   \left( 1 - \theta \right) X_{t-1}  + \left( 1 - \theta \right) X_t \end{eqnarray}$$

. . .

Si hacemos lo mismo para la estimaci√≥n en $X_{t-1}$

$$\begin{eqnarray}\widehat{X}_{t+1} &=& \theta^2 \left(\theta \widehat{X}_{t-2} + \left( 1 - \theta \right)X_{t-2} \right) +  \left( 1 - \theta \right) \left(X_t + \theta X_{t-1} \right) \nonumber \\ &=&
\theta^3 \widehat{X}_{t-2} + \theta^2 \left( 1 - \theta \right)X_{t-2} +  \theta \left( 1 - \theta \right)X_{t-1} + \left( 1 - \theta \right) X_t \nonumber \\ &=&
\theta^3 \widehat{X}_{t-2} + \left( 1 - \theta \right) \left(\theta^2 X_{t-2} +  \theta X_{t-1} +  X_t \right) \end{eqnarray}$$

---

## Alisado simple


Si repetimos el [**proceso iterativo**]{.hl-yellow} con $\widehat{X}_{t-1}$, $\widehat{X}_{t-2}$...

$$\widehat{X}_{t+1} =  \theta^t \widehat{X}_{1} + \left( 1 - \theta \right) \left(X_t + \theta X_{t-1}  +   \ldots + \theta^{t-1} X_{1}\right) = \theta^{t}\widehat{X}_{1} + \left(1 - \theta \right) \sum_{j=0}^{t-1} \theta^{j} X_{t-j}$$

[**¬øQu√© sucede cuando $t \to \infty$?**]{.hl-yellow}


. . . 

Dado que $\theta < 1$, entonces

$$\lim_{t \to \infty} \widehat{X}_{t+1} = \left( 1 - \theta \right) \sum_{j=0}^{\infty} \theta^j X_{t-j}$$

F√≠jate en la **serie geom√©trica** $\sum_{j=0}^{\infty} \theta^j$. ¬øRecuerdas cu√°l era la [**suma de una serie geom√©trica cuya raz√≥n es menor que 1**]{.hl-yellow}?

---

## Alisado simple


Dado que $\theta < 1$, tenemos que $\sum_{j=0}^{\infty} \theta^j = \frac{1}{1 - \theta} \rightarrow \left( 1 - \theta \right) \sum_{j=0}^{\infty} \theta^j = 1$, as√≠ que $\left( 1 - \theta \right) \sum_{j=0}^{t-1} \theta^j X_{t-j}$ representa una [**media ponderada del pasado (pesos decrecientes geom√©tricamente con suma 1 en el infinito)**]{.hl-yellow}.

. . .

:::: columns
::: {.column width="35%"}

Si te fijas adem√°s [**todas las observaciones son influyentes**]{.hl-yellow} pero su influencia va decreciendo. 

:::

::: {.column width="65%"}


```{r}
#| echo: false
theta <- tibble("x" = rep(1:30, 3),
                "theta" = c(0.9^(1:30), 0.6^(1:30), 0.3^(1:30)),
                "pesos" = c(rep("theta = 0.9", 30), rep("theta = 0.6", 30),
                            rep("theta = 0.3", 30)))
ggplot(theta) +
  geom_line(aes(x = x, y = theta, color = pesos), linewidth = 1.2) +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(title = "Decrecimiento exponencial de pesos")
```


:::
::::

---

## Alisado simple


F√≠jate que la f√≥rmula anterior se puede [**generalizar**]{.hl-yellow} para [**predicci√≥n a horizonte $h$**]{.hl-yellow}: si solo disponemos de informaci√≥n hasta el instante $t$, tenemos que

$$\begin{eqnarray}\widehat{X}_{t+1|t} &=&  \theta \widehat{X}_{t|t-1} + \left( 1 - \theta \right)X_t = \theta^{t}\widehat{X}_{1} + \left(1 - \theta \right) \sum_{j=0}^{t-1} \theta^{j} X_{t-j}, \nonumber \\ \widehat{X}_{t+2|t} &=&  \theta \widehat{X}_{t+1|t} + \left( 1 - \theta \right)X_{t+1} \simeq \theta \widehat{X}_{t+1|t} + \left( 1 - \theta \right)\widehat{X}_{t+1|t} = \widehat{X}_{t+1|t}, \nonumber \\ ...  \nonumber \\ \widehat{X}_{t+h|t} &=&\widehat{X}_{t+1|t} \quad \text{para todo h}
\end{eqnarray}$$

. . .

El [**m√©todo de alisado simple produce ¬´flat predictions¬ª**]{.hl-yellow}: siempre devuelve a horizonte $h$ el √∫ltimo valor  predicho $\widehat{X}_{t+1|t}$ (asume que la tendencia es localmente constante).

---

## Alisado simple

F√≠jate que el m√©todo de alisado simple es una simple media ponderada, de una forma muy particular, pero que podr√≠amos considerar [**otro tipo de m√©todos basados en promedios**]{.hl-yellow}:


* Si consideramos el conocido como [**m√©todo naive o trivial**]{.hl-yellow}, tenemos que la [**predicci√≥n a horizonte $h$**]{.hl-yellow} (con la informaci√≥n disponible hasta $t$) es  

$$\widehat{X}_{t+h|t} = X_t$$

la **√∫nica observaci√≥n importante es la √∫ltima**. Esto es [**equivalente a un alisado simple con $\theta = 0$**]{.hl-yellow}

---

## Alisado simple

F√≠jate que el m√©todo de alisado simple es una simple media ponderada, de una forma muy particular, pero que podr√≠amos considerar [**otro tipo de m√©todos basados en promedios**]{.hl-yellow}:


* Si consideramos el conocido como [**mean method**]{.hl-yellow}, lo que hemos llamado `poly0` (hace simplemente la media cte de todos los valores), la [**predicci√≥n a horizonte $h$**]{.hl-yellow} (con la informaci√≥n disponible hasta $t$) es 

$$\widehat{X}_{t+h|t} = \frac{1}{t} \sum_{j=1}^{t}X_j$$

todas las observaciones pasadas son **igual de importantes**.



---

## Alisado simple

El modelo de alisado simple anterior en realidad equivale a asumir que la [**estructura de nuestra serie es localmente constante**]{.hl-yellow}


$$X_t = T_t + \varepsilon_t, \quad T_t - T_{t-1} \simeq cte$$


una tendencia que va variando [**muy lentamente con el tiempo**]{.hl-yellow} (ya que el futuro se parece m√°s a los valores pasados inmediatos que a los valores m√°s lejanos).

. . .

Para el m√©todo de alisado simple [**necesitamos conocer el valor inicial y encontrar el mejor $\theta$**]{.hl-yellow} (por ejemplo, mediante m√≠nimos cuadrados de los errores observados para distintos valores).

&nbsp;

Supondremos [**$\theta$ constante**]{.hl-yellow} pero existen m√©todos (ver [aqu√≠](https://robjhyndman.com/forecasting/)) para considerar que $\theta$ se adapta a su vez a lo largo del tiempo

---

## Ejemplo en R

Vamos a trabajar con el paquete `{tsibbledata}` que nos permite usar varios [**ejemplos de series temporales**]{.hl-yellow}. En concreto vamos a usar el dataset `global_economy` que contiene distintas estad√≠sticas econ√≥micas para distintos a√±os pa√≠ses, y nos vamos a centrar solo en **datos de exportaciones de Algeria**



```{r}
library(tsibbledata)
library(tsibble)
algeria_economy <-
  global_economy |>
  filter(Country == "Algeria") |>
  # solo hay un pa√≠s, eliminamos key
  update_tsibble(key = NULL)
algeria_economy
```


---


## Ejemplo en R

Si observamos la serie [**no se aprecia una tendencia clara ni una estacionalidad evidente**]{.hl-yellow}, perfecto para nuestro m√©todo de alisado simple.


```{r}
#| code-fold: true
ggplot(algeria_economy) +
  geom_line(aes(x = Year, y = Exports)) +
  scale_x_continuous(n.breaks = 20) +
  theme_minimal()
```


---

## Ejemplo en R

[**¬øC√≥mo realizar la estimaci√≥n de alisado simple?**]{.hl-yellow}

. . .

Vamos a usar el universo de paquetes `{tidyverts}` (una forma de trabajar en modo tidyverse con series temporales), de los cuales ya conocemos `{tsibbledata}` y `{tsibble}`. Vamos a instalar tambi√©n el paquete [`{fable}`](https://fable.tidyverts.org/) que nos [**proporciona herramientas para la predicci√≥n de series temporales**]{.hl-yellow}


```{r}
library(fable)
```


---

## Ejemplo en R

Dentro de este paquete existe una funci√≥n llamada `model()` que nos permite [**ajustar distintos modelos a los datos**]{.hl-yellow}.

* `ETS(var_objetivo ~ componentes del modelo)`: para ajustar los [**m√©todos de alisado/suavizado exponencial**]{.hl-yellow} necesitamos especificarle dentro de `model()` que nuestro modelo es de tipo `ETS()` (exponential time-series smoothing model)


```{r}
#| eval: false
#| code-line-numbers: "4"
# NO EJECUTES QUE EST√Å A√öN SIN COMPLETAR
fit_algeria <-
  algeria_economy |>
  model(ETS(Exports ~ ...))
```


---

## Ejemplo en R


Una vez que hemos determinado nuestra variable objetivo, las [**componentes del modelo se pueden incorporar**]{.hl-yellow} con las [**funciones `error()`, `trend()` y `season()`**]{.hl-purple}


```{r}
#| eval: false
#| code-line-numbers: "4"
fit_algeria <-
  algeria_economy |>
  model(ETS(Exports ~
              error(tipo) + trend(tipo) + season(tipo)))
```


. . .

Cada una de las funciones admite dentro los siguientes tipos:

* `"A"`: [**aditivo**]{.hl-yellow} 
* `"M"`: [**multiplicativo**]{.hl-yellow} 
* `"N"`: [**ninguno**]{.hl-yellow} (sin esa componente)

---


## Ejemplo en R


En nuestro caso hemos visto que el alisado simple es para predecir una serie asumiendo que **no hay tendencia ni estacionalidad** (`"N"` en ambos)


```{r}
fit_algeria <-
  algeria_economy |> # f√≠jate que le podemos poner nombre al modelo
  model(alisado_simple = ETS(Exports ~ error("A") + trend("N") + season("N")))
fit_algeria
```


---

## Ejemplo en R

Se [**guarda en un objeto de tipo modelo**]{.hl-yellow} (`mdl_df` o `mable`): para poder ver la info en un **formato tabulado basta hacer `augment()`**: `.fitted` guarda las estimaciones, `.resid` residuales estimados (en este caso coincide con `.innov`)


```{r}
#| code-line-numbers: "2"
fit_algeria |>
  augment()
```



---

## Ejemplo en R


Tras modelizar la serie podemos predecir el futuro con `forecast(h = n_instantes_futuros)` indic√°ndole los **valores futuros** (para convertirlo a un **formato tabla**, dado que ya no es un modelo como antes, debemos convertirlo con `as_tibble()` o `as_tsibble()`)

F√≠jate que en el [**caso de alisado simple todas las predicciones son la misma**]{.hl-yellow} (ya que asume que es localmente constante)


```{r}
#| eval: false
#| code-line-numbers: "3"
predict_algeria <-
  fit_algeria |>
  forecast(h = 7) # 7 instantes futuros
predict_algeria |> as_tsibble()
```

```{r}
#| echo: false
predict_algeria <-
  fit_algeria |>
  forecast(h = 7) # 7 instantes futuros
predict_algeria |> select(.model, Year, .mean, Exports)
```



---

## Ejemplo en R

Podemos [**visualizar el ajuste (del pasado)**]{.hl-yellow} haciendo `augment() + pivot_longer()` (seleccion√°ndole antes las columnas a pivotar)


```{r}
#| code-fold: true
fit_algeria |>
  augment() |>
  as_tibble() |> 
  select(Year, Exports, .fitted) |> 
  rename(export_real = Exports, export_fit = .fitted) |> 
  pivot_longer(cols = -Year, names_to = "serie", values_to = "X_t") |> 
  ggplot() +
  geom_line(aes(x = Year, y = X_t, color = serie)) +
  ggthemes::scale_color_colorblind() +
  scale_x_continuous(n.breaks = 20) +
  theme_minimal() +
  labs(y = "% del PIB", title = "Exports: Algeria")
```


. . .

F√≠jate como el [**alisado simple "llega tarde" a los cambios**]{.hl-red} ya que lo que hace es asumir que todo va a seguir igual


---

## Ejemplo en R

Podemos [**unir las predicciones futuras a los valores ajustados del pasado**]{.hl-yellow} con un `full_join()` (conviertiendo lsa tablas a tibble, seleccionando las columnas que nos interesa y renombr√°ndolas)


```{r}
#| code-line-numbers: "5-6"
fit_predict_algeria <-
  fit_algeria |>
  augment() |>
  as_tibble() |> 
  full_join(predict_algeria |> as_tibble() |> select(Year, .mean),
            by = "Year") |> 
  select(Year, Exports, .fitted, .mean) |> 
  rename(export_real = Exports, export_fit = .fitted, export_predict = .mean)
fit_predict_algeria
```


---

## Ejemplo en R

Tras ello podemos visualizar (haciendo un `pivot_longer()` previo as usual)


```{r}
#| code-fold: true
fit_predict_algeria |>
  pivot_longer(cols = -Year, names_to = "serie", values_to = "X_t") |> 
  ggplot() +
  geom_line(aes(x = Year, y = X_t, color = serie)) +
  ggthemes::scale_color_colorblind() +
  scale_x_continuous(n.breaks = 20) +
  theme_minimal() +
  labs(y = "% del PIB", title = "Exports: Algeria")
```



---

## Ejemplo en R

Tambi√©n podemos simplificar el c√≥digo de la visualizaci√≥n haciendo [**uso de autoplot()**]{.hl-yellow}, que te incluye adem√°s unos **intervalos de confianza**. 


```{r}
#| code-fold: true
predict_algeria |>
  autoplot(algeria_economy) +
  geom_line(data = fit_algeria |> augment(),
            aes(y = .fitted), col = "#D55E00") +
  scale_x_continuous(n.breaks = 20) +
  theme_minimal() +
  labs(y = "% del PIB", title = "Exports: Algeria")
```


# Clase 11: [modelos con fable]{.flow} {#clase-11}

---


## Modelos con fable: MEAN()

No solo vamos a poder aplicar el alisado en esta nueva l√≥gica de programaci√≥n, sino que vamos a poder [**programar de manera sencilla los modelos que hemos investigado hasta ahora**]{.hl-yellow}

. . .

* `MEAN(var_objetivo)`: la predicci√≥n a horizonte $h$ se define como la [**media de todos los valores**]{.hl-yellow} conocidos de la serie $\widehat{X}_{t+h|t} = \frac{1}{t} \sum_{j=1}^{t} X_t$ hasta el instante $t$ (predicci√≥n constante, la que antes llam√°bamos `poly0`)


```{r}
#| code-line-numbers: "5"
airpass <- AirPassengers |> as_tsibble()
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        mean_cte = MEAN(value))
```


---

## Modelos con fable: TSLM()


* `TSLM(var_objetivo ~ formula)`: la predicci√≥n a horizonte $h$ se define como una [**regresi√≥n polin√≥mica**]{.hl-yellow} tal que $\widehat{X}_{t+h|t} = \beta_0 + \sum_{j=1}^{p} \beta_j (t+h)^p$


```{r}
#| code-line-numbers: "5"
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        mean_cte = MEAN(value),
        reg_poly = TSLM(value ~ index))
```



---

## Modelos con fable: RW()


* `RW(var_objetivo ~ drift())`: modelo conocido como [**random walk**]{.hl-yellow} o paseo aleatorio con [**drift o tendencia**]{.hl-yellow} tal que  $X_{t+1} = c +  X_{t} + \varepsilon_t$. La predicci√≥n a horizonte $h$ se define entonces como

$$\widehat{X}_{t+h|t} = c + X_{(t+h-1)|t} \simeq c + \widehat{X}_{(t+h-1)|t} \simeq 2*c +  \widehat{X}_{(t+h-2)|t} \simeq ... \simeq c*h + X_{t}$$


```{r}
#| code-line-numbers: "6"
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        mean_cte = MEAN(value),
        reg_poly = TSLM(value ~ index),
        rw_drift = RW(value ~ drift()))
```


---

## Modelos con fable: NAIVE()


* `NAIVE(var_objetivo)`: el [**modelo naive o trivial**]{.hl-yellow} es una simplificaci√≥n del random walk con $c=0$ (sin tendencia, de hecho `NAIVE()` y `RW()` sin drift hacen lo mismo. La predicci√≥n a horizonte $h$ se define entonces como simplemente el √∫ltimo valor conocido en tiempo $t$.

$$\widehat{X}_{t+h|t} =  X_{t}$$


```{r}
#| code-line-numbers: "7"
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        mean_cte = MEAN(value),
        reg_poly = TSLM(value ~ index),
        rw_drift = RW(value ~ drift()),
        naive = NAIVE(value))
```


---

## Modelos con fable: SNAIVE()

* `SNAIVE(var_objetivo ~ lag())`: similar al modelo anterior solo que en lugar de repetir el √∫ltimo valor [**repite los √∫ltimos valores estacionales**]{.hl-yellow} (por ejemplo, el √∫ltimo mes, el √∫ltimo a√±o, etc).

La predicci√≥n a horizonte $h$ se define entonces como

$$\widehat{X}_{t+h|t} =  X_{\left(t+h \right) - s*(k+1)}$$

* $s$ es el periodo

* $k$ es la parte entera de $(h-1)/s$ (el n√∫mero de periodos completos que han pasado hasta $t+h$). Por ejemplo, si $h = 26$ y $s = 12$, $(h-1)/s = 25/12 = 2.08333$, cuya parte entera es $k = 2$ (han pasado 2 a√±os completos hasta $h = 26$). Si $s = 12$, la predicci√≥n de cualquier de los enero futuros ser√° igual a la predicci√≥n del √∫ltimo enero conocido.

---

## Modelos con fable: SNAIVE()

* `SNAIVE(var_objetivo ~ lag())`: similar al modelo anterior solo que en lugar de repetir el √∫ltimo valor [**repite los √∫ltimos valores estacionales**]{.hl-yellow} (por ejemplo, el √∫ltimo mes, el √∫ltimo a√±o, etc).


```{r}
#| code-line-numbers: "8"
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        mean_cte = MEAN(value),
        reg_poly = TSLM(value ~ index),
        rw_drift = RW(value ~ drift()),
        naive = NAIVE(value),
        season_naive = SNAIVE(value ~ lag(12)))
```



---

## Modelos con fable

As√≠ de toda la colecci√≥n anterior de modelos podemos [**obtener las estimaciones**]{.hl-yellow} de manera sencilla con `augment()`


```{r}
estimaciones <- fit_airpass |> augment()

# Tenemos 144 estimaciones para cada uno de esos modelos
estimaciones |> 
  count(.model)
```


---

## Modelos con fable

As√≠ de toda la colecci√≥n anterior de modelos podemos [**obtener las estimaciones**]{.hl-yellow} de manera sencilla con `augment()`


```{r}
estimaciones
```



---

## Visualizaci√≥n estimaciones

Podemos [**visualizar el ajuste (del pasado)**]{.hl-yellow}


```{r}
#| code-fold: true
estimaciones |>
  ggplot() +
  geom_line(aes(x = index, y = value), color = "black",
            linewidth = 1.2, alpha = 0.7) +
  geom_line(aes(x = index, y = .fitted, color = .model),
            linewidth = 0.75, alpha = 0.8, linetype = 2) +
  scale_color_manual(values = ggthemes::colorblind_pal()(7)[-1]) +
  scale_x_yearquarter(date_breaks = "24 months") +
  theme_minimal()
```


---

## Visualizaci√≥n predicciones

Podemos [**visualizar las predicciones futuras**]{.hl-yellow} con `forecast() + autoplot()` (con `level = NULL` anulamos los intervalos de confianza).


```{r}
fit_airpass |>
  forecast(h = 24) |> 
  autoplot(airpass, level = NULL) +
  scale_color_manual(values = ggthemes::colorblind_pal()(7)[-1]) +
  scale_x_yearquarter(date_breaks = "24 months") +
  theme_minimal()
```


---

## M√©todos de decomposici√≥n

Tambi√©n podemos con el paquete `{feasts}` hacer uso de las [**decomposiciones que hemos aprendido**]{.hl-yellow}

* `classical_decomposition(var_objetivo ~ season(s),  type = ...)`: [**descomposici√≥n cl√°sica**]{.hl-yellow} aprendida de tipo multiplicativa o aditiva. La predicci√≥n a horizonte $h$ se define tal que

$$\widehat{X}_{t+h|t} = \widehat{T}_{t+h|t} + \widehat{S}_{t+h|t} \quad \text{type = "additive"}$$

$$\widehat{X}_{t+h|t} = \widehat{T}_{t+h|t} * \widehat{S}_{t+h|t} \quad \text{type = "mult"}$$


```{r}
#| code-line-numbers: "4"
library(feasts)
fit_airpass <-
  airpass |>
  model(cts = classical_decomposition(value ~ season(12), type = "additive"))
```


---

## M√©todos de decomposici√≥n

Para este tipo particular de m√©todos (**m√©todos de decomposici√≥n**) existe una funci√≥n `components()` que autom√°ticamente nos devuelve la tendencia estimada (`trend`), la estacionalidad (`seasonal`), el residuo estimado (`random`) y la serie desestacionalizada (`season_adjust`, serie - seasonal)


```{r}
#| code-line-numbers: "4"
fit_airpass <-
  airpass |>
  model(cts = classical_decomposition(value ~ season(12), type = "additive"))
fit_airpass |> components()
```


---

## M√©todos de decomposici√≥n

Con `autoplot()` nos [**visualiza cada componente**]{.hl-yellow}


```{r}
#| code-line-numbers: "5-6"
fit_airpass <-
  airpass |>
  model(cts = classical_decomposition(value  ~ season(12), type = "additive"))
fit_airpass |>
  components() |> 
  autoplot()
```


---

## M√©todos de decomposici√≥n

As√≠ quedar√≠a en [**modo multiplicativo**]{.hl-yellow}


```{r}
#| code-line-numbers: "3"
fit_airpass <-
  airpass |>
  model(cts = classical_decomposition(value ~ season(12), type = "mult"))
fit_airpass |>
  components() |> 
  autoplot()
```


---

## M√©todos de decomposici√≥n

F√≠jate que el [**modo multiplicativo es equivalente a tomar el logaritmo del valor objetivo**]{.hl-yellow}


```{r}
#| code-line-numbers: "3"
fit_airpass <-
  airpass |>
  model(cts = classical_decomposition(log(value) ~ season(12), type = "additive"))
fit_airpass |>
  components() |> 
  autoplot()
```


---

## M√©todos de decomposici√≥n

* `STL(var_objetivo ~ trend(window = ..., degre = ...) + season(period = ..., window = "periodic"))`:  la predicci√≥n a horizonte $h$ se define tambi√©n como una [**descomposici√≥n cl√°sica en tendencia y estacionalidad**]{.hl-yellow} pero ahora la [**tendencia es estimada mediante una regresi√≥n local**]{.hl-yellow} conocida como **regresi√≥n LOESS o LOWESS**. Dicha regresi√≥n ajusta a los datos una **regresi√≥n polin√≥mica (degree) pero de manera LOCAL**, en cada punto solo se utiliza un n√∫mero `window` de observaciones).



```{r}
#| code-line-numbers: "4"
library(feasts)
fit_airpass <-
  airpass |>
  model(stl = STL(value ~ trend(window = 7, degree = 1) + season(period = 12, window = "periodic")))
```


---

## M√©todos de decomposici√≥n

Para este tipo particular de m√©todos (**m√©todos de decomposici√≥n STL**) `components()` nos devuelve la tendencia estimada (`trend`), la estacionalidad (`season_s`), el residuo estimado (`remainder`) y la serie desestacionalizada (`season_adjust`, serie - season_s)


```{r}
#| code-line-numbers: "4"
fit_airpass <-
  airpass |>
  model(stl = STL(log(value) ~ trend(window = 7, degree = 1) + season(period = 12, window = "periodic")))
fit_airpass |> components()
```


---

## M√©todos de decomposici√≥n


```{r}
#| code-line-numbers: "3"
fit_airpass |>
  components() |> 
  autoplot()
```


---

## M√©todos de decomposici√≥n


F√≠jate que a menor valor en `window = ...` m√°s err√°tica es la tendencia...


```{r}
#| code-line-numbers: "3"
fit_airpass <-
  airpass |>
  model(stl = STL(log(value) ~ trend(window = 3, degree = 1) + season(period = 12, window = "periodic")))
fit_airpass |>
  components() |> 
  autoplot()
```


---

## M√©todos de decomposici√≥n


... y a mayor valor en `window = ...` m√°s "recta" se convierte.


```{r}
#| code-line-numbers: "3"
fit_airpass <-
  airpass |>
  model(stl = STL(log(value) ~ trend(window = 30, degree = 1) + season(period = 12, window = "periodic")))
fit_airpass |>
  components() |> 
  autoplot()
```


---

## üíª Tu turno


### [**Ejercicio 1**]{.hl-yellow}

üíª Repite el proceso de creaci√≥n de modelos (los fable y los feast) y predicci√≥n (y su visualizaci√≥n) para los conjuntos de datos

* `retiro`
* `global_economy` (solo para Espa√±a) del paquete  `{tsibbledata}`
* `beer_ts` del paquete `{timeSeriesDataSets}`
* `elec_ts` del paquete `{timeSeriesDataSets}`
* `pedestrian` (variable `count`) del paquete `{timeSeriesDataSets}`

Determina a cual de los 9 tipos de series de Pegel pertenece cada una.


# Clase 12: [m√©todos de alisado]{.flow} {#clase-12}

---

## Alisado doble (lineal) de Holt

Como hemos visto el m√©todo anterior de alisado solo ajusta bien la [**hip√≥tesis de que nuestros datos son localmente constantes**]{.hl-yellow} tal que

$$\widehat{X}_{t+1|t} = \theta \widehat{X}_{t|t-1} + \left( 1 - \theta \right)X_t = \theta^{t}\widehat{X}_{1} + \left(1 - \theta \right) \sum_{j=0}^{t-1} \theta^{j} X_{t-j}$$

. . .

A ese **primer valor** disponible se le conoce como [**nivel de la serie**]{.hl-yellow} y lo denotaremos como $\ell_0$

$$\widehat{X}_{t+1|t} = \theta^{t}\widehat{X}_{1} + \left(1 - \theta \right) \sum_{j=0}^{t-1} \theta^{j} X_{t-j} = \theta^{t}\ell_0 +  \sum_{j=0}^{t-1} \left(1 - \theta \right) \theta^{j} X_{t-j}$$

---


## Alisado doble (lineal) de Holt

$$\widehat{X}_{t+1|t} = \theta^{t}\widehat{X}_{1} + \left(1 - \theta \right) \sum_{j=0}^{t-1} \theta^{j} X_{t-j} = \theta^{t}\ell_0 +  \sum_{j=0}^{t-1} \left(1 - \theta \right) \theta^{j} X_{t-j}$$

La ecuaci√≥n anterior la podemos [**expresar de manera iterativa**]{.hl-yellow} como
 
$$\begin{eqnarray}\widehat{X}_{t+h|t} &=& \ell_t \quad \text{predicci√≥n en base a componentes} \nonumber \\ \ell_t &=& \theta \ell_{t-1}+ \left( 1 - \theta \right)X_t \quad \text{suavizado de componentes} \end{eqnarray}$$


---

## Alisado doble (lineal) de Holt

$$\begin{eqnarray}\widehat{X}_{t+h|t} &=& \ell_t \quad \text{predicci√≥n (en base a componentes)} \nonumber \\ \ell_t &=& \theta \ell_{t-1}+ \left( 1 - \theta \right)X_t \quad \text{suavizado de componentes} \end{eqnarray}$$

En 1957 Holt propuso extender este m√©todo a [**datos con tendencia**]{.hl-yellow}, incluyendo ahora dos componentes: nivel $\ell_t$ y tendencia $\mu_t$ (realizando un [**doble suavizado iterativo**]{.hl-yellow})

$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{red}{\ell_t + h*\mu_t} \quad \text{predicci√≥n (en base a componentes)} \nonumber \\ \color{purple}{\ell_{t}} &=& \color{purple}{\theta_1 \widehat{X}_{t|t-1} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \ell_{t-1}+ \mu_{t-1} \right) + \left( 1 - \theta_1 \right)X_t} \quad \text{suavizado nivel} \nonumber \\ \color{green}{\mu_{t}} &=& \color{green}{\theta_2 \mu_{t-1}  + \left( 1 - \theta_2 \right)\widehat{\mu}_{t} = \theta_2 \mu_{t-1}  + \left( 1 - \theta_2 \right) \left( \ell_t - \ell_{t-1} \right)} \quad \text{suavizado tendencia} \end{eqnarray}$$

---

## Alisado doble (lineal) de Holt


$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{red}{\ell_t + h*\mu_t} \quad \text{predicci√≥n (en base a componentes)} \nonumber \\ \color{purple}{\ell_{t}} &=& \color{purple}{\theta_1 \widehat{X}_{t|t-1} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \ell_{t-1}+ \mu_{t-1} \right) + \left( 1 - \theta_1 \right)X_t} \quad \text{suavizado nivel} \nonumber \\ \color{green}{\mu_{t}} &=& \color{green}{\theta_2 \mu_{t-1}  + \left( 1 - \theta_2 \right)\widehat{\mu}_{t} = \theta_2 \mu_{t-1}  + \left( 1 - \theta_2 \right) \left( \ell_t - \ell_{t-1} \right)} \quad \text{suavizado tendencia} \end{eqnarray}$$

. . .

* [**Predicci√≥n a horizonte $h$**]{.hl-red}: √∫ltimo nivel conocido m√°s $h$ veces la √∫ltima tendencia conocida (en $h$ instantes avanza $h$ veces la tendencia)

. . .

* [**Estimaci√≥n nivel**]{.hl-purple}: media ponderada entre el √∫ltimo valor de la serie y $\widehat{X}_{t|t-1}$ (predicci√≥n a horizonte $h = 1$ en tiempo $t$)

. . .

* [**Estimaci√≥n tendencia**]{.hl-green}: media ponderada entre el √∫ltimo valor de la tendencia y la estimaci√≥n de la tendencia a tiempo $t$ (diferencia de nivel $\ell_t$ y $\ell_{t-1}$)


---

## Alisado doble (lineal) de Holt

Para ello simplemente debemos usar `ETS()` incluyendo ahora tendencia aditiva `trend("A")`


```{r}
airpass <- AirPassengers |> as_tsibble()
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        alisado_doble = ETS(value ~ error("A") + trend("A") + season("N")))

estimaciones <- fit_airpass |> augment()
predicciones <- fit_airpass |> forecast(h = 12)
estimaciones
```


---

## Alisado doble (lineal) de Holt


```{r}
#| code-fold: true
predicciones |> 
  autoplot(airpass, level = NULL) +
  geom_line(data = estimaciones, aes(x = index, y = .fitted, color = .model)) +
  geom_line(data = predicciones, aes(x = index, y = .mean, color = .model)) +
  scale_color_manual(values = ggthemes::colorblind_pal()(3)[-1]) +
  theme_minimal()
```



---


## Alisado doble de Gardner-McKenzie

Las predicciones generadas por el alisado doble de Holt nos dan una [**tendencia constante (creciente o decreciente) indefinidamente hacia el infinito**]{.hl-yellow}. Sin embargo, la evidencia emp√≠rica indica que estos m√©todos [**tienden a sobrepronosticar a horizontes de previsi√≥n m√°s largos**]{.hl-yellow}.

. . .

Para solventar esto Gardner y McKenzie (1985) introdujeron un [**par√°metro de amortiguaci√≥n $\phi$**]{.hl-yellow}: la [**predicci√≥n a futuro empieza siendo una l√≠nea recta que termina dobl√°ndose**]{.hl-yellow} hasta acabar en una recta.

---

## Alisado doble de Gardner-McKenzie


$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{red}{\ell_t} + \color{blue}{\left( \phi+\phi^2 + \dots + \phi^{h}\right)*\mu_t} \quad \text{predicci√≥n (en base a componentes)} \nonumber \\ \color{purple}{\ell_{t}} &=& \color{purple}{\theta_1 ( \ell_{t-1}+} \color{blue}{\phi}\color{purple}{\mu_{t-1} ) + \left( 1 - \theta_1 \right)X_t} \quad \text{suavizado nivel} \nonumber \\ \color{green}{\mu_{t}} &=& \color{green}{\theta_2} \color{blue}{\phi\mu_{t-1}} \color{green}{+ \left( 1 - \theta_2 \right) \left( \ell_t - \ell_{t-1} \right)} \quad \text{suavizado tendencia} \end{eqnarray}$$

con $\color{blue}{0 < \phi < 1}$.

. . .

* [**Predicci√≥n a horizonte $h$**]{.hl-red}: √∫ltimo nivel conocido m√°s un amortiguamiento de nivel $h$ de la √∫ltima tendencia conocida

As√≠ cuando $h \to infty$ tenemos que la predicci√≥n acaba siendo una constante.

$$\begin{eqnarray}\lim_{h \to \infty} \widehat{X}_{t+h|t} &=& \ell_t + \lim_{h \to \infty} \left( \phi+\phi^2 + \dots + \phi^{h}\right)*\mu_t \nonumber \\
&=& \ell_t + \frac{\phi}{1-\phi}\mu_t\end{eqnarray}$$

---

## Alisado doble de Gardner-McKenzie

Para ello simplemente debemos usar `ETS()` incluyendo ahora tendencia aditiva `trend("Ad")` (additive dumped)


```{r}
airpass <- AirPassengers |> as_tsibble()
fit_airpass <-
  airpass |>
  model(alisado_simple = ETS(value ~ error("A") + trend("N") + season("N")),
        alisado_doble = ETS(value ~ error("A") + trend("A") + season("N")),
        alisado_doble_amortiguado = ETS(value ~ error("A") + trend("Ad", phi = 0.4) + season("N")))

estimaciones <- fit_airpass |> augment()
predicciones <- fit_airpass |> forecast(h = 8)
estimaciones
```


---

## Alisado doble de Gardner-McKenzie


```{r}
#| code-fold: true
predicciones |> 
  autoplot(airpass, level = NULL) +
  geom_line(data = estimaciones, aes(x = index, y = .fitted, color = .model)) +
  geom_line(data = predicciones, aes(x = index, y = .mean, color = .model)) +
  scale_color_manual(values = ggthemes::colorblind_pal()(4)[-1]) +
  theme_minimal()
```



# Clase 13: [m√©todos de alisado]{.flow} {#clase-13}

---

## Alisado triple de Holt-Winters

Veamos un **repaso de los m√©todos de alisado** explicados.

. . .

* [**Alisado simple**]{.hl-yellow}: la predicci√≥n de la serie a horizonte $h$ es el √∫ltimo nivel de la serie estimado (donde se estim√≥ que la serie la √∫ltima vez). Dicho nivel se obtiene de manera iterativa [**ponderando el nivel previo (la estimaci√≥n anterior) y la √∫ltima observaci√≥n real**]{.hl-yellow}

$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{purple}{\ell_t} \quad \text{predicci√≥n (en base a componentes)} \nonumber \\ \color{purple}{\ell_t} &=& \theta \color{purple}{\ell_{t-1}}+ \left( 1 - \theta \right)X_t \quad \text{suavizado de componentes} \end{eqnarray}$$

---

## Alisado triple de Holt-Winters


* [**Alisado doble**]{.hl-yellow}: la predicci√≥n de la serie a horizonte $h$ es el [**√∫ltimo nivel de la serie estimado (donde se estim√≥ la serie la √∫ltima vez) y la √∫ltima pendiente estimada**]{.hl-yellow}: para predecir donde estar√° en la carretera a las 5h uso donde estaba a las 4h y la pendiente de la carretera en ese momento.

Como antes, el nivel se obtiene iterativamente **ponderando el nivel previo (estimaci√≥n anterior) y la √∫ltima observaci√≥n**. La pendiente se estima de la misma forma, usando la predicci√≥n de la estimaci√≥n y la √∫ltima estimaci√≥n disponible.

$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{purple}{\ell_t} + h*\color{green}{\mu_t} \quad \text{predicci√≥n (en base a componentes)} \nonumber \\ \color{purple}{\ell_{t}} &=& \theta_1 \color{red}{\widehat{X}_{t|t-1}} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \color{purple}{\ell_{t-1}}+ \color{green}{\mu_{t-1}} \right) + \left( 1 - \theta_1 \right)X_t \quad \text{suavizado nivel} \nonumber \\ \color{green}{\mu_{t}} &=& \theta_2 \widehat{\mu}_{t}   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}}= \theta_2 \left( \color{purple}{\ell_t - \ell_{t-1}} \right)   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}} \quad \text{suavizado tendencia} \end{eqnarray}$$

---

## Alisado triple de Holt-Winters

Unos a√±os m√°s tarde, Holt y Winters (1960) ampliaron los m√©todos anteriores para [**poder capturar la estacionalidad**]{.hl-yellow}. Se conoce como [**alisado triple**]{.hl-yellow} ya que tendremos ahora tres ecuaciones de suavizado: suavizado del nivel, suavizado de la tendencia y suavizado de la componente estacional.

. . .

Ahora tendremos 4 par√°metros:

* par√°metros de suavizado $\left(\theta_1, \theta_2, \theta_3 \right)$ 

* par√°metro de estacionalidad $s$

. . .

Y tendremos [**dos tipos de formas de incluir la estacionalidad**]{.hl-yellow}: aditivo (varianza constante a lo largo del tiempo, la componente estacional se expresa en t√©rminos absolutos) y multiplicativo (la componente estacional se expresa en t√©rminos relativos, en porcentajes).

---

## Alisado triple de Holt-Winters

La idea es que ahora, al nivel y la tendencia, para predecir la serie a horizonte $h$ le sumaremos una [**componente estacional**]{.hl-yellow}. Imagina que tenemos una serie mensual con $s= 12$ (periodicidad anual). ¬øCu√°nto valdr√° la predicci√≥n de $t+1$ con los datos hasta $t$?

. . .


$$\color{red}{\widehat{X}_{t+1|t}} = \color{purple}{\ell_t} + \color{green}{\mu_t} + \color{blue}{s_{t+1}}$$

¬øY en $t+2$?

. . .

$$\color{red}{\widehat{X}_{t+2|t}} = \color{purple}{\ell_t} + 2*\color{green}{\mu_t} + \color{blue}{s_{t+2}}$$

F√≠jate que en el [**nivel y la tendencia el valor m√°s reciente que podemos usar es $t$*]{.hl-yellow} pero $s_{t+2}$ s√≠ est√° disponible ya que es peri√≥dica, es decir, $s_{t+2} = s_{(t+2) - s}$

. . .

¬øY si $h = 13$?


---

## Alisado triple de Holt-Winters


$$\color{red}{\widehat{X}_{t+13|t}} = \color{purple}{\ell_t} + 13*\color{green}{\mu_t} + \color{blue}{s_{t+13}}$$
Pero como tenemos una periodicidad $s = 12$, entonces $s_{t+13} = s_{t+13-12} = s_{t+1}$: le [**hemos restado la cantidad de a√±os completos que hab√≠a en $h = 13$**]{.hl-yellow}.

. . .

De manera coloquial podemos expresarlo de manera general como

$$\color{red}{\widehat{X}_{t+h|t}} = \color{purple}{\ell_t} + h*\color{green}{\mu_t} + \color{blue}{s_{(t+h) - s*per.comp}}$$

donde $per.comp$ es el n√∫mero de **periodos completos** (en este caso a√±os) que han pasado en tiempo $h$. Matem√°ticamente es

$$\color{red}{\widehat{X}_{t+h|t}} = \color{purple}{\ell_t} + h*\color{green}{\mu_t} + \color{blue}{s_{(t+h) - s*(k + 1)}}, \quad k = \lfloor \frac{h-1}{s} \rfloor$$

---

## Alisado triple de Holt-Winters


$$\color{red}{\widehat{X}_{t+h|t}} = \color{purple}{\ell_t} + h*\color{green}{\mu_t} + \color{blue}{s_{(t+h) - s*per.comp}}$$
Se conoce como [**alisado triple**]{.hl-yellow} ya que tendremos que suavizar de manera iterativa tres componentes: nivel, tendencia y estacionalidad. 

. . .

Si entendemos el **nivel como ¬´en que punto est√° la serie¬ª**, y dado que la componente estacional es c√≠clica (¬´por d√≥nde va la tasa de paro¬ª deber√≠a ser ajeno a si es navidad o verano), el [**nivel ser√° suavizado ponderando la √∫ltima predicci√≥n y la serie desestacionalizada por el periodo previo**]{.hl-yellow}


$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{purple}{\ell_t} + h*\color{green}{\mu_t} + \color{blue}{s_{(t+h) - s*per.comp}} \nonumber \\ \color{purple}{\ell_{t}} &=& \theta_1 \color{red}{\widehat{X}_{t|t-1}} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \color{purple}{\ell_{t-1}}+ \color{green}{\mu_{t-1}} \right) + \left( 1 - \theta_1 \right) \left(X_t - \color{blue}{s_{t-s}} \right) \nonumber \\ \color{green}{\mu_{t}} &=& \theta_2 \widehat{\mu}_{t}   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}}= \theta_2 \left( \color{purple}{\ell_t - \ell_{t-1}} \right)   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}}\end{eqnarray}$$

---

## Alisado triple de Holt-Winters

Por √∫ltimo, el suavizado de la estacionalidad ser√° similar: una [**ponderaci√≥n entre la estimaci√≥n de la misma**]{.hl-yellow} y la [**√∫ltima estacionalidad (hace s periodos)**]{.hl-yellow}

$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{purple}{\ell_t} + h*\color{green}{\mu_t} + \color{blue}{s_{(t+h) - s*per.comp}} \nonumber \\ \color{purple}{\ell_{t}} &=& \theta_1 \color{red}{\widehat{X}_{t|t-1}} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \color{purple}{\ell_{t-1}}+ \color{green}{\mu_{t-1}} \right) + \left( 1 - \theta_1 \right) \left(X_t - \color{blue}{s_{t-s}} \right) \nonumber \\ \color{green}{\mu_{t}} &=& \theta_2 \widehat{\mu}_{t}   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}}= \theta_2 \left( \color{purple}{\ell_t - \ell_{t-1}} \right)   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}} \nonumber \\ \color{blue}{s_{t}} &=& \theta_3 \color{blue}{\widehat{s}_{t}} + \left( 1 - \theta_3 \right) \color{blue}{s_{t-s}} = \theta_3(X_{t}-\color{purple}{\ell_{t-1}} - \color{green}{\mu_{t-1}}) + \left( 1 - \theta_3 \right) \color{blue}{s_{t-s}},\end{eqnarray}$$

---

## Alisado triple de Holt-Winters

Para realizar el triple alisado simplemente debemos usar `ETS()` incluyendo ahora estacionalidad (aditiva `season("A")` o  multiplicativa `season("M")`)


```{r}
airpass <- AirPassengers |> as_tsibble()
fit_airpass <-
  airpass |>
  model("alisado_simple" = ETS(value ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(value ~ error("A") + trend("A") + season("N")),
        "alisado_triple" = ETS(value ~ error("A") + trend("A") + season("A")))

estimaciones <- fit_airpass |> augment()
predicciones <- fit_airpass |> forecast(h = 36)
estimaciones
```


---

## Alisado triple de Holt-Winters

F√≠jate que las ¬´monta√±itas¬ª ahora suben pero no incrementan su altura, simplemente acaban un poco m√°s arriba ya que tenemos una componente de tendencia.


```{r}
#| code-fold: true
predicciones |> 
  autoplot(airpass, level = NULL) +
  geom_line(data = estimaciones, aes(x = index, y = .fitted, color = .model)) +
  geom_line(data = predicciones, aes(x = index, y = .mean, color = .model)) +
  scale_color_manual(values = ggthemes::colorblind_pal()(4)[-1]) +
  theme_minimal()
```


---

## Alisado triple de Holt-Winters

$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \color{purple}{\ell_t} + h*\color{green}{\mu_t} + \color{blue}{s_{(t+h) - s*per.comp}} \nonumber \\ \color{purple}{\ell_{t}} &=& \theta_1 \color{red}{\widehat{X}_{t|t-1}} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \color{purple}{\ell_{t-1}}+ \color{green}{\mu_{t-1}} \right) + \left( 1 - \theta_1 \right) \left(X_t - \color{blue}{s_{t-s}} \right) \nonumber \\ \color{green}{\mu_{t}} &=& \theta_2 \widehat{\mu}_{t}   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}}= \theta_2 \left( \color{purple}{\ell_t - \ell_{t-1}} \right)   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}} \nonumber \\ \color{blue}{s_{t}} &=& \theta_3 \color{blue}{\widehat{s}_{t}} + \left( 1 - \theta_3 \right) \color{blue}{s_{t-s}} = \theta_3(X_{t}-\color{purple}{\ell_{t-1}} - \color{green}{\mu_{t-1}}) + \left( 1 - \theta_3 \right) \color{blue}{s_{t-s}},\end{eqnarray}$$

La [**versi√≥n multiplicativa**]{.hl-yellow} ser√≠a realizar el mismo alisado pero la estacionalidad ahora [**siempre aparece multiplicando**]{.hl-yellow}

. . .

$$\begin{eqnarray}\color{red}{\widehat{X}_{t+h|t}} &=& \left(\color{purple}{\ell_t} + h*\color{green}{\mu_t} \right) \color{blue}{s_{(t+h) - s*per.comp}} \nonumber \\ \color{purple}{\ell_{t}} &=& \theta_1 \color{red}{\widehat{X}_{t|t-1}} + \left( 1 - \theta_1 \right)X_t = \theta_1 \left( \color{purple}{\ell_{t-1}}+ \color{green}{\mu_{t-1}} \right) + \left( 1 - \theta_1 \right) \frac{X_t}{\color{blue}{s_{t-s}}} \nonumber \\ \color{green}{\mu_{t}} &=& \theta_2 \widehat{\mu}_{t}   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}}= \theta_2 \left( \color{purple}{\ell_t - \ell_{t-1}} \right)   + \left( 1 - \theta_2 \right) \color{green}{\mu_{t-1}} \nonumber \\ \color{blue}{s_{t}} &=& \theta_3 \color{blue}{\widehat{s}_{t}} + \left( 1 - \theta_3 \right) \color{blue}{s_{t-s}} = \theta_3\left(\frac{X_{t}}{\color{purple}{\ell_{t-1}} + \color{green}{\mu_{t-1}}} \right) + \left( 1 - \theta_3 \right) \color{blue}{s_{t-s}},\end{eqnarray}$$

---


## Alisado triple de Holt-Winters

Si `season("M")` el residuo tambi√©n ser√° multiplicativo  `error("M")`


```{r}
airpass <- AirPassengers |> as_tsibble()
fit_airpass <-
  airpass |>
  model("alisado_simple" = ETS(value ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(value ~ error("A") + trend("A") + season("N")),
        "alisado_triple_ad" = ETS(value ~ error("A") + trend("A") + season("A")),
        "alisado_triple_mult" = ETS(value ~ error("M") + trend("A") + season("M")))

estimaciones <- fit_airpass |> augment()
predicciones <- fit_airpass |> forecast(h = 36)
estimaciones
```


---

## Alisado triple de Holt-Winters



```{r}
#| code-fold: true
predicciones |> 
  autoplot(airpass, level = NULL) +
  geom_line(data = estimaciones, aes(x = index, y = .fitted, color = .model)) +
  geom_line(data = predicciones, aes(x = index, y = .mean, color = .model)) +
  scale_color_manual(values = ggthemes::colorblind_pal()(5)[-1]) +
  theme_minimal()
```



# Clase 14: [evaluaci√≥n]{.flow} {#clase-14}

---


## Diagnosis errores

Como ya hicimos una vez en clase, es importante realizar una [**diagnosis correcta de los residuales**]{.hl-yellow}. Para ello tenemos la funci√≥n `gg_tsresiduals()` del paquete `{feasts}` que nos permite visualizar su **evoluci√≥n temporal**, sus **autocorrelaciones** y su **distribuci√≥n**


```{r}
#| eval: false
alisado_simple <-
  airpass |>
  model("alisado_simple" = ETS(value ~ error("A") + trend("N") + season("N")))

alisado_simple |>
  gg_tsresiduals()
```


---

## Diagnosis errores

Por ejemplo en el caso del alisado simple observamos como los [**errores van aumentando seg√∫n avanza el tiempo debido a la heterocedasticidad**]{.hl-red} de la serie, con magnitudes muy elevadas, y teniendo adem√°s unas autocorrelaciones muy altas en algunos retardos.


```{r}
#| code-fold: true
alisado_simple <-
  airpass |>
  model("alisado_simple" = ETS(value ~ error("A") + trend("N") + season("N")))

alisado_simple |>
  gg_tsresiduals()
```



---


## Diagnosis errores

En el caso del alisado triple multiplicativo observamos como los [**errores ya no aumentan seg√∫n avanza el tiempo (magnitudes peque√±as)**]{.hl-green}, y teniendo adem√°s unas autocorrelaciones casi todas dentro de la banda.



```{r}
#| code-fold: true
alisado_triple_mult <-
  airpass |>
  model("alisado_triple_mult" = ETS(value ~ error("M") + trend("A") + season("M")))

alisado_triple_mult |>
  gg_tsresiduals()
```


---

## Train vs test

![](https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png)

Como sucede en otro tipo de modelos, es importante darse cuenta de que a la hora de [**evaluar un modelo**]{.hl-yellow} tendremos que considerar dos aspectos diferentes:


* ¬øC√≥mo funciona el modelo con los [**datos que conoce**]{.hl-yellow}? Es lo que hemos llamado hasta ahora estimaciones.

* ¬øC√≥mo funcionar√≠a el modelo con unos [**datos que no conoce**]{.hl-purple}? Es lo que hemos llamado hasta ahora predicciones

. . .

El problema es que las [**predicciones hasta ahora no pod√≠amos evaluarlas**]{.hl-red} ya que el dato real del futuro no lo tenemos...¬øY si [**partimos nuestras series (train y test)**]{.hl-yellow} de forma que solo le dejamos usar una parte de la informaci√≥n para dise√±ar el modelo, y as√≠ poder evaluarlo en el otro subconjunto?


---

## Train vs test

Vamos a [**dividir nuestra serie temporal de pasajeros a√©reos**]{.hl-yellow} en train y test. Normalmente el [**% de datos en test es igual al horizonte al que queremos evaluar**]{.hl-yellow} c√≥mo funciona (por ejemplo, si $h = 24$, usaremos los 2 √∫ltimos a√±os como test).


```{r}
airpass_train <-
  airpass |> 
  filter(year(index) <= 1958)

airpass_test <-
  airpass |> 
  filter(year(index) > 1958)
```



---

## Evaluaci√≥n

Lo que haremos por tanto ser√° [**pasarle solo la info de `airpass_train` al modelo**]{.hl-yellow}. Tras ello [**predeciremos a horizonte $h = 24$**]{.hl-yellow}


```{r}
airpass_fit <-
  airpass_train |>
  model("mean_cte" = MEAN(value),
        "alisado_simple" = ETS(value ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(value ~ error("A") + trend("A") + season("N")),
        "alisado_triple_ad" = ETS(value ~ error("A") + trend("A") + season("A")),
        "alisado_triple_mult" = ETS(value ~ error("M") + trend("A") + season("M")))
estimaciones <- airpass_fit |> augment()
predicciones <- airpass_fit |> forecast(h = 24)
```


---


## Evaluaci√≥n

En el gr√°fico le indicaremos las **predicciones futuras** (de datos que tenemos guardados en el **dataset completo airpass**)


```{r}
#| code-fold: true
predicciones |> 
  autoplot(airpass, level = NULL) +
  geom_line(data = estimaciones,
            aes(x = index, y = .fitted, color = .model), linewidth = 0.75) +
  scale_color_manual(values = ggthemes::colorblind_pal()(6)[-1]) +
  theme_minimal()
```



---


## Evaluaci√≥n


Con la funci√≥n `accuracy(estimaciones)` y `accuracy(predicciones, datos_test)` podemos evaluar **m√©tricas de error** en test

$$\widehat{\varepsilon}_{t+h|t} = X_{t+h|t} - \widehat{X}_{t+h|t}$$



```{r}
accuracy(fit_airpass)
accuracy(predicciones, airpass_test)
```


---



## Evaluaci√≥n

$$\widehat{\varepsilon}_{t+h|t} = X_{t+h|t} - \widehat{X}_{t+h|t}$$

* [**Error medio (ME)**]{.hl-yellow}: definido como $\frac{1}{n} \sum_{t=1}^{n}  \widehat{\varepsilon}_{t}$

* [**Error medio absoluto (MAE)**]{.hl-yellow}: definido como $\frac{1}{n} \sum_{t=1}^{n}  \left| \widehat{\varepsilon}_{t} \right|$

* [**Error cuadr√°tico medio (RSME)**]{.hl-yellow}: definido como $\frac{1}{n} \sum_{t=1}^{n}  \widehat{\varepsilon}_{t}^2$

Ambos [**dependen de la escala de los datos**]{.hl-red}


```{r}
accuracy(predicciones, airpass_test)
```


---

## Evaluaci√≥n


$$\widehat{\varepsilon}_{t+h|t} = X_{t+h|t} - \widehat{X}_{t+h|t}$$

* [**Error  medio porcentual (MPE)**]{.hl-yellow}: definido como $\frac{1}{n} \sum_{t=1}^{n}  100* \frac{\widehat{\varepsilon}_{t} }{X_t}$

* [**Error absoluto medio porcentual (MAPE)**]{.hl-yellow}: definido como

$$\frac{1}{n} \sum_{t=1}^{n}  100*\left| \frac{\widehat{\varepsilon}_{t} }{X_t}\right|$$


Ambos son [**adimensionales**]{.hl-green} pero [**pueden tender a infinito si $X_t \to 0$**]{.hl-red}


```{r}
accuracy(predicciones, airpass_test)
```



---

## Validaci√≥n cruzada

Como suele ser habitual en el campo de la calibraci√≥n de modelos, una opci√≥n muy habitual es la de la [**validaci√≥n**]{.hl-yellow}:

1. Construir distintos modelos con la informaci√≥n de train

2. Usar los [**conjuntos de la validaci√≥n para evaluar los modelos (o qu√© configuraci√≥n de hiperpar√°metros)**]{.hl-yellow} y decidir cu√°l de ellos  es mejor

3. Una vez elegido el modelo, volver a lanzarlo y evaluarlo en test

---

## Validaci√≥n cruzada

Una de las formas de validaci√≥n m√°s habitual es la [**validaci√≥n cruzada**]{.hl-yellow}: las observaciones del conjunto de train van [**rotando su rol**]{.hl-yellow}.

. . .

Por ejemplo, si tenemos 100 observaciones en train, podemos hacer 100 iteraciones de validaci√≥n, de manera que en cada una entrenamos el modelo con 99 de ellas y otra queda reservada solo para evaluar los modelos.

![](https://interactivechaos.com/sites/default/files/styles/max_800_px/public/2023-03/tutorial_ml_0237.png)

---

## Validaci√≥n cruzada

En el caso de las series temporales una estrategia habitual suele ser la siguiente:

1. Descartar las primeras $n$ observaciones para validaci√≥n: habr√° un conjunto m√≠nimo que siempre formar√° parte de train

2. Iteraci√≥n i: entrenamos con las primeras $n+i$ observaciones, evaluamos con una √∫nica observaci√≥n $n+i+1$. 

3. Realizamos el promedio de las m√©tricas de evaluaci√≥n obtenidas de los conjuntos de validaci√≥n.

![](https://otexts.com/fpp3/fpp_files/figure-html/cv1-1.png)


---

## Validaci√≥n cruzada

F√≠jate que lo anterior est√° basado en una [**one-step forecast**]{.hl-yellow} (predicci√≥n a horizonte $h = 1$), pero quiz√°s nuestro inter√©s est√© en ver c√≥mo funciona nuestro m√©todo a **horizontes de predicci√≥n mayores**

. . .

1. Descartar las primeras $n$ observaciones para validaci√≥n: habr√° un conjunto m√≠nimo que siempre formar√° parte de train

2. Iteraci√≥n i: entrenamos con las primeras $n+i$ observaciones, evaluamos con una √∫nica observaci√≥n $n+i+h$. 

3. Realizamos el promedio de las m√©tricas de evaluaci√≥n obtenidas de los conjuntos de validaci√≥n.

---

## Validaci√≥n cruzada


El ejemplo inferior es para $h = 4$.

![](https://otexts.com/fpp3/fpp_files/figure-html/cv4-1.png)

---

## Validaci√≥n cruzada

Para generar los subconjuntos vamos primero como antes a dividir nuestro dataset en train y test.


```{r}
airpass_train <-
  airpass |> filter(year(index) < 1958)

airpass_test <-
  airpass |> filter(year(index) >= 1958)
```


---

## Validaci√≥n cruzada


Tras ello vamos a [**generar los subconjuntos de validaci√≥n usando trian**]{.hl-yellow}  con `stretch_tsibble()`, indic√°ndole el n√∫mero de valores iniciales que siempre estar√°n en train, el tama√±o que queremos incrementar los sucesivos conjuntos y un identificador de cada slot

Por ejemplo, vamos a reservar los 2 primeros a√±os y vamos a avanzar a horizonte 1.


```{r}
airpass_cv <-
  airpass_train |> 
  stretch_tsibble(.init = 24, .step = 1, .id = "cv")
airpass_cv
```



---

## Validaci√≥n cruzada

Tras generar los slots de validaci√≥n [**entrenamos los modelos con dichos datos**]{.hl-yellow}


```{r}
airpass_fit <-
  airpass_cv |>
  model("alisado_simple" = ETS(value ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(value ~ error("A") + trend("A") + season("N")),
        "alisado_triple_ad" = ETS(value ~ error("A") + trend("A") + season("A")),
        "alisado_triple_mult" = ETS(value ~ error("M") + trend("A") + season("M")))
estimaciones <- airpass_fit |> augment()
```


---

## Validaci√≥n cruzada

Tendremos las [**m√©tricas para cada modelo y cada slot de cv**]{.hl-yellow} que podemos promediar


```{r}
airpass_fit |> 
  accuracy() |> 
  summarise(across(c(ME, RMSE, MAE, MPE, MAPE), mean), .by = .model)

airpass_fit |>
  forecast(h = 1) |> 
  accuracy(airpass_test) |> 
  summarise(across(c(ME, RMSE, MAE, MPE, MAPE), mean), .by = .model)
```


---


## Validaci√≥n cruzada

¬øC√≥mo [**visualizar las m√©tricas de validaci√≥n cruzada**]{.hl-yellow}


```{r}
#| code-fold: true
airpass_fit |>
  accuracy() |> 
  ggplot(aes(x = .model, y = RMSE, fill = .model, color = .model)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter(width = 0.25, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
```

```{r}
#| eval: false
#| echo: false
airpass_fit |>
  forecast(h = 12) |>
  mutate("h" = row_number(), .by = c(.model, cv)) |>
  accuracy(airpass_test, by = c(".model", "h")) |> 
  ggplot(aes(x = h, y = RMSE, color = .model)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_line(alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  scale_x_continuous(breaks = 1:12) +
  theme_minimal()
```


---

## Ejemplo real: AEMET again

1. [**Cargamos**]{.hl-yellow} los datos


```{r}
#| code-fold: true
library(readr) # de tidyverse
retiro <- read_csv(file = "./datos/retiro_temp.csv")
retiro
```


---

## Ejemplo real: AEMET again

1. [**Convertimos a tsibble**]{.hl-yellow} los datos


```{r}
#| code-fold: true
retiro_ts <-
  retiro |>
  # index: variable temporal
  # key: si tuvi√©ramos varias series a la vez (varias estaciones)
  # regular = TRUE: regular time interval (para que detecte
  # adecuadamente la periodicidad de la serie, en este caso [1D])
  as_tsibble(index = fecha, key = NULL, regular = TRUE)
retiro_ts
```


---

## Ejemplo real: AEMET again

2. [**Preprocesamos**]{.hl-yellow} los datos. ¬øHay huecos?


```{r}
#| code-fold: true
#| eval: false
retiro_ts |> 
  count(year(fecha)) |> 
  arrange(n)
```


. . . 

Vemos que, am√©n de los datos que faltan (l√≥gicamente) en 2024, hay datos que faltan en 2022, as√≠ que antes [**debemos rellenar los huecos**]{.hl-yellow}: primero creando la fila (con valores vac√≠os) y luego rellenando la variable objetivo


```{r}
retiro_ts <-
  retiro_ts |> 
  fill_gaps() |> 
  fill(tmed, .direction = "down")
```


---

## Ejemplo real: AEMET again

3. [**Visualizamos**]{.hl-yellow} los datos.


```{r}
#| code-fold: true
ggplot(retiro_ts) +
  geom_line(aes(x = fecha, y = tmed)) +
  theme_minimal()
```


---

## Ejemplo real: AEMET again

3. [**¬øTiene tendencia?**]{.hl-yellow}


```{r}
#| code-fold: true
ggplot(retiro_ts, aes(x = fecha, y = tmed)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```


(en caso de existir desde luego es **aditiva**)

---

## Ejemplo real: AEMET again

3. [**¬øTiene tendencia?**]{.hl-yellow}


```{r}
lm(data = retiro_ts, formula = tmed ~ fecha) |> 
  summary()
```


Existe una **d√©bil (pero significativa) tendencia positiva**


---

## Ejemplo real: AEMET again


3. Determinar [**homocedasticidad/heterocedasticidad**]{.hl-yellow} y c√≥mo evoluciona su varianza.

Una primera opci√≥n es [**visualizaci√≥n de la dispersi√≥n**]{.hl-yellow} por periodos (por ejemplo, por meses)


```{r}
#| code-fold: true
retiro_ts <-
  retiro_ts |> 
  mutate("year_month" = yearmonth(fecha))
resumen_var <-
  retiro_ts |>
  index_by(year_month) |> 
  summarise("dispersion" = sd(tmed) / mean(tmed))

ggplot(resumen_var, aes(x = year_month, y = dispersion)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```



. . .

Al margen de posibles outliers: [**no hay un patr√≥n ni tendencia en la varianza**]{.hl-yellow}

---

## Ejemplo real: AEMET again

De hecho si realizamos el [**ajuste de un recta a la propia varianza agrupada**]{.hl-yellow} vemos que no es significativo: no hay evidencias de que exista una tendencia en la varianza.


```{r}
lm(data = resumen_var, formula = dispersion ~ year_month) |>
  summary()
```


---

## Ejemplo real: AEMET again

Recuerda que [**si tuvi√©ramos heterocedasticidad tendr√≠amos que aplicar alg√∫n tipo de transformaci√≥n**]{.hl-yellow} a los datos, conocidas como [**transformaciones Box-Cox**]{.hl-yellow} (adaptadas por [Bickel y Doksum (1981)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477649) para valores negativos):

$$\begin{equation}\widetilde{X}_t  = \begin{cases}  \log(X_t) & \text{si } \lambda=0  \nonumber \\ \frac{\text{sign}(X_t)|X_t|^\lambda-1}{\lambda} & \text{en otro caso} \end{cases}\end{equation}$$

. . .

¬øQu√© $\lambda$ ser√≠a **si no tuvi√©ramos que hacer ninguna transformaci√≥n (es decir, homoced√°stica)**?


---

## Ejemplo real: AEMET again

Para calcularlo vamos a usar `features(variable, features = ...)` del paquete `{fabletools}` (ya cargado en `{fable}`). En `features = ...` vamos a especificar la opci√≥n `guerrero` (un m√©todo para elegir el $\lambda$ √≥ptimo del paquete `{feasts}`)


```{r}
library(fable)
library(feasts)
retiro_ts |>
  features(tmed, features = guerrero)
```


. . .

Ese valor est√° muy cerca de $1$ lo que apoya lo anterior: salvo outliers, no tenemos heterocedasticidad.

---

## Ejemplo real: AEMET again


4. [**Determinar periodicidad**]{.hl-yellow} $s$



```{r}
pacf(retiro_ts$tmed)
```



No vemos a priori estacionalidad...porque recuerda que aqu√≠ tenemos datos diarios. ¬øY si aumentamos el n√∫mero de retardos?

---

## Ejemplo real: AEMET again



```{r}
pacf(retiro_ts$tmed, lag.max = 365*2)
```


Cuesta verlo pero...¬øves esa peque√±a monta√±ita en el 365?

---

## Ejemplo real: AEMET again


Esto tambi√©n lo podemos [**visualizar de manera conjunta**]{.hl-yellow} con `gg_tsdisplay()`


```{r}
retiro_ts |> 
  gg_tsdisplay(y = tmed, lag = 365)
```


. . .

F√≠jate que adem√°s de mostrarnos un **patr√≥n en las autocorrelaciones cada 365 valores** nos pinta la serie cada a√±o (cada 365 valores corta una curva) y vemos que [**efectivamente comparten un patr√≥n**]{.hl-yellow}



---

## Ejemplo real: AEMET again

Otra forma de comprobar que efectivamente es estacional $s = 365$  es haciendo una [**diferenciaci√≥n estacional de la serie**]{.hl-yellow}: a cada valor le vamos a restar su valor $s$ periodos previos. 

&nbsp;

La funci√≥n que nos lo permite se llama `difference()` (del paquete `{tsibble}`)

---

## Ejemplo real: AEMET again

* Si $s$ est√° [**mal elegido seguir√° presentando ese patr√≥n**]{.hl-red} (o uno a√∫n m√°s extra√±o)


```{r}
retiro_ts |>
  gg_tsdisplay(difference(tmed, 123), lag = 365)
```


---

## Ejemplo real: AEMET again

* Si $s$ est√° [**bien elegido las autocorrelaciones deber√≠a de desplomarse**]{.hl-green}


```{r}
retiro_ts |>
  gg_tsdisplay(difference(tmed, 365), lag = 365)
```


---

## Ejemplo real: AEMET again

* Si $s$ est√° [**bien elegido las autocorrelaciones deber√≠a de desplomarse**]{.hl-green}


```{r}
retiro_ts |>
  gg_tsdisplay(difference(tmed, 365), lag = 365,
               plot_type = "partial")
```


---

## Ejemplo real: AEMET again


Una vez que tenemos que $s = 365$ vamos a [**separar la muestra de momento solo en train-test**]{.hl-yellow}, por ejemplo para evaluar c√≥mo funcionan los modelos a 3 a√±os vista (es decir, usando los √∫ltimos 3 a√±os como test, en los que tenemos $h = 974$ valores)


```{r}
retiro_ts_train <-
  retiro_ts |> 
  filter(year(fecha) < 2022)

retiro_ts_test <-
  retiro_ts |> 
  filter(year(fecha) >= 2022)
```



---

## Ejemplo real: AEMET again

Vamos a realizar los 4 [**m√©todos de alisado**]{.hl-yellow} disponibles (simple-doble-triple aditivo-triple multiplicativo)


```{r}
fit_retiro <-
  # entrenamos SOLO con train
  retiro_ts_train |> 
   model("alisado_simple" = ETS(tmed ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(tmed ~ error("A") + trend("A") + season("N")),
        "alisado_triple_ad" = ETS(tmed ~ error("A") + trend("A") + season("A", period = "1 year")),
        "alisado_triple_m" = ETS(tmed ~ error("M") + trend("A") + season("M", period = "1 year")))
```


Si te fijas sale un error ya que los [**alisados en `model()` tienen una limitaci√≥n**]{.hl-red}: no permite un $s$ mayor de 24


---

## Ejemplo real: AEMET again

Para poder solventarlo (veremos los ARIMA para poder usarlos) de momento vamos a [**resumir la serie, realizando un resumen mensual de la temperatura**]{.hl-yellow}


```{r}
retiro_ts_monthly_train <-
  retiro_ts_train |>
  index_by("year_month" = yearmonth(fecha)) |>
  summarise("tmed" = mean(tmed))

retiro_ts_monthly_test <-
  retiro_ts_test |>
  index_by("year_month" = yearmonth(fecha)) |>
  summarise("tmed" = mean(tmed))
```

```{r}
# code-fold: true
ggplot(retiro_ts_monthly_train) +
  geom_line(aes(x = year_month, y = tmed)) +
  theme_minimal()
```


---

## Ejemplo real: AEMET again



```{r}
fit_retiro <-
  # entrenamos SOLO con train
  retiro_ts_monthly_train |> 
   model("alisado_simple" = ETS(tmed ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(tmed ~ error("A") + trend("A") + season("N")),
        "alisado_triple_ad" = ETS(tmed ~ error("A") + trend("A") + season("A", period = "1 year")),
        "alisado_triple_m" = ETS(tmed ~ error("M") + trend("A") + season("M", period = "1 year")))
estimaciones <- fit_retiro |> augment()
predicciones <- fit_retiro |> forecast(h = 12*2 + 8)
```


---

## Ejemplo real: AEMET again



```{r}
#| code-fold: true
predicciones |> 
  autoplot(retiro_ts |>
             index_by("year_month" = yearmonth(fecha)) |>
             summarise("tmed" = mean(tmed)), level = NULL) +
  geom_line(data = estimaciones, aes(x = year_month, y = .fitted, color = .model)) +
  geom_line(data = predicciones, aes(x = year_month, y = .mean, color = .model)) +
  scale_color_manual(values = ggthemes::colorblind_pal()(5)[-1]) +
  theme_minimal()
```


---

## Ejemplo real: AEMET again


La evaluaci√≥n num√©rica la haremos con `accuracy()`: el **triple aditivo funciona ligeramente mejor en train pero ligeramente peor en test**. Dado que funcionan similar y el aditivo es m√°s simple, por principio de parsimonia, nos quedamos con el triple aditivo.


```{r}
accuracy(fit_retiro)
accuracy(predicciones, retiro_ts_monthly_test)
```



---

## Ejemplo real: AEMET again

El [**diagn√≥stico de los residuos**]{.hl-yellow} (de momento solo visual, ya veremos con lso ARIMA como testar que los residuos son como los queremos, normales, etc) lo haremos con `gg_tsresiduals()` sobre el modelo "ganador"


```{r}
retiro_ts_monthly_train |> 
   model("alisado_triple_ad" = ETS(tmed ~ error("A") + trend("A") + season("A", period = "1 year"))) |> 
  gg_tsresiduals()
```



---


## Casos reales

Practica para a [**ejecutar todo el proceso**]{.hl-yellow} con estas series


```{r}
datos <- tsibbledata::gafa_stock |> filter(Symbol == "GOOG")
datos <- timeSeriesDataSets::beer_ts |> as_tsibble()
datos <- timeSeriesDataSets::co2_ts |> as_tsibble()
```


1. Preprocesa los datos de manera adecuada

2. An√°lisis y visualizaci√≥n descriptiva. Determinar si/no tendencia (y de qu√© tipo)

3. Determinar homocedasticidad/heterocedasticidad y c√≥mo evoluciona su varianza. En caso necesario aplicar transformaci√≥n.


---

## Casos reales

Prueba a [**ejecutar todo el proceso**]{.hl-yellow} con estas series


```{r}
datos <- tsibbledata::gafa_stock |> filter(Symbol == "GOOG")
datos <- timeSeriesDataSets::beer_ts |> as_tsibble()
datos <- timeSeriesDataSets::co2_ts |> as_tsibble()
```


4. Determinar periodicidad $s$

5. Realizar todos los m√©todos de alisado conocidos as√≠ como otros modelos que sepas de fable. Pensar ANTES de ver su resultado c√≥mo crees que van a funcionar y por qu√©

6. Evaluarlos en train-test

7. Usa el mejor de los m√©todos posibles en cada datasets y ahora ajustarlo pero con distintos valores de los par√°metros $\alpha$, $\beta$ y $\gamma$ y decidir los 3 mejores modelos haciendo uso de la validaci√≥n cruzada

---

## Casos reales: acciones de google

Vamos a ilustrar el inicio de c√≥mo trabajar con la [**serie temporal que captura el precio de cierre de las acciones de Google**]{.hl-yellow} (variable `Close` del paquete `{tsibbledata::gafa_stock}`)


```{r}
datos <- tsibbledata::gafa_stock |> filter(Symbol == "GOOG")
datos
```


---

## Acciones de google


1. [**Preprocesa los datos**]{.hl-yellow} de manera adecuada

¬øHay huecos?


```{r}
#| code-fold: true
#| eval: false
datos |> 
  count(year(Date)) |> 
  arrange(n)
```


Si te fijas [**ning√∫n a√±o tiene 365 valores**]{.hl-red} ya que los mercados burs√°tiles cierran los fines de semana. 

---

## Acciones de google

Para [**rellenar los huecos**]{.hl-yellow} vamos a usar primero `fill_gaps()` rellenando las fechas que no tenemos (con valores vac√≠os) y luego `fill(..., .direction = "down")` para **rellenar la variable objetivo** con los valores previos (el s√°badon y domingo ser√° el valor de cierre del viernes).


```{r}
datos  <-
  datos |> 
  fill_gaps() |> 
  fill(Close, .direction = "down")

datos |> 
  count(year(Date)) |> 
  arrange(n)
```


Si te fijas...[**¬°no hecho absolutamente nada!**]{.hl-red}

---

## Acciones de google

Si te fijas lo primero que estamos pidiendo es `fill_gaps()`: rellena huecos entre fechas. Por ejemplo, si los datos son diarios, debe buscar huecos entre d√≠as, pero para eso lo primero que tiene que saber es que la [**serie es diaria**]{.hl-yellow} y saber c√≥mo debe rellenar huecos.

. . .

Si te fijas en la [**cabecera del `tsibble`**]{.hl-yellow} aparece esto:


```{r}
# A tsibble: 1,258 x 8 [!]
# Key:       Symbol [1]
```


. . .

En `Key:` no deber√≠a figurar nada (ya que no tenemos distintas series al haber filtrado una sola) pero sobre todo...en [!]` deber√≠a figurar `[1D]` [**¬°pero no figura nada!**]{.hl-red}

---

## Acciones de google

Para arreglarlo vamos a [**redefinir el objeto de serie temporal**]{.hl-yellow} indic√°ndole dentro de `as_tsibble()` que 

* `index` variable temporal
* `key`: si tuvi√©ramos varias series a la vez (en este caso `NULL`)
* `regular = TRUE`: regular time interval (para que detecte adecuadamente la periodicidad de la serie, en este caso `[1D]`)


```{r}
datos <-
  datos |>
  as_tsibble(index = Date, key = NULL, regular = TRUE)
datos
```


---

## Acciones de google

Ahora ya s√≠ podemos [**rellenar los huecos**]{.hl-yellow} 


```{r}
datos  <-
  datos |> 
  fill_gaps() |> 
  fill(Close, .direction = "down")

datos |> 
  count(year(Date)) |> 
  arrange(n)
```


---

## Acciones de google

2. [**Visualizamos**]{.hl-yellow} los datos. [**¬øTiene tendencia?**]{.hl-yellow}


```{r}
#| code-fold: true
ggplot(datos, aes(x = Date, y = Close)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```



Parece que s√≠, que [**existe una tendencia positiva**]{.hl-yellow}


---

## Acciones de google


```{r}
ajuste_lineal <- lm(data = datos, formula = Close ~ Date)
ajuste_lineal |> summary()
```


Existe una **muy fuerte tendencia positiva**. Si te fijas $R^2 = 0.9$ y los contrastes son todos apoyando rechazar la hip√≥tesis nula as√≠ parece que tiene sentido que la [**tendencia sea lineal (es decir, ADITIVA)**]{.hl-yellow}

---

## Acciones de google

Una forma de chequear si es aditiva o no es ver los [**errores tras ajustar tendencia**]{.hl-yellow}: no se observa ning√∫n patr√≥n evidente que falte por modelizar (si fuese multiplicativa se ver√≠a una tendencia todav√≠a)


```{r}
#| code-fold: true
ggplot(tibble("fecha" = datos$Date, "res" = ajuste_lineal$residuals),
       aes(x= fecha, y = res)) +
  geom_line() +
  theme_minimal()
```



---

## Acciones de google

3. Determinar [**homocedasticidad/heterocedasticidad**]{.hl-yellow} y c√≥mo evoluciona su varianza.  En caso necesario aplicar transformaci√≥n.

Una primera opci√≥n es [**visualizaci√≥n de la dispersi√≥n**]{.hl-yellow} por periodos (por ejemplo, por meses)


```{r}
#| code-fold: true
datos <-
  datos |> mutate("year_month" = yearmonth(Date))
resumen_var <-
  datos |>
  index_by(year_month) |> 
  summarise("dispersion" = sd(Close) / mean(Close))

ggplot(resumen_var, aes(x = year_month, y = dispersion)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```


. . .

Al margen de posibles outliers: [**no hay un patr√≥n ni tendencia en la varianza**]{.hl-yellow}

---

## Acciones de google

De hecho si realizamos el [**ajuste de un recta a la propia varianza agrupada**]{.hl-yellow} vemos que no es significativo: no hay evidencias de que exista una varianza que cambien con el tiempo.


```{r}
lm(data = resumen_var, formula = dispersion ~ year_month) |>
  summary()
```



---

## Acciones de google


4. [**Determinar periodicidad**]{.hl-yellow} $s$



```{r}
pacf(datos$Close)
```



No vemos a priori estacionalidad...porque recuerda que aqu√≠ tenemos datos diarios. ¬øY si aumentamos el n√∫mero de retardos?

---

## Acciones de google



```{r}
pacf(datos$Close, lag.max = 365*3)
```


[**No se aprecia patr√≥n estacional**]{.hl-yellow}

---

## Acciones de google


Esto tambi√©n lo podemos [**visualizar de manera conjunta**]{.hl-yellow} con `gg_tsdisplay()`


```{r}
datos |> 
  gg_tsdisplay(y = Close, lag = 365)
```


. . .

F√≠jate que adem√°s de no mostrarnos un **patr√≥n en las autocorrelaciones** al pedirle que la serie cada a√±o (cada 365 valores corta una curva) y vemos que [**efectivamente no comparten ning√∫n patr√≥n anual**]{.hl-yellow}


---



## Acciones de google


Otra forma de comprobar que efectivamente no es estacionales haciendo una [**diferenciaci√≥n estacional de la serie**]{.hl-yellow}: a cada valor le vamos a restar su valor $s$ periodos previos. 

&nbsp;

La funci√≥n que nos lo permite se llama `difference()` (del paquete `{tsibble}`)

---

## Acciones de google

* Si $s$ est√° [**mal elegido seguir√° presentando ese patr√≥n**]{.hl-red} (o uno a√∫n m√°s extra√±o)


```{r}
datos |>
  gg_tsdisplay(difference(Close, 30), lag_max = 365*3)
```


---

## Acciones de google

* Si $s$ est√° [**mal elegido seguir√° presentando ese patr√≥n**]{.hl-red} (o uno a√∫n m√°s extra√±o)


```{r}
datos |>
  gg_tsdisplay(difference(Close, 180), lag_max = 365*3)
```


---

## Acciones de google

* Si $s$ est√° [**mal elegido seguir√° presentando ese patr√≥n**]{.hl-red} (o uno a√∫n m√°s extra√±o)


```{r}
datos |>
  gg_tsdisplay(difference(Close, 365), lag_max = 365*3)
```


---

## Acciones de google


A partir de aqu√≠ todo tuyo. Piensa:

* Dividir train/test

* ¬øQu√© modelos podr√≠an tener sentido y por qu√©? ¬øAlg√∫n alisado? ¬øAlguno de los dem√°s vistos en `{fable}`?

* ¬øC√≥mo evaluar los modelos?


# Clases 16: [procesos estacionarios]{.flow} {#clase-16}

---

## Modelos ARIMA

En los a√±os 50 y 60 las matem√°ticas y la probabilidad entraron de lleno (a√∫n m√°s) en el campo de las series temporales, introduciendo el concepto de [**proceso estoc√°stico**]{.hl-yellow}.

. . .

La idea es que la serie temporal $X_t$ que observamos en un instante temporal $t$ no es m√°s que la [**realizaci√≥n de una variable aleatoria**]{.hl-yellow} definida en dicho instante: el valor que observamos es un **valor particular de los infinitos valores que podr√≠a haber tomado**.

. . .

Veamos un ejemplo simulado

---

## Proceso estoc√°stico

Vamos a simular la siguiente serie temporal (lo que se conoce como [**paseo aleatorio**]{.hl-yellow}: el valor anterior m√°s una perturbaci√≥n aleatoria)

$$X_t = X_{t-1} + \varepsilon_t, \quad X_1 = 0, \quad \varepsilon \sim \mathcal{N} \left(0, \sigma^2 \right)$$


```{r}
#| code-fold: true
n <- 300
sigma <- 3

serie <- tibble("t" = 1:n, "X_t" = rep(0, n))

for (i in 2:n) {
  serie$X_t[i] <- serie$X_t[i - 1] + rnorm(1, 0, sigma)
}
```



¬øQu√© **patr√≥n** tendr√≠a al visualizalro?

---

## Proceso estoc√°stico


```{r}
#| code-fold: true
ggplot(serie) +
  geom_line(aes(x = t, y = X_t)) +
  theme_minimal()
```


---

## Proceso estoc√°stico

Al ser [**aleatorio**]{.hl-yellow} si nosotros volvi√©semos a generar el proceso obtendr√≠amos otro paseo aleatorio distinto


```{r}
#| code-fold: true

serie <- tibble("t" = 1:n, "X_t" = rep(0, n))
for (i in 2:n) {
  serie$X_t[i] <- serie$X_t[i - 1] + rnorm(1, 0, sigma)
}
ggplot(serie) +
  geom_line(aes(x = t, y = X_t)) +
  theme_minimal()
```


---


## Proceso estoc√°stico

¬øQu√© pasar√≠a si yo genero **50 realizaciones de dicho paseo aleatorio**?

. . .


```{r}
#| code-fold: true
serie_total <- tibble()
for (j in 1:50) {
  serie <- tibble("id_serie" = as.character(j), "t" = 1:n, "X_t" = rep(0, n))
  for (i in 2:n) {
    serie$X_t[i] <- serie$X_t[i - 1] + rnorm(1, 0, sigma)
  }
  serie_total <- 
    serie_total |> 
    bind_rows(serie)
}
ggplot(serie_total) +
  geom_line(aes(x = t, y = X_t, color = id_serie),
            linewidth = 0.5) +
  guides(color = "none") +
  theme_minimal()
```


---

## Proceso estoc√°stico



```{r}
#| code-fold: true
serie_total <- tibble()
for (j in 1:50) {
  serie <- tibble("id_serie" = as.character(j), "t" = 1:n, "X_t" = rep(0, n))
  for (i in 2:n) {
    serie$X_t[i] <- serie$X_t[i - 1] + rnorm(1, 0, sigma)
  }
  serie_total <- 
    serie_total |> 
    bind_rows(serie)
}
gg1 <- 
  ggplot(serie_total) +
  geom_line(aes(x = t, y = X_t, color = id_serie),
            linewidth = 0.5) +
  geom_vline(xintercept = c(50, 100, 150, 200, 250),
             color = c("red", "orange", "darkgreen",
                       "purple", "blue")) +
  guides(color = "none") +
  theme_minimal()

gg2 <- 
  ggplot(serie_total |>
           select(t, X_t) |>
           filter(t %in% c(50, 100, 150, 200, 250)) |>
           mutate(t = factor(t))) +
  ggridges::geom_density_ridges(aes(y= t, x = X_t, fill = t),
                                alpha = 0.3) +
  scale_fill_manual(values = c("red", "orange", "darkgreen",
                       "purple", "blue")) +
  coord_flip() +
  theme_minimal() 

library(patchwork)
gg1 + gg2
```



La [**serie temporal que observamos no es m√°s que una de las infinitas realizaciones**]{.hl-yellow} que podr√≠a haber tomado


---

## Proceso estoc√°stico



```{r}
#| echo: false
serie_total <- tibble()
for (j in 1:50) {
  serie <- tibble("id_serie" = as.character(j), "t" = 1:n, "X_t" = rep(0, n))
  for (i in 2:n) {
    serie$X_t[i] <- serie$X_t[i - 1] + rnorm(1, 0, sigma)
  }
  serie_total <- 
    serie_total |> 
    bind_rows(serie)
}
gg1 <- 
  ggplot(serie_total) +
  geom_line(aes(x = t, y = X_t, color = id_serie),
            linewidth = 0.5) +
  geom_vline(xintercept = c(50, 100, 150, 200, 250),
             color = c("red", "orange", "darkgreen",
                       "purple", "blue")) +
  guides(color = "none") +
  theme_minimal()

gg2 <- 
  ggplot(serie_total |>
           select(t, X_t) |>
           filter(t %in% c(50, 100, 150, 200, 250)) |>
           mutate(t = factor(t))) +
  ggridges::geom_density_ridges(aes(y= t, x = X_t, fill = t),
                                alpha = 0.3) +
  scale_fill_manual(values = c("red", "orange", "darkgreen",
                       "purple", "blue")) +
  coord_flip() +
  theme_minimal() 

library(patchwork)
gg1 + gg2
```


* [**Proceso estoc√°stico**]{.hl-yellow}: un conjunto de variables aleatorias $\left\lbrace X_t \right\rbrace_{t \in T}$ definidas sobre el mismo espacio de probabilidades $\left(\Omega, \mathcal{A}, P \right)$. El conjunto de √≠ndices $T$ suele ser un **espacio temporal continuo**.

En el gr√°fico: [**todo en su conjunto**]{.hl-purple}


---

## Proceso estoc√°stico



```{r}
#| echo: false
gg1 + gg2
```


Dado que $\left\lbrace X_t \right\rbrace_{t \in T}$ es un conjunto de variables aleatorias, dependientes tambi√©n de un espacio temporal, tendremos que

$$\begin{eqnarray}X: ~T \times \Omega & \to & S \nonumber \\  (t, \omega) & \to & X(t, \omega)\nonumber\end{eqnarray}$$

donde $S$ es el espacio de estados (valores posibles que puede tomar). 

---

## Proceso estoc√°stico



```{r}
#| echo: false
gg1 + gg2
```



* [**Realizaci√≥n o trayectoria de un proceso estoc√°stico**]{.hl-yellow}: si fijamos $\omega \in \Omega$ (una aleatoriedad), tenemos

$$\begin{eqnarray}X\left(\cdot, \omega \right): ~T  & \to & S \nonumber \\  t & \to & X_t (\omega) ~ \text{ una realizaci√≥n}\nonumber\end{eqnarray}$$
En el gr√°fico: [**si interpretamos una sola curva y vemos su evoluci√≥n en el tiempo**]{.hl-purple}

---

## Proceso estoc√°stico



```{r}
#| echo: false
gg1 + gg2
```



* [**Distribucion de probabilidad a tiempo t**]{.hl-yellow}: si fijamos $t \in T$

$$\begin{eqnarray}X\left(t, \cdot \right): ~\Omega  & \to & S \nonumber \\  \omega & \to & X_t (\cdot) ~ \text{ una distribuci√≥n aleatoria}\nonumber\end{eqnarray}$$

En el gr√°fico: [**si interpretamos la distribuci√≥n de todas las posibles curvas en un instante dado**]{.hl-purple}

---

## Proceso estoc√°stico

$$\begin{eqnarray}X: ~T \times \Omega & \to & S \nonumber \\  (t, \omega) & \to & X(t, \omega)\nonumber\end{eqnarray}$$

Dado que para cada instante $t$ tenemos una distribuci√≥n de probabilidad, podremos [**caracterizar la serie con algunos de sus par√°metros poblacionales**]{.hl-yellow}:





:::: columns
::: {.column width="40%"}

* [**Media del proceso**]{.hl-yellow}: fijado un $t$ tenemos que $\mu_t = E \left[X_t \right]$ (si es constante en el tiempo --> sin tendencia)

:::

::: {.column width="60%"}


```{r}
#| code-fold: true
ggplot(serie_total) +
  geom_line(aes(x = t, y = X_t, color = id_serie),
            linewidth = 0.5) +
  geom_line(data = serie_total |> summarise("mu_t" = mean(X_t), .by = t),
            aes(x = t, y = mu_t), linewidth = 1.1) +
  guides(color = "none") +
  theme_minimal()
```


:::
::::

---

## Proceso estoc√°stico

$$\begin{eqnarray}X: ~T \times \Omega & \to & S \nonumber \\  (t, \omega) & \to & X(t, \omega)\nonumber\end{eqnarray}$$

Dado que para cada instante $t$ tenemos una distribuci√≥n de probabilidad, podremos [**caracterizar la serie con algunos de sus par√°metros poblacionales**]{.hl-yellow}:

:::: columns
::: {.column width="40%"}


* [**Varianza del proceso**]{.hl-yellow}: fijado un $t$ tenemos que $\sigma_{t}^{2} = Var \left[X_t \right]$  (si es constante en el tiempo --> homoced√°stica)

:::

::: {.column width="60%"}



```{r}
#| code-fold: true
ggplot(serie_total) +
  geom_line(aes(x = t, y = X_t, color = id_serie),
            linewidth = 0.5) +
  geom_line(data = serie_total |> summarise("var_t" = sd(X_t), .by = t),
            aes(x = t, y = var_t), linewidth = 1.1) +
  guides(color = "none") +
  theme_minimal()
```


:::
::::

---


## Proceso estoc√°stico

Mientras que en los [**modelos de descomposici√≥n y en los alisados exponencial simplemente realizamos un ajuste de una curva observada**]{.hl-red} (sin tener en cuenta su distribuci√≥n estoc√°stica subyacente), en el momento en el que consideramos [**modelos ARIMA**]{.hl-yellow} (la serie como una realizaci√≥n de un conjunto de variables aleatorias correladas en el tiempo) ser√° importante su [**estructura de autocorrelaci√≥n**]{.hl-yellow}:  las [**diferentes distribuciones del proceso estoc√°stico dependen entre s√≠**]{.hl-yellow} para distintos $t$.

. . .

Dados $t_1$ y $t_2$, llamaremos [**funci√≥n de autocovarianzas**]{.hl-yellow} a

$$\gamma_{t_1, t_2} = Cov \left( X_{t_1}, X_{t_2} \right) = E \left[\left(X_{t_1} - \mu_{t_1} \right) \left(X_{t_2} - \mu_{t_2} \right) \right]$$

Dados $t_1$ y $t_2$, llamaremos [**funci√≥n de autocorrelaci√≥n**]{.hl-yellow} a

$$\rho_{t_1, t_2} = Cor \left( X_{t_1}, X_{t_2} \right) = \frac{\gamma_{t_1, t_2}}{\sqrt{\sigma_{t_{1}}^{2}\sigma_{t_{1}}^{2}
}}$$


---

## Proceso estacionario


```{r}
#| echo: false
gg1 + gg2
```



Todo lo anterior es sobre la base de que yo [**pudiese generar una colecci√≥n de curvas**]{.hl-yellow} de manera que puedo observarlas de manera longitudinal (tiempo) o transversal (distribuci√≥n probabil√≠stica a un tiempo dado). 

---

## Proceso estacionario


```{r}
#| echo: false
gg1 + gg2
```


El [**problema**]{.hl-red} es que normalmente es [**inviable obtener varias realizaciones**]{.hl-red}: si tengo una serie temporal de las temperaturas a lo largo del a√±o, esas temperaturas podr√≠an haber sido otras pero no puedo volver al pasado y generar otras nuevas.

. . .

As√≠ la √∫nica forma de [**estimar las caracter√≠sticas transversales (media, varianza, etc)**]{.hl-yellow} de una serie haciendo uso de su evoluci√≥n longitudinal es [**suponer que esas propiedades transversales (distribuci√≥n) en cada instante temporal son estables**]{.hl-gren} (no var√≠an)


---

## Proceso estacionario


Por lo tanto el primer [**requisito**]{.hl-yellow} que necesitaremos para la introducci√≥n de modelos ARIMA o modelos probabl√≠sticos ser√° el [**concepto de estacionariedad**]{.hl-yellow}: algo de la serie que **no var√≠e con el tiempo**
 
 
. . .

Diremos que $\left\lbrace X_{t} \right\rbrace$ es un [**proceso estacionario (en el tiempo)**]{.hl-yellow} cuando sus [**propiedades no dependen del instante $t$ en el que las medimos**]{.hl-yellow}. Dicho de manera informal, una serie ser√° estacionaria cuando mire donde mire veo *lo mismo*, cuyo [**patr√≥n no puede ser predicho**]{.hl-yellow}

. . .

Desde un punto de vista matem√°tico es [**estrictamente estacionario**]{.hl-yellow} si para cualquier conjunto de instantes temporales y retardo $h > 0$, tenemos que $\left(X_{t_{1}}, \ldots, X_{t_{n}} \right)$ y $\left(X_{t_{1}+h}, \ldots, X_{t_{n}+h} \right)$ tienen la **misma distribuci√≥n conjunta**: son indistinguibles.

---

## Proceso estacionario


Dado que est√° condici√≥n es **demasiado estricta** (nos obligar√≠a a saber su distribuci√≥n, si es una normal, una chi-cuadrado, etc), trabajaremos con [**procesos d√©bilmente estacionarios**]{.hl-yellow}

1. [**Media constante**]{.hl-yellow}: $\mu_{t} = \mu_{t+h} = cte$ para cualquier instante $t$ y retardo $h$.

2. [**Varianza constante**]{.hl-yellow}: $\sigma_{t}^{2} = \sigma_{t+h}^{2}$ para cualquier instante $t$ y retardo $h$.

3. [**Autocovarianzas independiente de t**]{.hl-yellow}: $\gamma_{s, t} = \gamma_{s+h, t+h} = \gamma_h$  la [**dependencia entre dos instantes solo depende de su distancia**]{.hl-yellow}, al margen de en qu√© punto lo medimos.


---

## Proceso estacionario


De lo anterior se derivan algunas [**propiedades sobre las autocorrelaciones**]{.hl-yellow} (las barras que nos dibuja el acf)


* Solo nos importan los $\gamma_h = \gamma_{t, t+h}$ para cada $h=0, 1, 2, ...$, tal que $\gamma_h = \gamma_{-h}$

. . .

* De la misma forma $\rho_h = \rho_{t, t+h} = \frac{\gamma_{t, t+h}}{\sigma_{t} \sigma_{t+h}} = \frac{\gamma_{t, t+h}}{\sigma^2} = \frac{\gamma_{h}}{\gamma_0}$ tal que $\rho_{h} = \rho_{-h}$ y $\rho_0 = 1$.


. . .


* Dado que $\left| \rho_{h} \right| \leq 1$ entonces $\left|\gamma_h \right| = \left| \rho_h \right| * \gamma_0 \leq \gamma_0$ para todo $h$ (por eso decrecen las barras del acf)


. . .

* Si $\left\lbrace X_t \right\rbrace$ es un proceso estacionario entonces su diferencia $X_{t} - X_{t-1}$ tambi√©n lo es.

---

## Ruido blanco

Uno de los [**procesos estoc√°sticos m√°s importantes**]{.hl-yellow} es el conocido como [**ruido blanco**]{.hl-yellow}


1. [**Media 0**]{.hl-yellow}: $\mu_{t} = \mu_{t+h} = 0$ para cualquier instante $t$ y retardo $h$.

2. [**Varianza constante**]{.hl-yellow}: $\sigma_{t}^{2} = \sigma_{t+h}^{2} = \sigma \neq 0$ para cualquier instante $t$ y retardo $h$.

3. [**Incorrelados temporalmente**]{.hl-yellow}: $\gamma_{s, t} = \gamma_{s+h, t+h} = \gamma_h = 0$ si $h\neq 0$ (si $h=0$, $\gamma_0 = \sigma_t \neq 0$)

. . .

Este proceso estacionario ser√° importante ya que, [**si he modelizado bien mi serie**]{.hl-green}, el [**residuo que queda sin explicar deber√≠a ser ruido blanco**]{.hl-yellow}

---

## Diagn√≥stico de residuos

Veamos un ejemplo con la serie de Google


```{r}
library(tsibble)
library(tsibbledata)
library(tidyverse)
library(feasts)
google <-
  gafa_stock |>
  filter(Symbol == "GOOG") |> 
  as_tsibble(regular = TRUE) |> 
  fill_gaps() |> 
  fill(Close)
google
```


---

## Diagn√≥stico de residuos


Ya vimos en su momento que tiene tendencia, por lo que [**no es un proceso estacionario**]{.hl-yellow} ya que su distribuci√≥n es distinta en distintos instantes temporales


```{r}
ggplot(google) +
  geom_line(aes(x = Date, y = Close)) +
  theme_minimal()
```


---

## Diagn√≥stico de residuos

Si pintamos las [**estimaciones muestrales (insesgadsa) de las autocorrelaciones**]{.hl-yellow} tenemos que [**no decrecen**]{.hl-red}, es decir, no es ruido blanco (queda algo por modelizar)


```{r}
# alternativa a acf(google$Close) en forma gpglot
google |>
  ACF(Close, lag_max = 100) |>
  autoplot() +
  labs(title = "Google closing stock price",
       y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```



---

## Diagn√≥stico de residuos

Para tener una cuantificaci√≥n inferencial m√°s rigurosa sobre si es o no ruido blanco podemos realizar el [**test de Ljung-Box que nos contrasta si un grupo cualquiera de autocorrelaciones son diferentes de cero**]{.hl-yellow} (se conoce como contrastes Portmanteau a aquellos que en lugar de probar la aleatoriedad en cada retardo distinto, prueba la aleatoriedad "en general")

Para ello usaremos  `features()` con la opci√≥n `ljung_box` (ya la usamos para las transformaciones Box-Cox)


```{r}
google |>
  features(Close, ljung_box, lag = 10)
```


Seg√∫n el contraste hay [**evidencia suficientes para rechazar que sean cero**]{.hl-red} -> no es ruido blanco

---

## Diagn√≥stico de residuos

¬øPero qu√© pasa si yo [**diferencio la serie**]{.hl-yellow} (resto a cada valor su instante anterior)? Si ahora te diese dos fotos de la serie, ¬øsabr√≠as ahora [**distinguir su instante temporal**]{.hl-yellow}?


```{r}
ggplot(google) +
  geom_line(aes(x = Date, y = difference(Close))) +
  theme_minimal()
```


---

## Diagn√≥stico de residuos

¬°Todas se desploman!


```{r}
# alternativa a acf(google$Close) en forma gpglot
google |>
  ACF(difference(Close), lag_max = 100) |>
  autoplot() +
  labs(title = "Google closing stock price",
       subtitle = "Serie diferenciada",
       y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```



---

## Diagn√≥stico de residuos


```{r}
google |>
  mutate(diff_close = difference(Close)) |>
  features(diff_close, ljung_box, lag = 10)
```


Simplemente aplicando una diferenciaci√≥n nuestro proceso se ha [**convertido en ruido blanco**]{.hl-yellow}

# Clases 17: [procesos MA]{.flow} {#clase-17}

---

## Correlogramas

Como hemos comentado los [**correlogramas**]{.hl-yellow} van a ser ahora fundamentales ya que os da informaci√≥n de la estructura del proceso estoc√°stico subyacente.

:::: columns
::: {.column width="50%"}


```{r}
google |>
  ACF(Close, lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
google |>
  ACF(difference(Close), lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::
::::

---


## Correlogramas


:::: columns
::: {.column width="50%"}


```{r}
#| echo: false
google |>
  ACF(Close, lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| echo: false
google |>
  ACF(difference(Close), lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::
::::


En lineas generales diremos que

* [**Series con tendencia**]{.hl-yellow} presentar√°n una ca√≠da muy lenta de las autocorrelaciones (la tendencia implica que el valor $t+1$ depende del $t, t-1, t-2, ...$)

* [**Series con estacionalidad**]{.hl-yellow} presentar√°n un patr√≥n c√≠clico de ca√≠da a lo largo del tiempo.

---

## Correlogramas

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
datos <- timeSeriesDataSets::a10_ts |> as_tsibble(regular = TRUE)
ggplot(datos) +
  geom_line(aes(x = index, y = value)) +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
datos |>
  ACF(value, lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::
::::

---


## Correlogramas

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
datos <- timeSeriesDataSets::beer_ts |> as_tsibble(regular = TRUE)
ggplot(datos) +
  geom_line(aes(x = index, y = value)) +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
datos |>
  ACF(value, lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::
::::

---

## Correlogramas

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
datos <- timeSeriesDataSets::elec_ts |> as_tsibble(regular = TRUE)
ggplot(datos) +
  geom_line(aes(x = index, y = value)) +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
datos |>
  ACF(value, lag_max = 100) |>
  autoplot() +
  labs(y = "Estimaci√≥n muestral de autocorrelaciones",
       x = "Retardos") +
  theme_minimal()
```


:::
::::

---

## Procesos lineales

En las pr√≥ximas clases trabajaremos siempre suponiendo que nuestros procesos ya son [**estacionarios**]{.hl-yellow}. Veremos m√°s adelante qu√© hacer cuando [**no son estacionarios en media**]{.hl-yellow} o [**en varianza**]{.hl-yellow}


. . .

Un resultado importante (se conoce como Teorema de descomposici√≥n de Wald) es que [**cualquier proceso estacionario puede transformado en un proceso lineal**]{.hl-yellow} definido como

$$X_t = \mu  + \sum_{j = -\infty}^{j=\infty} \Psi_j \varepsilon_{t-j} \quad \sum_{j = -\infty}^{j=\infty} \left| \Psi_j \right| < \infty, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$
Un proceso lineal es por tanto la [**combinaci√≥n lineal de instantes de ruido blanco**]{.hl-yellow} tal que los coeficientes sean absolutamente sumables.

---

## Procesos lineales


$$X_t = \mu + \sum_{j = -\infty}^{j=\infty} \Psi_j \varepsilon_{t-j}, \quad \sum_{j = -\infty}^{j=\infty} \left| \Psi_j \right| < \infty, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$
Si te fijas hay una [**constante inicial $\mu$ que representa la media (a largo plazo)**]{.hl-yellow} del proceso ya que, dado que la esperanza es lineal y $\left\lbrace \varepsilon_{t} \right\rbrace$ es ruido blanco, entonces

$$E \left[ X_t \right] = \mu + \sum_{j = -\infty}^{j=\infty} \Psi_j E \left[ \varepsilon_{t-j} \right] = \mu$$

. . .

De ahora en adelante [**asumiremos que $\mu = 0$ (proceso centrado)**]{.hl-yellow} ya que sino lo fuese basta con usar $X_t = X_t - \mu$

---

## Procesos MA

$$X_t = \sum_{j = -\infty}^{j=\infty} \Psi_j \varepsilon_{t-j}, \quad \sum_{j = -\infty}^{j=\infty} \left| \Psi_j \right| < \infty, \quad  \text{proceso lineal}$$


Uno de los tipos de procesos lineales m√°s importantes son los conocidos como [**procesos de medias moviles (MA)**]{.hl-yellow} definidos como

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} \quad q \geq 1, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

donde $q$ ser√° el **orden** (lo llamaremos [**procesos de medias moviles de orden q o MA(q)**]{.hl-yellow}). Si te fijas es un **caso particular de proceso lineal** donde todos los coeficientes son 0 salvo [**$\left(\Psi_0 = 1, \Psi_1 = -\theta_1,\ldots, \Psi_q = -\theta_q \right)$**]{.hl-green}.

---

## Procesos MA


$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} , \quad q \geq 1, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

Los procesos $MA(q)$ son procesos cuyo [**futuro se describe promediando los errores del pasado**]{.hl-yellow} y son tremendamente √∫tiles para modelizar fen√≥menos influenciados por sucesos que producen un efecto inmediato de corta duracci√≥n (por ejemplo, variables econ√≥micas).

---

## Operador de retardos

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} , \quad q \geq 1, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

Para simplificar notaci√≥n llamaremos [**operador de retardos**]{.hl-yellow} al operador


$$B \left( X_t \right) := B  X_t = X_{t-1}$$

. . .

De esta forma, si queremos el **retardo s** de la serie basta con hacer

$$B^s(X_t) = B^{s-1} (X_{t-1})  = B^{s-2} \left(  X_{t-2} \right) = B^{s-3} \left(  X_{t-3} \right)  = ... = X_{t-s}$$

---

## Operador de retardos

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} , \quad q \geq 1, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$


El proceso anteriormente definido como $MA(q)$ se puede por tanto [**expresar en funci√≥n del operador de retardos**]{.hl-yellow} como

$$\begin{eqnarray}X_t &=& \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}  = \varepsilon_{t} - \theta_1 B\varepsilon_t - \ldots -  \theta_q B^{q}\varepsilon_t \nonumber \\ &=& \left( 1 -\theta_1 B - \ldots - \theta_q B^{q} \right)\varepsilon_t = \Theta_{q} \left(B \right)\varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco} \nonumber \end{eqnarray}$$

. . .

El [**polinomio de retardos o polinomio de media m√≥vil de orden $q$**]{.hl-yellow} $\Theta_{q} \left(B \right)$ ser√° el que defina nuestro proceso


$$\Theta_{q} \left(B \right) = \displaystyle \sum_{j=0}^{\infty} \Psi_j B^{j}, \quad \Psi_0 = 1,~\Psi_j = -\theta_j ~(j =1, \ldots,q),\quad \Psi_j = 0 ~(j > q)$$


---

## Proceso MA

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}  = \left( 1 - \theta_1 B - \ldots - \theta_q B^{q} \right)\varepsilon_t = \Theta_q \left(B \right)\varepsilon_t$$

Dale valores $q=1, 2, 3, 4$ y escribe c√≥mo quedar√≠a el proceso

. . .

* **MA(1)**: $X_t  = \Theta_1 \left(B \right)\varepsilon_t = \left(1 - \theta_1 B \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1}$

* **MA(2)**: $X_t  = \Theta_2 \left(B \right)\varepsilon_t = \left(1 - \theta_1 B - \theta_2 B^2 \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2}$

* **MA(3)**: $X_t  = \Theta_3 \left(B \right)\varepsilon_t = \left(1 - \theta_1 B - \theta_2 B^2 - \theta_3 B^3 \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \theta_3 \varepsilon_{t-3}$

* **MA(4)**: $X_t  = \Theta_4 \left(B \right)\varepsilon_t = \left(1 - \theta_1 B - \theta_2 B^2 - \theta_3 B^3 - \theta_4 B^4 \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \theta_3 \varepsilon_{t-3} - \theta_4 \varepsilon_{t-4}$


---

## Proceso MA

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}   = \Theta_q \left(B \right)\varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

[**¬øEs un proceso estacionario?**]{.hl-yellow}

---


## Proceso MA

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}   = \Theta_q \left(B \right)\varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

1. [**Media**]{.hl-yellow}: dado que $\left\lbrace \varepsilon_{t} \right\rbrace$ es ruido blanco

$$\begin{eqnarray}E\left[X_t \right] &=& \left(E\left[ \varepsilon_{t} \right] - \theta_1 E\left[ \varepsilon_{t-1} \right]  - \ldots - \theta_q E\left[ \varepsilon_{t-q} \right] \right) \nonumber \\ &=& \left( 1 - \theta_1  - \ldots - \theta_q \right)E\left[ \varepsilon_{t} \right] = 0  \quad \text{(o bien } \mu \text{ si no est√° centrado)}\end{eqnarray}$$

. . .

[**‚úÖ Media constante en el tiempo**]{.hl-green}

---

## Proceso MA

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}   = \Theta_q \left(B \right)\varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

2. [**Varianza**]{.hl-yellow}: dado que $\left\lbrace \varepsilon_{t} \right\rbrace$ es ruido blanco

$$\begin{eqnarray}Var\left[X_t \right] &=& \left(Var\left[\varepsilon_{t}\right] + \theta_{1}^{2} Var\left[\varepsilon_{t-1}\right] + \ldots + \theta_{q}^2  Var\left[\varepsilon_{t-1}\right] \right)  \nonumber \\ &=& \left(1 + \theta_{1}^{2} + \ldots + \theta_{q}^2  \right) \sigma_{\varepsilon}^2 = cte \quad \text{¬øpero finita?}\end{eqnarray}$$

. . .

$$\left(1 + \theta_{1}^{2} + \ldots + \theta_{q}^2  \right)  < \infty \iff q < \infty \quad \text{y adem√°s} \quad \left| \theta_j \right| < \infty$$

[**‚úÖ Varianza constante (y finita) en el tiempo**]{.hl-green}

---


## Proceso MA

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}   = \Theta_q \left(B \right)\varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

3. [**Autocovarianzas**]{.hl-yellow}:

$$\begin{eqnarray}\gamma_h &=& Cov \left( X_t, X_{t+h} \right) = E \left[ \left( \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}\right) \left( \varepsilon_{t+h} - \theta_1 \varepsilon_{t+h-1} - \ldots - \theta_q \varepsilon_{t+h-q}\right)  \right] \nonumber \\ &=& E \left[ \varepsilon_t \varepsilon_{t+h} \right]  - \theta_1 E \left[ \varepsilon_{t}  \varepsilon_{t+h-1} \right] - \ldots  - \theta_q E \left[ \varepsilon_{t}  \varepsilon_{t+h-q} \right] \nonumber \\ &-& \theta_1 E \left[ \varepsilon_{t-1} \varepsilon_{t+h} \right]  + \theta_{1}^2 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-1} \right] + \ldots  + \theta_1\theta_q E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-q} \right] \nonumber \\ & & ... \nonumber \\ &-& \theta_q E \left[ \varepsilon_{t-q} \varepsilon_{t+h} \right]  + \theta_{1}\theta_{q} E \left[ \varepsilon_{t-q}  \varepsilon_{t+h-1} \right] + \ldots  + \theta_{q}^{2} E \left[ \varepsilon_{t-q}  \varepsilon_{t+h-q} \right]\end{eqnarray}$$

. . .

Para verlo m√°s sencillo analicemos primero [**los casos particulares $q=1$ y $q=2$**]{.hl-yellow}

---

## MA(1)

$$X_t =  \Theta_1 \left(B \right)\varepsilon_t = \varepsilon_t -\theta_1 \varepsilon_{t-1}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco} \nonumber$$

3. [**Autocovarianzas**]{.hl-yellow}:

$$\begin{eqnarray}\gamma_h &=& Cov \left( X_t, X_{t+h} \right) = E \left[ \left( \varepsilon_t - \theta_1 \varepsilon_{t-1}\right) \left( \varepsilon_{t+h} - \theta_1 \varepsilon_{t+h-1}\right)\right] \nonumber \\ &=& E \left[ \varepsilon_t \varepsilon_{t+h} \right]  - \theta_1 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h} \right] - \theta_1 E \left[ \varepsilon_{t}  \varepsilon_{t+h-1} \right] + \theta_{1}^2 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-1} \right]\end{eqnarray}$$

* [**$h = 0$**]{.hl-purple} --> $\gamma_0 = E \left[ \varepsilon_t \varepsilon_t  \right] + \theta_{1}^ 2E \left[ \varepsilon_{t-1} \varepsilon_{t-1}  \right] = \sigma^2 + \theta_{1}^2 \sigma^2 = \left(1 + \theta_{1}^2 \right) \sigma^2$

* [**$h = 1$**]{.hl-purple} --> $\gamma_1 = -\theta_1 \sigma^2$

* [**$h > 1$**]{.hl-purple} --> $\gamma_h = 0$

---

## MA(1)

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco con var } \sigma^2 \nonumber$$

3. [**Autocorrelaciones**]{.hl-yellow}:

$$\begin{eqnarray}\rho_h = \frac{\gamma_h}{\gamma_0} = \begin{cases} \frac{\gamma_0}{\gamma_0} = 1 \quad & &h = 0, \nonumber \\ \frac{\gamma_1}{\gamma_0} = \frac{-\theta_1 \sigma^2}{\left(1 + \theta_{1}^2 \right) \sigma^2} = \frac{-\theta_1}{1 + \theta_{1}^2}\quad & &h = 1, \nonumber \\ 0  \quad & &h > 1\end{cases}\end{eqnarray}$$

F√≠jate que el [**signo de $\rho_h$ ser√° el contrario al signo de $\theta_1$**]{.hl-yellow} y que, a partir de $q$, las autocorrelaciones son 0 (se dice que es [**q-correlacionado**]{.hl-yellow})

---


## MA(1)

[**üíª Dise√±a una funci√≥n**]{.hl-yellow} para simular $n$ trayectorias de un $MA(1)$ en funci√≥n de $\left(n,~\sigma,~\theta_1, \varepsilon_1\right)$ 

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco con varianza } \sigma^2 \nonumber$$


```{r}
#| code-fold: true
MA_1_simul <- function(n, sigma, theta_1, eps_1) {
  
  
  eps <- c(eps_1, rnorm(n = n - 1, mean = 0, sd = sigma))
  X_t <- eps_1
  for (i in 2:n) {
    X_t[i] <- eps[i] - theta_1*eps[i - 1]
  }
  ts <- tibble("t" = 1:n, "X_t" = X_t) |> as_tsibble(index = t)
  return(ts)
}
```


---

## MA(1)

üíª [**Aplica dicha funci√≥n**]{.hl-yellow} para $n = 300$, $\sigma = 3$, $\varepsilon_1 = 0$ y $\theta_1 = (-0.3, -1.5)$

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
ggplot(MA_1_simul(300, 3, -0.3, 0)) +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "MA(1) con theta_1 = -0.3 y sigma = 3") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
ggplot(MA_1_simul(300, 3, -1.5, 0)) +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "MA(1) con theta_1 = -1.5 y sigma = 3") +
  theme_minimal()
```


:::
::::



---

## MA(1)

üíª [**Calcula las ACF de ambas series**]{.hl-yellow}

. . .

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
MA_1_simul(300, 3, -0.3, 0) |> 
  ACF(X_t) |> 
  autoplot() +
  labs(title = "MA(1) con theta_1 = -0.3 y sigma = 3") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
MA_1_simul(300, 3, -1.5, 0) |> 
  ACF(X_t) |> 
  autoplot() +
  labs(title = "MA(1) con theta_1 = -1.5 y sigma = 3") +
  theme_minimal()
```


:::
::::

¬øTe has fijado en que [**las correlaciones caen a partir de $q$**]{.hl-yellow} (q-correlacionado poblacionalmente aunque a nivel muestral puedan caer algunas todav√≠a fuera de la banda de significaci√≥n)?

---

## MA(1)


Si te fijas, dado que $X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1}$, tenemos entonces que $\varepsilon_t = X_t + \theta_1 \varepsilon_{t-1}$ por lo que sustituyendo tenemos que

$$\begin{eqnarray}X_t &=& \varepsilon_t + \theta_1  \varepsilon_{t-1} = \varepsilon_t + \theta_1 \left( X_{t-1} + \theta_1 \varepsilon_{t-2} \right) = \varepsilon_t + \theta_1  X_{t-1} + \theta_{1}^2 \varepsilon_{t-2} \nonumber \\ &=& \varepsilon_t + \theta_1  X_{t-1} + \theta_{1}^2 X_{t-2} + \theta_{1}^{3} \varepsilon_{t-3} = ... \nonumber \\ &=& \varepsilon_{t} + \sum_{j=0}^{\infty} \theta_{1}^{j} X_{t-j}\end{eqnarray}$$

. . .

El **futuro** de la serie acaba siendo modelizado con el √∫ltimo error y una ponderaci√≥n del pasado, y dado que parece razonable [**asumir que los valores m√°s cercanos influyan m√°s que los valores m√°s lejanos**]{.hl-yellow} los pesos $\left| \theta_{1}\right|^{j}$ deben disminuir con el retardo: [**necesitamos que $\left| \theta_{1}\right| < 1$**]{.hl-yellow}

---

## Condici√≥n de invertibilidad

Esta [**condici√≥n (veremos porque se llama de invertibilidad)**]{.hl-yellow} ser√° fundamental en los procesos MA ya que, aunque la funci√≥n de autocorrelaci√≥n ser√° importante, tenemos un [**problema de identificaci√≥n**]{.hl-red}: dada una funci√≥n de autocorrelaci√≥n [**no existe un √∫nico proceso asociado**]{.hl-red}

. . .

Hagamos un ejercicio r√°pido con dos $MA(1)$ distintos

$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1}, \quad X_t = \varepsilon_t - \frac{1}{\theta_1} \varepsilon_{t-1}$$

¬øCu√°les son sus autocorrelaciones? 

---

## Condici√≥n de invertibilidad


$$X_t = \varepsilon_t - \theta_1 \varepsilon_{t-1}, \quad X_t = \varepsilon_t - \frac{1}{\theta_1} \varepsilon_{t-1}$$

En el primer caso

$$\begin{eqnarray}\rho_h = \frac{\gamma_h}{\gamma_0} = \begin{cases} 1 \quad & &h = 0, \nonumber \\  \frac{-\theta_1}{1 + \theta_{1}^2}\quad & &h = 1, \nonumber\end{cases}\end{eqnarray}$$

. . .

En el segundo caso

$$\begin{eqnarray}\rho_h = \frac{\gamma_h}{\gamma_0} = \begin{cases} 1 \quad & &h = 0, \nonumber \\  \frac{-\frac{1}{\theta_1}}{1 + \frac{1}{\theta_1^2}} = \frac{-\frac{\theta_{1}^{2}}{\theta_1}}{\theta_{1}^{2} + \frac{\theta_{1}^{2}}{\theta_1^2}} = \frac{-\theta_1}{1 + \theta_{1}^{2}}\quad & &h = 1 \nonumber\end{cases}\end{eqnarray}$$

**¬°Es la misma!**

---

## Condici√≥n de invertibilidad

Dicha condici√≥n $\left| \theta_{1}\right| < 1$ adem√°s nos permitir√° [**expresar los errores en funci√≥n de $X_t$**]{.hl-yellow} tal que

$$ \varepsilon_{t} = X_t + \theta_1 \varepsilon_{t-1} = X_t + \theta_1 \left( X_{t-1} + \theta_1 \varepsilon_{t-2} \right) = \ldots = \sum_{j=0}^{\infty} \theta_{1}^{j}X_{t-j} $$

Dicha serie solo converger√° si $\sum_{j=0}^{\infty}  \theta_{1}^{j} < \infty$, es decir, si y solo s√≠ [**$\left| \theta_1 \right| < 1$**]{.hl-yellow}

&nbsp;

En el primero ejemplo simulado se cumpl√≠an las condiciones de invertibilidad, en la segunda no.

---

## MA(1)

Volvamos a ver [**qu√© forma tiene la funci√≥n de autocorrelaciones**]{.hl-yellow} del primer ejemplo simulado donde se cumpl√≠an las **condiciones de intertibilidad**



```{r}
#| code-fold: true
set.seed(1234567)
datos <- MA_1_simul(300, 3, -0.3, 0)
library(feasts)
datos |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot() +
  scale_y_continuous(limits = c(-1, 1)) +
  scale_x_continuous(breaks = seq(0, 20, by = 1)) +
  theme_minimal()
```


Usaremos la [**funci√≥n de autocorrelaciones (ACF)**]{.hl-yellow} para determinar si se trata de un [**proceso MA(1): si a partir de la primera las correlaciones caen dr√°sticamente**]{.hl-yellow} (q-correlacionado), es probable que el proceso sea un $MA(1)$


---

## MA(1)

Recuerda que estamos visualizando las [**estimaciones muestrales**]{.hl-yellow} por lo que cuanto m√°s aumente $\theta_1$ m√°s lenta ser√° la ca√≠da (si $\theta_1 > 0$ las autocorrelaciones ir√°n cambiando de signo).


:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
datos <- MA_1_simul(300, 3, -0.9, 0)
library(feasts)
datos |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot() +
  scale_y_continuous(limits = c(-1, 1)) +
  scale_x_continuous(breaks = seq(0, 30, by = 1)) +
  theme_minimal()
```


:::


::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
datos <- MA_1_simul(300, 3, 0.9, 0)
library(feasts)
datos |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot() +
  scale_y_continuous(limits = c(-1, 1)) +
  scale_x_continuous(breaks = seq(0, 30, by = 1)) +
  theme_minimal()
```


:::
::::

---

## MA(2)


$$X_t =  \Theta_2 \left(B \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco} \nonumber$$

[**Deberes**]:


* Demuestra que sus **autocorrelaciones** son

$$\begin{eqnarray}\rho_h = \frac{\gamma_h}{\gamma_0} = \begin{cases} 1 \quad & &h = 0, \nonumber \\ \frac{-\theta_1  + \theta_1 \theta_2 }{1 + \theta_{1}^2  + \theta_{2}^2}\quad & &h = 1, \nonumber \\ \frac{-\theta_2}{1 + \theta_{1}^2  + \theta_{2}^2}\quad & &h = 2 \nonumber \\ 0  \quad & &h > 2\end{cases}\end{eqnarray}$$

* dise√±a una funci√≥n para simular un MA(2)

* simula varios y observa sus ACF

# Clases 18: [procesos MA]{.flow} {#clase-18}

---

## MA(2)


$$X_t =  \Theta_2 \left(B \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco} \nonumber$$

Hagamos de nuevo el ejercicio de calcular las [**autocovarianzas**]{.hl-yellow}:

$$\begin{eqnarray}\gamma_h &=& Cov \left( X_t, X_{t+h} \right) = E \left[ \left( \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2}\right) \left( \varepsilon_{t+h} - \theta_1 \varepsilon_{t+h-1} - \theta_2 \varepsilon_{t+h-2}\right)\right] \nonumber \\ &=& E \left[ \varepsilon_t \varepsilon_{t+h} \right]  - \theta_1 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h} \right] - \theta_1 E \left[ \varepsilon_{t}  \varepsilon_{t+h-1} \right] + \theta_{1}^2 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-1} \right] \nonumber \\ &-&  \theta_2 E \left[ \varepsilon_{t-2}  \varepsilon_{t+h} \right] + \theta_1 \theta_2 E \left[ \varepsilon_{t-2}  \varepsilon_{t+h-1} \right] + \theta_{2}^2 E \left[ \varepsilon_{t-2}  \varepsilon_{t-h-2} \right]  \nonumber \\ &-& \theta_2 E \left[ \varepsilon_{t}  \varepsilon_{t+h-2} \right] + \theta_1 \theta_2 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-2} \right]  \end{eqnarray}$$

* $h = 0$ -> $\gamma_0 = \sigma^2 + \theta_{1}^2 \sigma^2  + \theta_{2}^2 \sigma^2 = \left(1 + \theta_{1}^2  + \theta_{2}^2 \right) \sigma^2$

* $h = 1$ -> $\gamma_1 = -\theta_1 \sigma^2 + \theta_1 \theta_2 \sigma^2$

* $h = 2$ -> $\gamma_2 = -\theta_2 \sigma^2$

* $h > 2$ -> $\gamma_h = 0$

---

## MA(2)


$$X_t =  \Theta_2 \left(B \right)\varepsilon_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco} \nonumber$$

Por tanto las [**autocorrelaciones**]{.hl-yellow} ser√°n:

$$\begin{eqnarray}\rho_h = \frac{\gamma_h}{\gamma_0} = \begin{cases} \frac{\gamma_h}{\gamma_0} = 1 \quad & &h = 0, \nonumber \\ \frac{\gamma_1}{\gamma_0} = \frac{-\theta_1 \sigma^2 + \theta_1 \theta_2 \sigma^2}{ \left(1 + \theta_{1}^2  + \theta_{2}^2 \right) \sigma^2} = \frac{-\theta_1  + \theta_1 \theta_2 }{1 + \theta_{1}^2  + \theta_{2}^2}\quad & &h = 1, \nonumber \\ \frac{\gamma_2}{\gamma_0} = \frac{-\theta_2 \sigma^2}{ \left(1 + \theta_{1}^2  + \theta_{2}^2 \right) \sigma^2} = \frac{-\theta_2}{1 + \theta_{1}^2  + \theta_{2}^2}\quad & &h = 2 \nonumber \\ 0  \quad & &h > 2\end{cases}\end{eqnarray}$$

. . .

[**¬øC√≥mo generalizarlo a MA(q)?**]{.hl-yellow}


---

## MA(q)


$$\begin{eqnarray}\gamma_h &=& Cov \left( X_t, X_{t+h} \right) = E \left[ \left( \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}\right) \left( \varepsilon_{t+h} - \theta_1 \varepsilon_{t+h-1} - \ldots - \theta_q \varepsilon_{t+h-q}\right)  \right] \nonumber \\ &=& E \left[ \varepsilon_t \varepsilon_{t+h} \right]  - \theta_1 E \left[ \varepsilon_{t}  \varepsilon_{t+h-1} \right] - \ldots  - \theta_q E \left[ \varepsilon_{t}  \varepsilon_{t+h-q} \right] \nonumber \\ &-& \theta_1 E \left[ \varepsilon_{t-1} \varepsilon_{t+h} \right]  + \theta_{1}^2 E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-1} \right] + \ldots  + \theta_1\theta_q E \left[ \varepsilon_{t-1}  \varepsilon_{t+h-q} \right] \nonumber \\ & & ... \nonumber \\ &-& \theta_q E \left[ \varepsilon_{t-q} \varepsilon_{t+h} \right]  + \theta_{1}\theta_{q} E \left[ \varepsilon_{t-q}  \varepsilon_{t+h-1} \right] + \ldots  + \theta_{q}^{2} E \left[ \varepsilon_{t-q}  \varepsilon_{t+h-q} \right]\end{eqnarray}$$

$$\begin{eqnarray}\gamma_h = \begin{cases} \sigma^2 + \theta_{1}^{2}\sigma^2  +  \ldots + \theta_{q}^{2}\sigma^2\quad & &h = 0, \nonumber \\  - \theta_1 \sigma^2 + \theta_1 \theta_2 \sigma^2 + \ldots + \theta_{q-1} \theta_{q} \sigma^2 \quad & &h = 1, \nonumber \\ -\theta_2 \sigma^2 + \theta_1 \theta_3 \sigma^2 + \ldots + \theta_{q-2} \theta_{q} \sigma^2\quad & &h = 2, \nonumber \\  -\theta_3 \sigma^2 + \theta_1 \theta_{4} \sigma^2 +  \ldots +  \theta_{q-3} \theta_{q} \sigma^2 \quad & &h =3\nonumber \end{cases}\end{eqnarray}$$

---


## MA(q)

De manera general las [**autocovarianzas de un MA(q)**]{.hl-yellow} son las siguientes

$$\begin{eqnarray}\gamma_h = \begin{cases} \left(1 + \theta_{1}^{2} + \ldots + \theta_{q}^{2} \right)\sigma^2  \quad & &h = 0, \nonumber \\  \left(-\theta_h + \sum_{j = 1}^{q-h} \theta_{j}  \theta_{j + h} \right) \sigma^2  & &h = 1, \ldots,q \nonumber \\ 0  \quad & &h > q\nonumber \end{cases}\end{eqnarray}$$

$$\begin{eqnarray}\rho_h = \frac{\gamma_h}{\gamma_0} = \begin{cases} 1 \quad & &h = 0, \nonumber \\  \frac{\left(-\theta_h + \sum_{j = 1}^{q-h} \theta_{j}  \theta_{j + h} \right)}{\left(1 + \theta_{1}^{2} + \ldots + \theta_{q}^{2} \right)} & &h = 1, \ldots,q \nonumber \\ 0  \quad & &h > q\nonumber \end{cases}\end{eqnarray}$$

---

## MA(q): invertibilidad

¬øQu√© [**condiciones**]{.hl-yellow} necesitamos?

. . .

Como antes vamos a intentar poner los errores en funci√≥n de $X_t$

$$\begin{eqnarray}\varepsilon_{t} &=& X_t + \theta_1 \varepsilon_{t-1} + \ldots +  \theta_q \varepsilon_{t-q} \nonumber \\ &=& X_t + \theta_1 \left( X_{t-1} + \theta_1 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q-1} \right) +\theta_2 \varepsilon_{t-2}  + \ldots +  \theta_q \varepsilon_{t-q} \nonumber \\ &=& X_t + \theta_1 X_{t-1} + \theta_{1}^{2} \varepsilon_{t-2} + \ldots + \theta_1\theta_q \varepsilon_{t-q-1} \nonumber \\ & & + \theta_2 \varepsilon_{t-2}  + \ldots +  \theta_q \varepsilon_{t-q} \nonumber \\ &=& X_t + \theta_1 X_{t-1} + \theta_{1}^{2} \left( X_{t-2} + \theta_1 \varepsilon_{t-3} + \ldots + \theta_q \varepsilon_{t-q-2} \right) + \ldots + \theta_1\theta_q \varepsilon_{t-q-1}\nonumber \\ & & + \theta_2 \left( X_{t-2} + \theta_1 \varepsilon_{t-3} + \ldots + \theta_q \varepsilon_{t-q-2} \right)  + \ldots +  \theta_q \varepsilon_{t-q} \nonumber \\ &=& \ldots = \sum_{j=0}^{\infty} \pi_j X_{t-j} \end{eqnarray}$$

---

## MA(q): invertibilidad


Diremos que un [**proceso MA(q) es invertible**]{.hl-yellow} si

$$\varepsilon_{t} = \sum_{j=0}^{\infty} \pi_j X_{t-j} = \Pi \left(B \right) X_t, \quad  \Pi \left(B \right) = \sum_{j=0}^{\infty} \pi_j B^j = 1 + \pi_1 B + \pi_2 B^2 + ...$$


Dado que por definici√≥n $X_t = \Theta_q (B) \varepsilon_t$ entonces 

$$\varepsilon_{t}  = \Pi \left(B \right) \Theta_q (B) \varepsilon_t$$

por lo que los [**coeficientes de $\Pi (B)$ se determinar√°n**]{.hl-yellow} tal que $\Pi \left(B \right) \Theta_q (B) = 1$.

---

## MA(q): invertibilidad

Si $\Pi \left(B \right) \Theta_q (B) = 1$, entonces $\Pi (B) = \Theta_{q}^{-1} (B)$.

Es decir, dicho polinomio $\Theta_{q}(B)$ tiene que ser por tanto invertible. Se puede demostrar como un [**proceso MA(q) es invertible**]{.hl-yellow} si y solo si las ra√≠ces de

$$\Theta_{q}(z) = 1 - \theta_1 z - \ldots - \theta_q z^q = 0$$

son (en m√≥dulo) mayores que la unidad ($\left| z \right| > 1$). 

---

## MA(q) invertibilidad

El paquete `{pracma}` nos permite [**determinar las ra√≠ces de un polinomio con la funci√≥n `roots()`**]{.hl-yellow} introduciendo los coeficientes del polinomio de m√°s grado a menos.

Por ejemplo si queremos calcular las ra√≠ces de $-z^3 +2 z^2 +1.5* z - 2$ 


```{r}
pracma::roots(c(-1, 2, 1.5, -2))
```


. . .

Si queremos definir un $MA(q)$, por ejemplo, $X_t = \varepsilon_t - 0.2 \varepsilon_{t-1} + 0.5 \varepsilon_{t-2}$, y ver [**si cumple las condiciones de invertibilidad**]{.hl-yellow} basta con meter los **param√©tros de antiguo a reciente y cambiando el signo**


```{r}
pracma::roots(c(-0.5, 0.2, 1))
```


--- 

## MA(q)

[**üíª Dise√±a una funci√≥n**]{.hl-yellow} para simular $n$ trayectorias de un $MA(q)$ en funci√≥n de $q$, $n$, $\sigma$, $\left(\theta_1,~\theta_2, \ldots, ~\theta_q\right)$ y  $\left(\varepsilon_1, \ldots, \varepsilon_q\right)$


```{r}
#| code-fold: true
MA_q_simul <- function(q, n, sigma, theta, eps_q) {
  
  X_t <- eps_q
  eps <- c(eps_q, rnorm(n = n - q, mean = 0, sd = sigma))
  for (i in (q + 1):n) {
    X_t[i] <- eps[i] - sum(theta * eps[(i - 1):(i - q)])
  } 
  ts <- tibble("t" = 1:n, "X_t" = X_t) |> as_tsibble(index = t)
  return(ts)
}
```



---


## MA(q)

üíª [**Aplica dicha funci√≥n**]{.hl-yellow} para simular un $MA(1)$ con $n = 10000$, $\sigma = 1$, $\varepsilon_1 = 0$ y $\theta_1 = -0.7$, y visualiza el proceso



```{r}
#| code-fold: true
set.seed(1234567)
ggplot(MA_q_simul(1, 10000, 1, theta = -0.7, eps = rep(0, 1))) +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "MA(1) con theta_1 = -0.7 y sigma = 1") +
  theme_minimal()
```


---

## MA en fable

Parece impredecible pero...¬°no!

. . .

Vamos a realizar el **proceso completo con el MA(1) anteriormente generado**


```{r}
set.seed(1234567)
datos <- MA_q_simul(1, 10000, 1, theta = -0.7, eps = rep(0, 1))
```


1. ¬øEs ya [**ruido blanco**]{.hl-yellow}?


```{r}
library(feasts)
datos |> 
  features(X_t, ljung_box)
```


[**Rechazamos la hip√≥tesis nula: no es ruido blanco**]{.hl-red} --> seguimos

---

## MA en fable


2. ¬øEs un [**proceso estacionario**]{.hl-yellow}?

Para ello aplicaremos el conocido como [**Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test**]{.hl-yellow} (Kwiatkowski et al., 1992)] incluyendo `unitroot_kpss` en `features()`. En dicho test su **hip√≥tesis nula es que el proceso es estacionario** (cualquier p-valor por encima de 0.1 te lo trunca a 0.1)


```{r}
# install.packages("urca")
datos |> 
  features(X_t, unitroot_kpss)
```



[**No rechazamos la hip√≥tesis nula de estacionariedad**]{.hl-green} --> seguimos (no hay que aplicar transformaciones).

---


## MA en fable


3. ¬øC√≥mo son las [**autocorrelaciones (ACF)**]{.hl-yellow} para determinar el $q$ adecuado?


```{r}
datos |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot()
```


La **primera autocorrelaci√≥n que sale de la banda** (y adem√°s muy evidente) es la primera --> ¬øser√° $q=1$? (recuerda que el n√∫mero de $\rho_h$ no nulas nos caracteriza un MA)


---

## MA en fable


4. Realizamos el [**ajuste ARIMA**]{.hl-yellow}. De momento solo estamos en el final, la parte $MA(q)$ pero el proceso final se denota como $ARIMA(p, d, q)$. Para indicarle los √≠ndices usamos dentro de `ARIMA(var_objetivo ~ pdq()` la funci√≥n `pdq()` (de momento los dem√°s 0)


```{r}
fit <- 
  datos |> 
  model("MA_1" = ARIMA(X_t ~ pdq(0, 0, 1)))
estimaciones <- fit |> augment()
estimaciones
```



---

## MA en fable


5. ¬øC√≥mo han quedado los [**residuos**]{.hl-yellow}?


Normales, sin patr√≥n longitudinal y todas las ACF dentro de la banda


```{r}
fit |> 
  gg_tsresiduals()
```


---

## MA en fable


5. ¬øC√≥mo han quedado los [**residuos**]{.hl-yellow}?


[**Todas las ACF dentro de la banda**]{.hl-green}


```{r}
estimaciones |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


---

## MA en fable


6. ¬øEl [**residuo es ruido blanco**]{.hl-yellow}?


[**No se rechaza la hip√≥tesis nula de ruido blanco**]{.hl-green} --> hemos terminado. Podemos ver la [**estimaci√≥n de $\theta_1$**]{.hl-yellow} haciendo `report()` del modelo (los par√°metros tienen el signo cambiado a nuestra ecuaci√≥n)


```{r}
estimaciones |> 
  features(.resid, ljung_box)

report(fit)
```



---

## Unicidad del modelo

[**Importante**]{.hl-red}: un proceso MA(1) podr√≠a ser ajustado tambi√©n con un MA(q) CON $q > 1$ ya que basta con que poner el resto de coeficientes a 0 (f√≠jate en la estimaci√≥n que nos devuelve el `report()`) --> [**principio de parsimonia: el modelo m√°s sencillo**]{.hl-yellow}


```{r}
fit <- 
  datos |> 
  model("MA_4" = ARIMA(X_t ~ pdq(0, 0, 4)))
estimaciones <- fit |> augment()
estimaciones |> 
  features(.resid, ljung_box)
report(fit)
```



---


## MA(q)

üíª [**Aplica dicha funci√≥n de simulaci√≥n**]{.hl-yellow} para simular un $MA(4)$ con $n = 10000$, $\sigma = 1$, $\left(\varepsilon_1, \varepsilon_2, \varepsilon_3, \varepsilon_4 \right) = 0$ y $\left(\theta_1, ~ \theta_2,~ \theta_3,~ \theta_4 \right) = (-0.75, -0.9, 0.9, -0.7)$



```{r}
#| code-fold: true
set.seed(1234567)
ggplot(MA_q_simul(4, 10000, 1, theta = c(-0.75, -0.9, 0.9, -0.7), eps = rep(0, 4))) +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "MA(2) con theta_1 = -0.75, theta_2 = -0.9, theta_3 = 0.9 y theta_4 = -0.7 y sigma = 1") +
  theme_minimal()
```


---

## MA(q)

Vamos a realizar el **proceso completo con el MA(2) anteriormente generado**


```{r}
set.seed(1234567)
datos <- MA_q_simul(4, 10000, 1, theta = c(-0.75, -0.9, 0.9, -0.7), eps = rep(0, 4))
```


1. ¬øEs ya [**ruido blanco**]{.hl-yellow}?


```{r}
datos |> 
  features(X_t, ljung_box)
```


[**Problema**]{.hl-red} --> se supone que no deber√≠a ser ruido blanco y cuando pasamos el contraste **no nos rechaza la hip√≥tesis nula**. ¬øPor qu√©?


---

## MA(q)

[**Recuerda**]{.hl-yellow}: el proceso [**tiene que cumplir las condiciones de invertibilidad**]{.hl-yellow} 


```{r}
Mod(pracma::roots(c(-rev(c(-0.75, -0.9, 0.9, -0.7)), 1)))
```




---

## MA(q)


üíª [**Aplica dicha funci√≥n de simulaci√≥n**]{.hl-yellow} para simular un $MA(4)$ con $n = 10000$, $\sigma = 1$, $\left(\varepsilon_1, \varepsilon_2, \varepsilon_3, \varepsilon_4 \right) = 0$ y $\left(\theta_1, ~ \theta_2,~ \theta_3,~ \theta_4 \right) = (-0.1, 0.2, 0.1, 0.3)$ y comprueba que se cumple las condiciones de invertibilidad



```{r}
#| code-fold: true
Mod(pracma::roots(c(-rev(c(-0.1, 0.2, 0.1, 0.3)), 1)))

set.seed(1234567)
ggplot(MA_q_simul(4, 10000, 1, theta = c(-0.1, 0.2, 0.1, 0.3), eps = rep(0, 4))) +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "MA(2) con theta_1 = -0.2, theta_2 = 0.2, theta_3 = 0.1 y theta_4 = 0.3 y sigma = 1") +
  theme_minimal()
```


---

## MA en fable

Vamos a realizar el **proceso completo con el MA(4) anteriormente generado**


```{r}
set.seed(1234567)
datos <- MA_q_simul(4, 10000, 1, theta = c(-0.1, 0.2, 0.1, 0.3), eps = rep(0, 4))
```


1. ¬øEs ya [**ruido blanco**]{.hl-yellow}?


```{r}
datos |> 
  features(X_t, ljung_box)
```


[**Rechazamos la hip√≥tesis nula: no es ruido blanco**]{.hl-red} --> seguimos

. . .

2. ¬øEs un [**proceso estacionario**]{.hl-yellow}?



```{r}
# install.packages("urca")
datos |> 
  features(X_t, unitroot_kpss)
```



[**No rechazamos la hip√≥tesis nula de estacionariedad**]{.hl-green} --> seguimos (no hay que aplicar transformaciones).

---


## MA en fable


3. ¬øC√≥mo son las [**autocorrelaciones (ACF)**]{.hl-yellow} para determinar el $q$ adecuado?


```{r}
datos |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot()
```


Ahora las **cuatro primeras autocorrelaci√≥n se salen de la banda** (y adem√°s muy evidente) pero tambi√©n un poco la quinta  --> ¬øser√° $q=4$ o $q=5$? 


---

## MA en fable


4. Realizamos los dos [**ajustes ARIMA**]{.hl-yellow}. 


```{r}
fit_4 <- 
  datos |> model("MA_4" = ARIMA(X_t ~ pdq(0, 0, 4)))
fit_5 <- 
  datos |> model("MA_5" = ARIMA(X_t ~ pdq(0, 0, 5)))
estimaciones_4 <- fit_4 |> augment()
estimaciones_5 <- fit_5 |> augment()
```


. . .

5. ¬øC√≥mo son las [**autocorrelaciones de los residuos**]{.hl-yellow}?


[**En ambos casos las ACF dentro de la banda**]{.hl-green}

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones_4 |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones_5 |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::
::::

---

## MA en fable


6. ¬øLos [**residuos son ruido blanco**]{.hl-yellow}?



```{r}
estimaciones_4 |> 
  features(.resid, ljung_box)
estimaciones_5 |> 
  features(.resid, ljung_box)
report(fit_4)
```



[**No se rechaza la hip√≥tesis nula de ruido blanco en ning√∫n caso**]{.hl-green} --> nos quedamos con el modelo m√°s sencillo (MA(4)). Podemos ver la [**estimaci√≥n de $\theta_1$**]{.hl-yellow} haciendo `report()` del modelo (de hecho si lo hacemos con `fit_5` en realidad es un MA(5) con el √∫ltimo muy peque√±o).





# Clases 19: [procesos AR]{.flow} {#clase-19}

---

## Modelos AR

Hasta ahora hemos visto solo procesos cuya dependencia se contruye [**promediando los errores pasados**]{.hl-yellow}. ¬øY si hacemos ¬´lo mismo¬ª pero [**promediando el pasado de la serie**]{.hl-yellow}?

. . .

Dado que la idea es hacer una **regresi√≥n con su propio pasado**, a estos modelos se conoce como [**procesos autorregresivos (AR)**]{.hl-yellow} 


$$X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \ldots + \phi_p X_{t-p} + \varepsilon_t, \quad p \geq 1, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

donde $p$ ser√° el **orden autorregresivo** (lo llamaremos [**procesos autorregresivos de orden p o AR(p)**]{.hl-yellow}).

---

## Modelos MA vs AR


Simplemente por fijar conceptos, estos son los dos modelos


:::: columns
::: {.column width="50%"}

$$\begin{eqnarray}X_t &=& \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} \nonumber \\ &=& \varepsilon_t - \sum_{j=1}^{q} \theta_j \varepsilon_{t-j} \quad q \geq 1, \quad \quad \text{MA(q)}\end{eqnarray}$$
:::

::: {.column width="50%"}

$$\begin{eqnarray}X_t &=& \phi_1 X_{t-1} + \ldots + \phi_p X_{t-p} + \varepsilon_t \nonumber \\ &=&  \varepsilon_{t} + \sum_{i=1}\phi_i X_{t-i}, \quad p \geq 1, \quad \quad \text{AR(p)}\end{eqnarray}$$
:::
::::

. . .

De la misma forma podemos hacer uso del [**operador de retardos**]{.hl-yellow} (ahora $\Phi_p(B)$)

$$X_t =  \phi_1 X_{t-1} + \ldots + \phi_p X_{t-p} + \varepsilon_t \quad \Rightarrow \quad \Phi_p \left(B \right)X_t = \varepsilon_t, \quad \Phi_p(B) = 1 - \phi_1B - \ldots - \phi_p B^p \quad $$

. . .


:::: columns
::: {.column width="50%"}

$$\begin{eqnarray}X_t &=& \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} \nonumber \\ &=& \Theta_q (B) \varepsilon_t \quad \quad \text{MA(q)} \end{eqnarray}$$
:::

::: {.column width="50%"}


$$\begin{eqnarray}X_t &=& \varepsilon_t + \phi_1 X_{t-1} + \ldots + \phi_q X_{t-q} \nonumber \\ \Phi_p (B)X_t &=&  \varepsilon_t \quad \quad \text{AR(p)} \end{eqnarray}$$

:::
::::


---

## Proceso AR

$$\begin{eqnarray}X_t &=& \varepsilon_t + \phi_1 X_{t-1} + \ldots + \phi_q X_{t-q} \nonumber \\ \Phi_p (B)X_t &=&  \varepsilon_t \quad \quad \text{AR(p)} \end{eqnarray}$$

Dale valores $p=1, 2, 3, 4$ y escribe c√≥mo quedar√≠a el proceso

. . .

* **AR(1)**: $\Phi_1 \left(B \right)X_t  = \varepsilon_t \Rightarrow \left(1 - \phi_1 B \right)X_t = \varepsilon_t \Rightarrow X_t = \phi_1 X_{t-1} + \varepsilon_t$

* **AR(2)**: $\Phi_2 \left(B \right)X_t  = \varepsilon_t \Rightarrow \left(1 - \phi_1 B - \phi_2 B^2 \right)X_t = \varepsilon_t \Rightarrow X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \varepsilon_t$

* **AR(3)**: $\Phi_3 \left(B \right)X_t  = \varepsilon_t \Rightarrow \left(1 - \phi_1 B - \phi_2 B^2 - \phi_3 B^3  \right)X_t = \varepsilon_t \Rightarrow X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \phi_3 X_{t-3} + \varepsilon_t$


---

## AR(p)

[**üíª Dise√±a una funci√≥n**]{.hl-yellow} para simular $n$ trayectorias de un $AR(p)$ en funci√≥n de $p$, $n$, $\sigma$, $\left(\phi_1,~\phi_2, \ldots, ~\phi_p\right)$ y  $\left(X_1, \ldots, X_p\right)$


```{r}
#| code-fold: true
AR_p_simul <- function(p, n, sigma, phi, X) {
  
  X_t <- X
  for (i in (p + 1):n) {
    X_t[i] <-
      sum(phi * X_t[(i - 1):(i-p)]) + rnorm(1, mean = 0, sd = sigma)
  } 
  ts <- tibble("t" = 1:n, "X_t" = X_t) |> as_tsibble(index = t)
  return(ts)
}
```


---

## AR(p)

üíª [**Aplica dicha funci√≥n**]{.hl-yellow} para simular un $AR(2)$ con $n = 300$, $\sigma = 3$, $\left(X_1, X_2 \right) = 0$ y $\left(\phi_1, ~ \phi_2 \right) = (-0.3, 0.5)$


```{r}
#| code-fold: true
set.seed(1234567)
AR_p_simul(p = 2, n = 300, sigma = 3, phi = c(-0.3, 0.5), X = c(0, 0)) |> 
  ggplot() +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "AR(2) con phi_1 = -0.3, phi_2 = 0.5 y sigma = 3") +
  theme_minimal()
```



---

## AR(p)

üíª [**Aplica la funci√≥n MA_q**]{.hl-yellow} para simular un $MA(2)$ con $n = 10000$, $\sigma = 3$, $\left(\varepsilon_1, \varepsilon_2 \right) = 0$ y $\left(\theta_1, ~ \theta_2 \right) = (-0.3, 0.5)$. Compara ambos procesos (**mismos par√°metros**)

:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
MA_q_simul(2, 10000, 3, theta = c(-0.3, 0.5), eps = rep(0, 2)) |> 
  ggplot() +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "MA(2) con theta_1 = -0.3, theta_2 = 0.5 y sigma = 3") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
set.seed(1234567)
AR_p_simul(p = 2, n = 10000, sigma = 3, phi = c(-0.3, 0.5), X = c(0, 0)) |> 
  ggplot() +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "AR(2) con phi_1 = -0.3, phi_2 = 0.5 y sigma = 3") +
  theme_minimal()
```


:::
::::

. . .

Ambos [**parecen algo impredible y aleatorio y ¬°no es cierto!**]{.hl-red}, simplemente son estacionarios (pero una parte de su comportamiento podemos predecirlo).

---

## MA/AR vs ruido blanco

De hecho en ambos casos al [**realizar el contraste de ruido blanco**]{.hl-yellow} de Ljung-Box obtenemos p-valores menores que 0.05 ==> [**rechazamos la hip√≥tesis nula de ruido blanco**]{.hl-red}

:::: columns
::: {.column width="50%"}


```{r}
set.seed(1234567)
MA_q_simul(2, 10000, 3, theta = c(-0.3, 0.5), eps = rep(0, 2)) |> 
  features(X_t, ljung_box)
```


:::

::: {.column width="50%"}


```{r}
set.seed(1234567)
AR_p_simul(p = 2, n = 10000, sigma = 3, phi = c(-0.3, 0.5), X = c(0, 0)) |> 
  features(X_t, ljung_box)
```


:::
::::

. . .

Pero [**s√≠ son estacionarios**]{.hl-green}

:::: columns
::: {.column width="50%"}


```{r}
set.seed(1234567)
MA_q_simul(2, 10000, 3, theta = c(-0.3, 0.5), eps = rep(0, 2)) |> 
  features(X_t, unitroot_kpss)
```


:::

::: {.column width="50%"}


```{r}
set.seed(1234567)
AR_p_simul(p = 2, n = 10000, sigma = 3, phi = c(-0.3, 0.5), X = c(0, 0)) |> 
  features(X_t, unitroot_kpss)
```


:::
::::


---

## AR(1)

[**¬øC√≥mo demostrar que es estacionario?**]{.hl-yellow}

. . . 

Vamos a empezar por AR(1)

$$X_t = \phi_1 X_{t-1}  + \varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

$$\begin{eqnarray}X_t &=& \phi_1 X_{t-1} +  \varepsilon_t = \phi_1 \left(\phi_1 X_{t-2} +  \varepsilon_{t-1} \right) + \varepsilon_t = \phi_{1}^2 X_{t-2} + \phi_{1}\varepsilon_{t-1}   + \varepsilon_t \nonumber \\ &=& \phi_{1}^2 \left( \phi_1 X_{t-3} +  \varepsilon_{t-2}\right) + \phi_{1}\varepsilon_{t-1}   + \varepsilon_t =\phi_{1}^3 X_{t-3} + \phi_{1}^{2}\varepsilon_{t-2} + \phi_{1}\varepsilon_{t-1}  + \varepsilon_t  \nonumber \\ &=& \ldots = \sum_{j=0}^{\infty} \phi_{1}^j \varepsilon_{t-j} \end{eqnarray}$$


. . .

1. [**Media**]{.hl-yellow}: $E\left[X_t \right] = \sum_{j=0}^{\infty} \phi_{1}^j E\left[ \varepsilon_{t} \right] = 0$

2. [**Varianza**]{.hl-yellow}: $Var\left[X_t \right] = \sum_{j=0}^{\infty} \phi_{1}^{2j} Var\left[ \varepsilon_{t} \right] = \sigma^2 \sum_{j=0}^{\infty} \phi_{1}^{2j} = \sigma^2 \frac{1}{1 - \phi_{1}^2}$ 

---

## AR(1)

3. [**Autocovarianzas**]{.hl-yellow}: teniendo en cuenta que $\left\lbrace \varepsilon_t \right\rbrace$ es ruido blanco (¬°e incorrelado con $X_t$!)

$$\begin{eqnarray}\gamma_h &=& Cov\left[X_t, X_{t+h} \right] = E \left[\sum_{j=0}^{\infty} \phi_{1}^j \varepsilon_{t-j} * \sum_{j=0}^{\infty} \phi_{1}^j \varepsilon_{t+h-j} \right]\nonumber \\  &=& E \left[\left(\varepsilon_{t} + \phi_{1} \varepsilon_{t-1} + \phi_{1}^2 \varepsilon_{t-2} + \ldots \right) \left(\varepsilon_{t+h} + \phi_{1} \varepsilon_{t+h-1} + \phi_{1}^2 \varepsilon_{t+h-2} + \ldots \right) \right] \nonumber \\  &=&\begin{cases}  \sigma^2 \frac{1}{1 - \phi_{1}^2} \quad h = 0, \\ \phi_1 E \left[\varepsilon_{t}\varepsilon_{t} \right] + \phi_{1}^{3}E \left[\varepsilon_{t-1}\varepsilon_{t-1} \right] + \phi_{1}^{5}E \left[\varepsilon_{t-2}\varepsilon_{t-2} \right] + \ldots = \sigma^2 \sum_{j=0}^{\infty} \phi_{1}^{2*j+1}= \sigma^2 \frac{\phi_1}{1 - \phi_{1}^2} \quad h = 1, \\ \phi_{1}^{2} E \left[\varepsilon_{t}\varepsilon_{t} \right] + \phi_{1}^{4}E \left[\varepsilon_{t-1}\varepsilon_{t-1} \right] + \phi_{1}^{6}E \left[\varepsilon_{t-2}\varepsilon_{t-2} \right] + \ldots = \sigma^2 \sum_{j=0}^{\infty} \phi_{1}^{2*j+2}= \sigma^2 \frac{\phi_{1}^2}{1 - \phi_{1}^2} \quad h = 2 \\ ... \quad h > 2 \end{cases}  \nonumber \\ &=& \sigma^2 \sum_{j=0}^{\infty} \phi_{1}^{2*j+h}= \sigma^2 \frac{\phi_{1}^h}{1 - \phi_{1}^2}\end{eqnarray}$$


---


3. [**Autocorrelaciones**]{.hl-yellow}: 

$$\begin{eqnarray}\rho_h &=& \frac{\gamma_h}{\gamma_0} = \frac{\sigma^2 \frac{\phi_{1}^h}{1 - \phi_{1}^2}}{\sigma^2 \frac{1}{1 - \phi_{1}^2}} = \phi_{1}^h\end{eqnarray}$$

lo cual [**implica que $\left| \rho_1 \right| < 1$**]{.hl-yellow} (ya que las correlaciones deber√≠an ser menores que 1 en valor absoluto y **decrecientes**).

&nbsp;

F√≠jate que ahora las [**autocorrelaciones $\rho_h$ NO decaen a 0**]{.hl-purple}

---

## AR(1)

:::: columns
::: {.column width="50%"}


```{r}
set.seed(1234567)
MA_q_simul(q = 1, n = 10000, sigma = 1, theta = c(0.75), eps = c(0)) |> 
  ACF(X_t, lag_max = 30)  |> 
  autoplot() +
  labs(title = "MA(1) con theta_1 = 0.75 y sigma = 1") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
set.seed(1234567)
AR_p_simul(p = 1, n = 10000, sigma = 1, phi = c(0.75), X = c(0)) |> 
  ACF(X_t, lag_max = 30)  |> 
  autoplot() +
  labs(title = "AR(1) con phi_1 = 0.75 y sigma = 1") +
  theme_minimal()
```


:::
::::

¬°La [**funci√≥n ACF nunca va a anularse por completo!**]{.hl-red}, de hecho tardar√° mucho en caer dentro de la banda de incorrelaci√≥n.

---

## AR(1)

F√≠jate adem√°s que dado  que $\rho_h = \phi_{1}^{h}$, [**decrecen de manera exponencial con raz√≥n $\phi_1$**]{.hl-yellow} (si aumentamos $\phi_1$ tarda mucho m√°s en caer)



```{r}
set.seed(1234567)
AR_p_simul(p = 1, n = 10000, sigma = 1, phi = c(0.9), X = c(0)) |> 
  ACF(X_t, lag_max = 30)  |> 
  autoplot()
```




---




## AR(p)

En general $X_t = \phi_1 X_{t-1}  + \phi_2 X_{t-2} + \ldots + \phi_p X_{t-p}  + \varepsilon_t$

$$\begin{eqnarray}X_t &=& \phi_1 X_{t-1} + \phi_2 X_{t-2} + \ldots + \phi_p X_{t-p}  + \varepsilon_t \nonumber \\ &=& \phi_1 \left(\phi_1 X_{t-2} + \ldots + \phi_p X_{t-p-1} + \varepsilon_{t-1} \right)  +  \phi_2 X_{t-2} + \ldots + \phi_p X_{t-p} +\varepsilon_t \nonumber \\ &=&  \left(\phi_{1}^{2} + \phi_2 \right) X_{t-2} + \left(\phi_1 \phi_2 + \phi_3 \right) X_{t-3} + \left(\phi_1 \phi_2 + \phi_3 \right) X_{t-3}+\left(\phi_1 \phi_3 + \phi_4 \right) X_{t-4} \nonumber \\ && + \ldots  +\left(\phi_1 \phi_{p-1} + \phi_{p} \right) X_{t-p} + \phi_1 \phi_{p} X_{t-p-1} + \phi_1 \varepsilon_{t-1} + \varepsilon_t   \nonumber \\ \ &=&  \ldots =^{¬ø?} \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} \quad \text{(proceso MA(}\infty\text{))} \end{eqnarray}$$

. . .

Diremos  que un proceso $AR(p)$, definido como $\Phi_p (B)X_t = \varepsilon_t$, es un [**proceso estacionario (o causal) si admite la representaci√≥n $MA(\infty)$**]{.hl-yellow} anterior, tal que

$$X_t = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} = \Psi (B) \varepsilon_t, \quad \Psi (B) = \sum_{j=0}^{\infty} \psi_j B^j, \quad \sum_{j=0}^{\infty} \left| \psi_j \right| < \infty, ~\psi_0 = 1$$

. . .

Al igual que antes, dado que $\Phi_p X_t = \varepsilon_t$, los [**coeficientes de $\Psi (B)$ se determinar√°n**]{.hl-yellow} tal que $ \Phi_p \Psi \left(B \right) = 1$.

---

## AR(p): invertibilidad

Si $\Phi_p (B) \Psi \left(B \right)  = 1$, entonces $\Psi (B) = \Phi_{p}^{-1} (B)$.

Es decir, dicho polinomio $\Phi_{p}(B)$ **tiene que ser de nuevo invertible**. Se puede demostrar como un [**proceso AR(p) es invertible**]{.hl-yellow} si y solo si las ra√≠ces de

$$\Phi_{p}(z) = 1 - \phi_1 z - \ldots - \phi_p z^p = 0$$

son (en m√≥dulo) mayores que la unidad ($\left| z \right| > 1$). 


---

## AR(p)

Bajo dichas [**condiciones de estacionariedad**]{.hl-yellow}


1. [**Media**]{.hl-yellow}: $E\left[X_t \right] = \sum_{j=0}^{\infty} \psi_j E\left[ \varepsilon_{t-j} \right] = 0$

2. [**Varianza**]{.hl-yellow}: $Var\left[X_t \right] = \sum_{j=0}^{\infty} \psi_{j}^2 Var\left[ \varepsilon_{t-j} \right] = \sigma^2 \sum_{j=0}^{\infty} \psi_{j}^2 = cte < \infty$ (ya que $\sum_{j=0}^{\infty} \left| \psi_{j} \right| < \infty$)

3. [**Autocovarianzas**]{.hl-yellow}: multiplicando la ecuaci√≥n por $X_{t-h}$ tenemos que

$$\begin{eqnarray}\gamma_h &=& E \left[X_{t-h}X_t \right]= E \left[X_{t-h} \left(\phi_1 X_{t-1} + \ldots + \phi_p X_{t-p} + \varepsilon_{t} \right) \right] \nonumber \\ &=& \phi_1 E \left[X_{t-h}  X_{t-1} \right] + \ldots + \phi_p E \left[X_{t-h} X_{t-p} \right] + E \left[X_{t-h} \varepsilon_{t}  \right]\ \nonumber \\ &=& \phi_1 \gamma_{h-1} + \ldots + \phi_p \gamma_{h-p} \quad h \geq 0 \end{eqnarray}$$

---

## AR(p)

3. [**Autocorrelaciones**]{.hl-yellow}: 

$$\begin{eqnarray}\rho_h &=& \frac{\gamma_h}{\gamma_0} =  \phi_1 \frac{\gamma_{h-1}}{\gamma_0} + \ldots + \phi_p \frac{\gamma_{h-p}}{\gamma_0} = \phi_1 \rho_{h-1}  + \ldots + \phi_p \rho_{h-p} \quad h \geq 1 \nonumber \\ \Phi_p (B) \rho_h &=& 0 \end{eqnarray}$$
Como vemos la estructura es m√°s compleja ahora, de hecho deber√≠amos hacer un proceso iterativo complicado, pero lo que [**no sucede es que caigan a 0**]{.hl-yellow}

---

## Ecuaciones Yule-Walker

Si nos fijamos en las primeras $p$ autocorrelaciones tenemos un [**sistema de ecuaciones conocido como ecuaciones de Yule-Walker**]{.hl-yellow}

$$\begin{eqnarray}\rho_1 &=&  \phi_1   +  \phi_2 \rho_1  + \ldots + \phi_p \rho_{p-1} \nonumber \\ \rho_2 &=&  \phi_1 \rho_{1} +  \phi_2 + \ldots + \phi_p \rho_{p-2} \nonumber \\ \vdots & & \quad \vdots \quad  \quad \vdots  \quad \quad \vdots \quad \quad \quad \vdots \nonumber \\  \rho_p &=&  \phi_1 \rho_{p-1} +  \phi_2 \rho_{p-2}  + \ldots + \phi_p \end{eqnarray}$$

---

## AR(p)

üíª [**Aplica la funci√≥n `AR_p`**]{.hl-yellow} para simular un $AR(2)$ con $n = 1000$, $\sigma = 3$, $\left(X_1, X_2 \right) = 0$ y $\left(\phi_1, ~ \phi_2 \right) = (0.3, 0.5)$



```{r}
#| code-fold: true
set.seed(1234567)
ggplot(AR_p_simul(p = 2, 1000, 3, phi = c(0.3, 0.5), X = rep(0, 2))) +
  geom_line(aes(x = t, y = X_t)) +
  labs(title = "AR(2) con phi_1 = 0.3, phi_2 = 0.5 y sigma = 3") +
  theme_minimal()
```

```{r}
pracma::roots(c(0.5, 0.3, 1))
Mod(pracma::roots(c(0.5, 0.3, 1)))
```


---

## AR(p)


```{r}
set.seed(1234567)
AR_p_simul(p = 2, 1000, 3, phi = c(0.3, 0.5), X = rep(0, 2)) |> 
  ACF(X_t, lag_max = 50) |> 
  autoplot() +
  labs(title = "AR(2) con phi_1 = 0.3, phi_2 = 0.5 y sigma = 3") +
  theme_minimal()
```


---

## Autocorrelaci√≥n parcial

Es obvio que en el caso de los procesos $MA(q)$ la [**funci√≥n ACF nos permite su identificaci√≥n**]{.hl-yelow} pero [**no as√≠ en el caso de los procesos $AR(p)$**]{.hl-red}

¬øC√≥mo caracterizarlos? [**¬øQu√© diferencia a un AR(1) de un AR(2)?**]{.hl-yellow}

. . .

* **AR(1)**: $X_t$ y $X_{t-2}$ est√°n relacionados indirectamente  ya que $X_t = \phi_1X_{t-1} + \varepsilon_{t} = \phi_{1}^2 X_{t-2} + \phi_{1} \varepsilon_{t-1} + \varepsilon_t$ pero dado que $\left| \phi_1 \right| < 1$ es una relaci√≥n que se va diluyendo

. . .

* **AR(2)**: sin embargo aqu√≠ $X_t$ y $X_{t-2}$ **s√≠ est√°n relacionados directamente** ya que $X_t = \phi_1X_{t-1} + \phi_2 X_{t-2} + \varepsilon_{t}$ pero dado que $\left| \phi_1 \right| < 1$ es una relaci√≥n que se va diluyendo

. . .

Dicho de otra forma: en el caso de los $AR(1)$, [**si conocemos $X_{t-1}$, el valor de $X_{t-2}$ es irrelevante**]{.hl-yellow} (algo que no sucede en los $AR(2)$). Si yo pudiese calcular la [**autocorrelaci√≥n entre $X_t$ y $X_{t-2}$ ELIMINANDO el efecto**]{.hl-green} de $X_{t-1}$, tendr√≠a que observar como dicho valor es nulo.


---

## Autocorrelaci√≥n parcial

[**¬øC√≥mo eliminar de $X_{t+k}$ el efecto de $X_{t+1},\ldots,X_{t+k-1}$**?]{.hl-yellow}

. . .

En lugar de calular $\rho_h = Cor[X_t, X_{t+h}]$ calcularemos

$$\alpha_h = Cor[X_t - efecto_{X_t~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)}, X_{t+h} - efecto_{X_{t+h}~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)}]$$

la [**correlaci√≥n de $X_t$ vs $X_{t+h}$ pero quitando el efecto de los retardos intermedios**]{.hl-yellow}

---

## Autocorrelaci√≥n parcial

$$\alpha_h = Cor[X_t - efecto_{X_t~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)}, X_{t+h} - efecto_{X_{t+h}~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)}]$$

Dicho [**efecto lo estimaremos calculando el mejor predictor lineal √≥ptimo**]{.hl-yellow} de $X_t$ y $X_{t+h}$ en funci√≥n de los retardos entre ellas $\left(X_{t+1}, \ldots, X_{t+h-1} \right)$

$$\begin{eqnarray}\widetilde{X}_t &=& efecto_{X_t~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)} = \beta_1 X_{t+1} + \ldots +\beta_{t+h-1} X_{t+h-1} \quad \text{reg lineal} \nonumber \\ \widetilde{X}_{t+h} &=& efecto_{X_{t+h}~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)} = \delta_1 X_{t+1} + \ldots +\delta_{t+h-1} X_{t+h-1} \quad \text{reg lineal} \end{eqnarray}$$

---

## Autocorrelaci√≥n parcial

De esta forma tendremos que

$$\begin{eqnarray}\alpha_h &=& Cor[X_t - efecto_{X_t~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)}, X_{t+h} - efecto_{X_{t+h}~vs~\left(X_{t+1}, \ldots, X_{t+h-1} \right)}] \nonumber \\ &=& \begin{cases} \rho_1 \quad & & h = 1, \\ \frac{Cov \left( X_t -  \widetilde{X}_t, X_{t+h} - \widetilde{X}_{t+h} \right)}{\sqrt{Var \left[ X_t -  \widetilde{X}_t \right] Var \left[X_{t+h} - \widetilde{X}_{t+h} \right]}} \quad & &h > 1\end{cases} \end{eqnarray}$$

y [**se demuestra que $\alpha_p = \phi_p$ y $\alpha_h = 0$ para $h > p$**]{.hl-yellow}. Este truncamiento a partir de $h > p$ no se da en un $MA(q)$ ya que se puede entender como un $AR(\infty)$.

---

## PACF: AR vs MA


:::: columns
::: {.column width="50%"}


```{r}
set.seed(12345)
MA_q_simul(q = 2, n = 10000, sigma = 1, theta = c(-0.3, 0.4), eps = rnorm(n = 2, sd = 1)) |>
  PACF(X_t, lag_max = 30)  |> 
  autoplot() +
  labs(title = "MA(1) con theta_1 = -0.3, theta_2 = 0.4 y sigma = 1") +
  theme_minimal()
```


:::

::: {.column width="50%"}


```{r}
set.seed(12345)
AR_p_simul(p = 2, n = 10000, sigma = 1, phi = c(-0.3, 0.4), X = rnorm(n = 2, sd = 1)) |>
  PACF(X_t, lag_max = 30)  |> 
  autoplot() +
  labs(title = "AR(1) con phi_1 = -0.3, phi_2 = 0.4 y sigma = 1") +
  theme_minimal()
```


:::
::::

* **procesos $MA(q)$**: las [**autocorrelaciones parciales caen exponencialmente**]{.hl-yellow}

* **procesos $AR(p)$**: las [**autocorrelaciones son aprox 0 a partir de $p$ y la correlaci√≥n correspondiente a $p$ se sale de manera evidente**]{.hl-yellow}


---


## MA/AR en fable

[**¬øC√≥mo predecir un MA o AR en fable?**]{.hl-yellow}

Lo primero que haremos ser√° generar un proceso $AR(3)$ y **dividir en train y test**


```{r}
set.seed(1234567)
datos <-
  AR_p_simul(p = 3, n = 1000, sigma = 3,
             phi = c(-0.2, -0.3, 0.4), X = c(0, 0, 0)) 

train <- datos |> slice(1:950)
test <- datos |> slice(951:1000)
```


---

## MA/AR en fable

Tras ello vamos a aplicar en `model()`  el **alisado simple y doble**, y los [**modelos AR y MA haciendo uso de la funci√≥n `ARIMA()`**]{.hl-yellow} (basta con indicar la media  y los √≥rdenes p y q en la funci√≥n `pdq()`)


```{r}
fit_arima <- 
  train |> 
  model("alisado_simple" = ETS(X_t ~ error("A") + trend("N") + season("N")),
        "alisado_doble" = ETS(X_t ~ error("A") + trend("A") + season("N")),
        # AR
        "AR1" = ARIMA(X_t ~ 0 + pdq(1, 0, 0)),
        "AR2" = ARIMA(X_t ~ 0 + pdq(2, 0, 0)),
        "AR3" = ARIMA(X_t ~ 0 + pdq(3, 0, 0)),
        # MA(1)
        "MA1" = ARIMA(X_t ~ 0 + pdq(0, 0, 1)),
        "MA2" = ARIMA(X_t ~ 0 + pdq(0, 0, 2)),
        "MA3" = ARIMA(X_t ~ 0 + pdq(0, 0, 3)))
```


---

## MA/AR en fable

Como antes podemos usar `augment()` para las **estimaciones** y `forecast()` para **extraer las predicciones**


```{r}
estimaciones <- fit_arima |> augment()
predicciones <- fit_arima |> forecast(h = 50)
```


---

## MA/AR en fable

Y con `autoplot()` pintamos los modelos.


```{r}
#| code-fold: true
predicciones |> 
  autoplot(datos, level = NULL) +
  geom_line(data = estimaciones,
            aes(x = t, y = .fitted, color = .model), linewidth = 0.75) +
  theme_minimal()
```



---

## MA/AR en fable

Con `accuracy()` vamos a obtener la [**calidad de cada modelo**]{.hl-yellow}


```{r}
fit_arima |>
  accuracy() |> 
  bind_rows(predicciones |> accuracy(test))
```


---

## MA/AR en fable

Tanto en train como en test el [**mejor modelo es el AR(3)**]{.hl-yellow} (l√≥gico ya que los datos los hemos generado as√≠)


```{r}
fit_arima |>
  accuracy() |> 
  bind_rows(predicciones |> accuracy(test)) |> 
  slice_min(RMSE,n = 1, by = .type)
```


---

## MA/AR en fable

Vamos entonces a [**repetir el proceso solo con dicho modelo**]{.hl-yellow}. Adem√°s con `report()` vamos a poder obtener las [**estimaci√≥n de los coeficientes**]{.hl-yellow} ($0.1914$, $0.3162$ y $-0.4154$, bastante preciso)


```{r}
fit_arima <- 
  train |> 
  model("AR3" = ARIMA(X_t ~ 0 + pdq(3, 0, 0)))
estimaciones <- fit_arima |> augment()
predicciones <- fit_arima |> forecast(h = 50)
report(fit_arima)
```


---

## MA/AR en fable

Tras el proceso, para asegurarnos de que aunque sea el mejor modelo lo que [**nos queda sin explicar (residuo) sea ruido blanco**]{.hl-yellow} vamos a aplicar el **test de Ljung-Box**.


```{r}
# antes de modelizar
datos |> 
  features(X_t, ljung_box)

# despu√©s (a los errores)
estimaciones |> 
  features(.innov, ljung_box)
```


---

## MA/AR en fable

Por √∫ltimo [**visualizamos el diagn√≥stico de los residuos**]{.hl-yellow}


```{r}
fit_arima |> 
  gg_tsresiduals()
```



# Clases 20: [procesos ARMA]{.flow} {#clase-20}

---


## Modelos MA vs AR


Simplemente por fijar conceptos, estos son los dos modelos que hemos visto hasta ahora haciendo uso del [**operador de retardos**]{.hl-yellow}

:::: columns
::: {.column width="50%"}

[**MA(q)**]{.hl-purple}

$$\begin{eqnarray}X_t &=& \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q} \nonumber \\ &=&\Theta_q (B) \varepsilon_t \nonumber \\ 0 &=&1 - \theta_1 z - \ldots - \theta_q z^q  \quad \text{ra√≠ces  } \left| z \right| > 1 \end{eqnarray}$$
:::

::: {.column width="50%"}

[**AR(p)**]{.hl-green}

$$\begin{eqnarray}X_t &=& \varepsilon_t + \phi_1 X_{t-1} + \ldots + \phi_q X_{t-q},  \nonumber \\  \Phi_p (B)X_t &=&  \varepsilon_t \nonumber \\ 0 &=&1 - \phi_1 z - \ldots - \phi_q z^q  \quad \text{ra√≠ces  } \left| z \right| > 1 \end{eqnarray}$$

:::
::::

* [**Funci√≥n de autocorrelaciones (ACF)**]{.hl-yellow}: en su versi√≥n te√≥rica, $\rho_h = 0$ a partir de $h > q$ para los MA(q).


* [**Funci√≥n de autocorrelaciones parciales (PACF)**]{.hl-yellow}: en su versi√≥n te√≥rica, $\alpha_h = 0$ a partir de $h > p$ para los AR(p).

---

## Procesos ARMA


Como dijimos al inicio los procesos MA eran un [**caso particular de lo que se conoce como procesos lineales**]{.hl-yellow}


$$X_t = \sum_{j = -\infty}^{j=\infty} \Psi_j \varepsilon_{t-j}, \quad \sum_{j = -\infty}^{j=\infty} \left| \Psi_j \right| < \infty, \quad  \text{proceso lineal}$$

. . .

* [**MA(q)**]{.hl-yellow}: $\Psi_0 = 1$ y $\Psi_j = 0$ para todo $j \geq q$

* [**AR(p)**]{.hl-yellow}: $\Psi_0 = 1$ y $\Psi_j = 0$ para todo $j <0$, tal que $\Phi_p(B) \Psi(B) = 1$ (se imponen condiciones en el decrecimiento de los coeficientes)

. . .

Los [**procesos ARMA**]{.hl-yellow} intentar√°n combinar ambas propiedades para poder representar nuestros procesos con √≥rdenes $p$ y $q$ bajos.

---

## Procesos ARMA

Diremos que $\left\lbrace X_t \right\rbrace_{t}$ es un [**proceso (mixto) autorregresivo-media m√≥vil de orden $(p,q)$, denotado como $ARMA(p,q)$**]{.hl-yellow} si


$$X_t - \phi_1 X_{t-1} - \ldots - \phi_p X_{t-p} = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

donde $p \geq 1$ ser√° el **orden autorregresivo** y $q \geq 1$ ser√° el **orden de medias m√≥vil**

. . .

Usando el **polinomio de retardos**, podemos [**redefinir un ARMA(p, q)**]{.hl-yellow} como

$$\Phi_p (B)X_t  = \Theta_q (B) \varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$
tal que si $\Phi_p (B)=1$ es un proceso puro MA(q) y si $\Theta_q (B)=1$ es un proceso puro AR(p). 

---

## Procesos ARMA
 
Dado que $\Phi_p (B)$ y $\Theta_q (B)$ son polinomios, ambos **pueden ser expresados en funci√≥n de sus ra√≠ces** tal que

$$\Phi_p (B) = \left( 1- \lambda_1 B\right) \ldots \left( 1- \lambda_p B\right), \quad \Theta_q (B) = \left( 1- \delta_1 B\right) \ldots \left( 1- \delta_q B\right)$$

tal que

$$\left( 1- \lambda_1 B\right) \ldots \left( 1- \lambda_p B\right)X_t  = \left( 1- \delta_1 B\right) \ldots \left( 1- \delta_q B\right) \varepsilon_t$$

. . .

[**¬øQu√© tienen que cumplir esas ra√≠ces?**]{.hl-yellow}
 
 
---

## Procesos ARMA


$$\left( 1- \lambda_1 B\right) \ldots \left( 1- \lambda_p B\right)X_t  = \left( 1- \delta_1 B\right) \ldots \left( 1- \delta_q B\right) \varepsilon_t$$

Deben de ser diferentes: los [**polinomios $\Phi_p (B)$ y $\Theta_q (B)$ no pueden tener ra√≠ces comunes**]{.hl-yellow} ya que si tuviesen, por ejemplo, $\lambda_1 = \delta_1$, entonces podr√≠amos cancelar en ambos lados

$$\left( 1- \lambda_2 B\right) \ldots \left( 1- \lambda_p B\right)X_t  = \left( 1- \delta_2 B\right) \ldots \left( 1- \delta_q B\right) \varepsilon_t$$
teniendo una [**sobreparametrizaci√≥n ya que en realidad el proceso ser√≠a ARMA(p-1, q-1)**]{.hl-yellow}.


---

## Procesos ARMA


$$\Phi_p (B)X_t  = \Theta_q (B) \varepsilon_t, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

Adem√°s vamos a pedir las [**dos condiciones que hemos visto anteriormente**]{.hl-yellow}

* [**ARMA invertible**]{.hl-green}: pediremos que las [**ra√≠ces del polinomio $\Theta_q (B)$**]{.hl-yellow} tengan sus ra√≠ces fuera del c√≠rculo unidad.

* [**ARMA causal (estacionario)**]{.hl-green}: pediremos que las [**ra√≠ces del polinomio $\Phi_p(B)$**]{.hl-yellow} tengan sus ra√≠ces fuera del c√≠rculo unidad.

---

## Procesos ARMA

Bajo dichas condiciones estos procesos se pueden [**re-expresar tanto como $AR(\infty)$ como $MA(\infty)$**]{.hl-yellow}. Veamos un **ejemplo con un ARMA(1, 1)**

$$X_t - \phi_1 X_{t-1}  = \varepsilon_t - \theta_1 \varepsilon_{t-1}, \quad \left\lbrace \varepsilon_{t} \right\rbrace \text{ ruido blanco}$$

. . .

[**Condiciones de invertibilidad y estacionalidad**]{.hl-yellow}: $\left| \phi_1 \right| < 1$ y $\left| \theta_1 \right| < 1$

. . .

Tenemos por tanto que $\Phi_1 (B) X_t = \Theta_1 (B)\varepsilon_t$.

---

## Procesos ARMA

Bajo condiciones de estacionariedad  $X_t = \Pi(B) \varepsilon_t$, entonces

$$\left( 1 - \phi_1 B \right) \Pi(B) = \left( 1 - \theta_1 B \right) \quad  \Rightarrow \quad \left( 1 - \phi_1 B \right) \left( \pi_0 + \pi_1 B + \pi_2 B^2 - \ldots \right)  = \left( 1 - \theta_1 B \right)$$

**Igualando ambos lados** tenemos que 

$$\pi_0 - \left( \phi_1 \pi_0 - \pi_1 \right)B - \left(\phi_1 \pi_1 -\pi_2 \right)B^2 + \ldots = 1 - \theta_1B$$
$$\pi_0 = 1, \quad \pi_1 = \theta_1 -\phi_1, \quad \pi_j = \phi_{1}^{j-1} \left(\theta_1 -\phi_1 \right), ~j > 1 $$

. . .

Bajo condiciones de invertibilidad  $X_t = \Psi(B) \varepsilon_t$, entonces


$$\psi_0 = 1, \quad \psi_1 = \theta_1 -\phi_1, \quad \psi_j = \theta_{1}^{j-1} \left(\phi_1 - \theta_1 \right), ~j > 1 $$

---

## Procesos ARMA

[**¬øEs estacionario?**]{.hl-yellow}

. . .

Bajo condiciones de invertibilidad  $X_t = \Psi(B) \varepsilon_t$

1. [**Media**]{.hl-yellow}: $E\left[X_t \right] = \sum_{j=0}^{\infty} \psi_{j} E\left[ \varepsilon_{t} \right] = 0$

2. [**Varianza**]{.hl-yellow}: $Var \left[X_t \right] = \sum_{j=0}^{\infty} \psi_{j}^{2} Var\left[ \varepsilon_{t-j} \right] = \sigma^2 \sum_{j=0}^{\infty} \psi_{j}^{2}$ 

---

## Procesos ARMA

[**¬øEs estacionario?**]{.hl-yellow}


3. [**Autocovarianzas y autocorrelaciones**]{.hl-yellow}:

$$\begin{eqnarray}\gamma_h &=& Cov\left[X_t, X_{t+h}\right] = E\left[X_t, X_{t+h}\right] = E \left[ \left(\sum_{j=0}^{\infty} \psi_{j} \varepsilon_{t-j} \right) \left( \sum_{j=0}^{\infty} \psi_{j} \varepsilon_{t+h-j} \right) \right] \nonumber \\ &=& \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+h}, \nonumber \\ \rho_h &=& Cor\left[X_t, X_{t+h}\right] =\begin{cases} 1 \quad & h & = 0 \\ \frac{\sum_{j=0}^{\infty} \psi_j \psi_{j+h}}{\sum_{j=0}^{\infty} \psi_{j}^2 } \quad &h& \geq 1 \end{cases} \end{eqnarray}$$ 

[**No caen a 0**]{.hl-yellow}

---

## Procesos ARMA

[**Veamos un ejemplo con el ARMA(1, 1)**]{.hl-yellow}


3. [**Autocovarianzas**]{.hl-yellow}:

$$\begin{eqnarray}\gamma_h &=& Cov\left[X_t, X_{t+h}\right] = E\left[X_t, X_{t+h}\right] = E \left[ \left(\sum_{j=0}^{\infty} \psi_{j} \varepsilon_{t-j} \right) \left( \sum_{j=0}^{\infty} \psi_{j} \varepsilon_{t+h-j} \right) \right] \nonumber \\ &=& \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+h} = \sigma^2 \left( \psi_0 \psi_{h} +\sum_{j=1}^{\infty} \theta_{1}^{j-1} \left(\phi_1 - \theta_1 \right) \theta_{1}^{j+h-1} \left(\phi_1 - \theta_1 \right) \right)\nonumber \\ &=&  \begin{cases} \sigma^2 \left( 1 +\sum_{j=1}^{\infty} \theta_{1}^{2(j-1)} \left(\phi_1 - \theta_1 \right)^2 \right) = \sigma^2 \left( 1 + \frac{\left(\phi_1 - \theta_1 \right)^2 }{1 - \theta_{1}^{2}} \right) \quad & & h = 0, \\ \sigma^2 \left(  \left(\phi_1 - \theta_1 \right) +\sum_{j=1}^{\infty} \theta_{1}^{j-1}  \theta_{1}^{j} \left(\phi_1 - \theta_1 \right)^2 \right) =  \sigma^2 \left( \left(\phi_1 - \theta_1 \right) + \frac{\theta_1 \left(\phi_1 - \theta_1 \right)^2}{1 - \theta_{1}^2} \right) & & h = 1, \\ \sigma^2 \left( \theta_{1}^{h-1} \left(\phi_1 - \theta_1 \right) +  \theta_{1}^{h-1}\sum_{j=1}^{\infty} \theta_{1}^{j-1}  \theta_{1}^{j} \left(\phi_1 - \theta_1 \right)^2 \right) =   \theta_{1}^{h-1} \gamma_1 & & h >1 \\ \end{cases}\end{eqnarray}$$ 

---

## Procesos ARMA

[**Veamos un ejemplo con el ARMA(1, 1)**]{.hl-yellow}


3. [**Autocorrelaciones**]{.hl-yellow}:

$$\begin{eqnarray}\rho_h &=& \frac{\gamma_h}{\gamma_0} =    \theta_{1}^{h-1}  \frac{ \left( 1 - \theta_1 \phi_1 \right) \left(\phi_1 - \theta_1 \right)}{1 + \phi_{1}^2-2 \theta_1 \phi_1} \neq 0 \quad h\geq 1 \end{eqnarray}$$ 

---




## Funci√≥n arima.sim


Podemos hacer uso a partir de ahora de la [**funci√≥n `arima.sim()` para simular procesos ARMA**]{.hl-yellow} 

[**üíª Aplica dicha funci√≥n**]{.hl-yellow} para simular y visualizar $n = 10000$ trayectorias de un $ARMA(2, 1)$ con $\sigma = 1.5$, $\phi = \left(-0.2, 0.7\right)$,  $\theta = 0.8$, $\left(X_1, X_{2}\right) \sim N\left(0, \sigma = 1.5 \right)$ y $\varepsilon_1 = 0$


```{r}
X_t <- arima.sim(n = 10000, list(ar = c(-0.2, 0.7), ma = 0.8), sd = 1.5)
ARMA_2_1 <- tibble("t" = 1:10000, "X_t" = X_t) |> as_tsibble(index = t)
```

```{r}
#| code-fold: true
ggplot(ARMA_2_1) +
  geom_line(aes(x = t, y = X_t)) +
  theme_minimal()
```



---

## ARMA en fable


1. ¬øEs ya [**ruido blanco**]{.hl-yellow}?


```{r}
ARMA_2_1 |> 
  features(X_t, ljung_box)
```


[**Rechazamos la hip√≥tesis nula: no es ruido blanco**]{.hl-red} --> seguimos

. . .

2. ¬øEs un [**proceso estacionario**]{.hl-yellow}?



```{r}
ARMA_2_1 |> 
  features(X_t, unitroot_kpss)
```



[**No rechazamos la hip√≥tesis nula de estacionariedad**]{.hl-green} --> seguimos (no hay que aplicar transformaciones).

---

## ARMA en fable

3. ¬øC√≥mo son las [**autocorrelaciones (ACF)**]{.hl-yellow}?



```{r}
#| code-fold: true
ARMA_2_1 |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot()
```



Las [**autocorrelaciones decrecen exponencialmente debido a la parte autorregresiva**]{.hl-yellow}

---


## ARMA en fable

3. ¬øC√≥mo son las [**autocorrelaciones parciales (PACF)**]{.hl-yellow}?



```{r}
#| code-fold: true
ARMA_2_1 |> 
  PACF(X_t, lag_max = 30) |> 
  autoplot()
```



Las [**autocorrelaciones parciales tambi√©n decrecen exponencialmente debido a la parte de medias m√≥viles**]{.hl-yellow}

. . .

[**¬°No vamos a poder identificar (p, q) con ellas salvo que tratemos primero una de las partes!**]{.hl-red}


---

## ARMA en fable


```{r}
#| code-fold: true
ARMA_2_1 |> 
  gg_tsdisplay(plot_type = "partial")
```


Claramente se salen las primeras de cada gr√°fica as√≠ que el primer paso ser√° [**modelizar un ARMA(1, 1)**]{.hl-yellow}


---

## ARMA en fable


```{r}
fit <-
  ARMA_2_1 |> 
  model("ARMA_1_1" = ARIMA(X_t ~ pdq(1, 0, 1)))
estimaciones <- fit |> augment()
```


:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  PACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::
::::

Vemos que [**a pesar de descender en magnitud siguen sin decaer lo suficiente**]{.hl-red} -> aumentamos el orden (del MA por ejemplo)

---

## ARMA en fable


```{r}
fit <-
  ARMA_2_1 |> 
  model("ARMA_1_2" = ARIMA(X_t ~ pdq(1, 0, 2)))
estimaciones <- fit |> augment()
```


:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  PACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::
::::

No parece el adecuado --> ¬øy ARMA(2, 1)?

---

## ARMA en fable


```{r}
fit <-
  ARMA_2_1 |> 
  model("ARMA_2_1" = ARIMA(X_t ~ pdq(2, 0, 1)))
estimaciones <- fit |> augment()
```


:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  PACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::
::::

¬°Por fin!

---

## ARMA en fable

Efectivamente los [**residuos son ruido blanco**]{.hl-yellow}


```{r}
estimaciones |> 
  features(.resid, ljung_box)
```




---

## Ajuste autom√°tico

Dado que este proceso puede ser farragoso podemos con `ARIMA()` realizar un [**ajuste autom√°tico**]{.hl-yellow} dejando libre los √≥rdenes e indic√°ndole el $p$ inicial con el que buscar, el $q$ final y la [**m√©trica para elegir el mejor modelo (BIC penaliza modelos complejos y es consistente)**]{.hl-yellow}


```{r}
fit <-
  ARMA_2_1 |> 
  model("ARMA" = ARIMA(X_t ~ pdq(p_init = 1, q_init = 1), ic = "bic"))
report(fit)
```


---

## Ajuste autom√°tico

![](https://otexts.com/fpp3/fpp_files/figure-html/ARMAgridsearch-1.png)

Por defecto este [**ajuste autom√°tico se realiza haciendo uso del algoritmo Hyndman-Khandakar**]{.hl-yellow} que lo realiza stepwise y greedy: empezando por la esquina superior de modelos, selecciona el mejor seg√∫n la m√©trica dada; tras ello **busca en su entorno** (moviendo y restado 1 p y q); si encuentro alguno mejor, cambia autom√°ticamente.

---

## Ajuste autom√°tico

![](https://otexts.com/fpp3/fpp_files/figure-html/ARMAgridsearch-1.png)

* si `ARIMA(..., greedy = FALSE)`: evalua todo el entorno, no se cambia al primero que encuentre mejor

* si `ARIMA(..., stepwise = FALSE)`: evalua un grid m√°s amplio de modeos


---

## Ajuste autom√°tico

Tambi√©n puedes [**indicarle un vector concreto de $p$ y $q$ a probar**]{.hl-yellow}


```{r}
fit <-
  ARMA_2_1 |> 
  model("ARMA" = ARIMA(X_t ~ pdq(p = 0:4, q = 0:4), ic = "bic"))
report(fit)
```



---


## Validaci√≥n cruzada

Como suele ser habitual en el campo de la [**calibraci√≥n de modelos**]{.hl-yellow}, una opci√≥n muy habitual es la de la [**validaci√≥n**]{.hl-yellow}:

1. Construir distintos modelos con la informaci√≥n de train

2. Usar los [**conjuntos de la validaci√≥n para evaluar los modelos (o qu√© configuraci√≥n de hiperpar√°metros)**]{.hl-yellow} y decidir cu√°l de ellos  es mejor

3. Una vez elegido el modelo, volver a lanzarlo y evaluarlo en test

---

## Validaci√≥n cruzada

Una de las formas de validaci√≥n m√°s habitual es la [**validaci√≥n cruzada**]{.hl-yellow}: las observaciones del conjunto de train van [**rotando su rol**]{.hl-yellow}.

. . .

Por ejemplo, si tenemos 100 observaciones en train, podemos hacer 100 iteraciones de validaci√≥n, de manera que en cada una entrenamos el modelo con 99 de ellas y otra queda reservada solo para evaluar los modelos.

![](https://interactivechaos.com/sites/default/files/styles/max_800_px/public/2023-03/tutorial_ml_0237.png)

---

## Validaci√≥n cruzada

En el **caso de las series temporales** una estrategia habitual suele ser la siguiente:

1. Descartar las primeras $n$ observaciones para validaci√≥n: habr√° un conjunto m√≠nimo que siempre formar√° parte de train

2. **Iteraci√≥n i**: entrenamos con las primeras $n+i$ observaciones, evaluamos con una √∫nica observaci√≥n $n+i+1$. 

3. Realizamos el promedio de las m√©tricas de evaluaci√≥n obtenidas de los conjuntos de validaci√≥n.

![](https://otexts.com/fpp3/fpp_files/figure-html/cv1-1.png)


---

## Validaci√≥n cruzada

F√≠jate que lo anterior est√° basado en una [**one-step forecast**]{.hl-yellow} (predicci√≥n a horizonte $h = 1$), pero quiz√°s nuestro inter√©s est√© en ver c√≥mo funciona nuestro m√©todo a **horizontes de predicci√≥n mayores**

. . .

1. Descartar las primeras $n$ observaciones para validaci√≥n: habr√° un conjunto m√≠nimo que siempre formar√° parte de train

2. **Iteraci√≥n i**: entrenamos con las primeras $n+i$ observaciones, evaluamos con una √∫nica observaci√≥n $n+i+h$. 

3. Realizamos el promedio de las m√©tricas de evaluaci√≥n obtenidas de los conjuntos de validaci√≥n.

---

## Validaci√≥n cruzada


El ejemplo inferior es para $h = 4$.

![](https://otexts.com/fpp3/fpp_files/figure-html/cv4-1.png)

---

## Validaci√≥n cruzada

Vamos a generar un ARMA(2, 3) con la [**funci√≥n `arima.sim()` para simular procesos ARMA**]{.hl-yellow}


```{r}
#| code-fold: true
set.seed(1234567)
X_t <- arima.sim(n = 2500, list(ar = c(-0.2, 0.4), ma = c(0.6, -0.5, 0.7)), sd = 5)
ARMA_2_3 <- tibble("t" = 1:2500, "X_t" = X_t) |> as_tsibble(index = t)
ARMA_2_3
```


---

## Validaci√≥n cruzada


```{r}
#| code-fold: true
ggplot(ARMA_2_3) +
  geom_line(aes(x = t, y = X_t)) +
  theme_minimal()
```


---

## Validaci√≥n cruzada



1. ¬øEs ya [**ruido blanco**]{.hl-yellow}?


```{r}
ARMA_2_3 |> 
  features(X_t, ljung_box)
```


[**Rechazamos la hip√≥tesis nula: no es ruido blanco**]{.hl-red} --> seguimos

. . .

2. ¬øEs un [**proceso estacionario**]{.hl-yellow}?



```{r}
ARMA_2_3 |> 
  features(X_t, unitroot_kpss)
```



[**No rechazamos la hip√≥tesis nula de estacionariedad**]{.hl-green} --> seguimos (no hay que aplicar transformaciones).

---

## Validaci√≥n cruzada

3. ¬øC√≥mo son las [**autocorrelaciones (ACF)**]{.hl-yellow}?



```{r}
#| code-fold: true
ARMA_2_3 |> 
  ACF(X_t, lag_max = 30) |> 
  autoplot()
```



Las [**autocorrelaciones decrecen exponencialmente**]{.hl-yellow} -> existe parte autorregresiva

---


## Validaci√≥n cruzada

3. ¬øC√≥mo son las [**autocorrelaciones parciales (PACF)**]{.hl-yellow}?



```{r}
#| code-fold: true
ARMA_2_3 |> 
  PACF(X_t, lag_max = 30) |> 
  autoplot()
```



Las [**autocorrelaciones parciales tambi√©n decrecen exponencialmente**]{.hl-yellow} -> existe parte de medias m√≥viles


---

## Validaci√≥n cruzada


```{r}
#| code-fold: true
ARMA_2_3 |> 
  gg_tsdisplay(plot_type = "partial")
```


Claramente se salen las primeras de cada gr√°fica as√≠ que el primer paso ser√° [**modelizar un ARMA(1, 1)**]{.hl-yellow}


---

## Validaci√≥n cruzada

Vamos a [**dividir primero nuestros datos en train (90%) vs test (10%)**]{.hl-yellow}


```{r}
train <-
  ARMA_2_3 |> slice(1:450)

test <-
  ARMA_2_3 |> slice(451:500)
```


---

## Validaci√≥n cruzada


Tras ello vamos a [**generar los subconjuntos de validaci√≥n usando trian**]{.hl-yellow}  con `stretch_tsibble()`, indic√°ndole el n√∫mero de valores iniciales que siempre estar√°n en train, el tama√±o que queremos incrementar los sucesivos conjuntos y un identificador de cada slot

Por ejemplo, vamos a **reservar los 350 primeros valores** y vamos a avanzar a horizonte 1 (es decir, 100 slots de validaci√≥n)


```{r}
data_cv <-
  train |> 
  stretch_tsibble(.init = 350, .step = 1, .id = "cv")
data_cv
```



---

## Validaci√≥n cruzada

Tras generar los slots de validaci√≥n [**entrenamos los modelos con dichos datos**]{.hl-yellow}. Vamos a probar los siguientes modelos:

* $ARMA(1, 1)$
* $ARMA(2, 2)$
* $ARMA(p, q)$ autom√°tico stepwise greedy
* $ARMA(p, q)$ autom√°tico stepwise no greedy
* $ARMA(p, q)$ autom√°tico no stepwise

---

## Validaci√≥n cruzada

* $ARMA(1, 1)$
* $ARMA(2, 2)$
* $ARMA(p, q)$ autom√°tico stepwise greedy
* $ARMA(p, q)$ autom√°tico stepwise no greedy
* $ARMA(p, q)$ autom√°tico no stepwise


```{r}
fit <-
  data_cv |>
  model("ARMA_11" = ARIMA(X_t ~ pdq(p = 1, d = 0, q = 1), ic = "bic"),
        "ARMA_22" = ARIMA(X_t ~ pdq(p = 2, d = 0, q = 2), ic = "bic"),
        "ARMA_stepwise_greedy" = 
          ARIMA(X_t ~ pdq(p_init = 1, d = 0, q_init = 1), ic = "bic"),
        "ARMA_stepwise" = 
          ARIMA(X_t ~ pdq(p_init = 1, d = 0, q_init = 1), ic = "bic", greedy = FALSE),
        "ARMA" = 
          ARIMA(X_t ~ pdq(p_init = 1, d = 0, q_init = 1), ic = "bic", greedy = FALSE, stepwise = FALSE))
estimaciones <- fit |> augment()
```


---

## Validaci√≥n cruzada

Tendremos las [**m√©tricas para cada modelo y cada slot de cv**]{.hl-yellow} que podemos promediar


```{r}
resumen_eval_cv <-
  fit |> 
  accuracy() |> 
  summarise(across(c(ME, RMSE, MAE, MPE, MAPE), mean), .by = .model)
resumen_eval_cv 
```


---


## Validaci√≥n cruzada

¬øC√≥mo [**visualizar las m√©tricas de validaci√≥n cruzada**]{.hl-yellow}


```{r}
#| code-fold: true
fit |>
  accuracy() |> 
  ggplot(aes(x = .model, y = RMSE, fill = .model, color = .model)) +
  geom_boxplot(alpha = 0.5) +
  geom_jitter(width = 0.25, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
```


Si te fijas los [**mejores son el ARMA (completo) y el ARMA stepwise (pero no greedy)**]{.hl-green}: aunque tardan m√°s los resultados son mejores. F√≠jate que en el caso de los greedy tenemos un gap en la calidad seg√∫n hac√≠a la direcci√≥n a la que haya decidido orientarse: [**a veces llega al mejor pero no siempre**]{.hl-red}



---

## Validaci√≥n cruzada

Dado que los [**mejores son el ARMA (completo) y el ARMA stepwise (pero no greedy)**]{.hl-green} de manera similar, optaremos por este √∫ltimo ya que es m√°s r√°pido.


```{r}
fit <-
  train |>
  model("ARMA_stepwise" = 
          ARIMA(X_t ~ pdq(p_init = 1, d = 0, q_init = 1), ic = "bic",
                greedy = FALSE))
estimaciones <- fit |> augment()
report(fit)
```


Nada mal: nuestros datos generados bajo un ARMA(2, 3) han sido **modelizados con un ARMA(2, 3)**.

---

## Ra√≠ces


```{r}
gg_arma(fit)
```


Adem√°s las ra√≠ces de los polinomios asociados caen fueran del c√≠rculo unidad

---

## Diagn√≥stico de residuos


```{r}
fit |>  gg_tsresiduals()
```


---

## Diagn√≥stico de residuos


:::: columns
::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  ACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::

::: {.column width="50%"}


```{r}
#| code-fold: true
estimaciones |> 
  PACF(.resid, lag_max = 30) |> 
  autoplot()
```


:::
::::

[**Todas las autocorrelaciones dentro de la banda**]{.hl-green} 

---

## Diagn√≥stico de residuos

Efectivamente los [**residuos son ruido blanco**]{.hl-yellow}


```{r}
estimaciones |> 
  features(.resid, ljung_box)
```




# Clases 21: [procesos ARIMA]{.flow} {#clase-21}


---

- no estacionario en media
- no estacionario en varianza
- ejemplos datasets reales

---

# Clase 22: [procesos SARIMA]{.flow} {#clase-22}


---

## ...

# Clase 23: [ejemplos]{.flow} {#clase-23}





